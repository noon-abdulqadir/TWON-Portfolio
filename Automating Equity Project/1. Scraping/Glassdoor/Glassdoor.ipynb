{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "if code_dir_name not in str(Path.cwd()).split('/')[-1]:\n",
    "    for _ in range(5):\n",
    "\n",
    "        parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "        if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "            code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "            if code_dir is not None:\n",
    "                break\n",
    "else:\n",
    "    code_dir = str(Path.cwd())\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Function to check for pop-ups\n",
    "def popup(driver):\n",
    "\n",
    "    # Signup popup\n",
    "    try:\n",
    "        signup_popup = WebDriverWait(driver, 10).until(\n",
    "            lambda driver: driver.find_element_by_class_name(\"modal_content\")\n",
    "        )\n",
    "        if (signup_popup.is_displayed()) and (signup_popup.is_enabled()):\n",
    "            print(\"Signup popup detected.\")\n",
    "            close_signup_popup = driver.find_element_by_id(\"prefix__icon-close-1\")\n",
    "            close_signup_popup.click()\n",
    "            print(\"Signup popup dismissed.\")\n",
    "            # WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'SearchForm')))\n",
    "    except errors:\n",
    "\n",
    "        # Feedback popup\n",
    "        try:\n",
    "            feedback_popup = WebDriverWait(driver, 10).until(\n",
    "                lambda driver: driver.find_element_by_class_name(\n",
    "                    \"modal_main gdGrid common__modalContent___3TYQJ\"\n",
    "                )\n",
    "            )\n",
    "            if (feedback_popup.is_displayed()) and (feedback_popup.is_enabled()):\n",
    "                # print('Feedback popup detected.')\n",
    "                close_feedback_popup = driver.find_element_by_class_name(\n",
    "                    \"SVGInline modal_closeIcon\"\n",
    "                )\n",
    "                close_feedback_popup.click()\n",
    "                # print('Feedback popup dismissed.')\n",
    "                # WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'SearchForm')))\n",
    "        except errors:\n",
    "\n",
    "            # Accept Alert\n",
    "            try:\n",
    "                click_alert = driver.switch_to.alert()\n",
    "                # print('Alert found.')\n",
    "                click_alert.accept()\n",
    "                # print('Alert accepted.')\n",
    "                # WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'SearchForm')))\n",
    "            except errors:\n",
    "\n",
    "                # Cookie popup\n",
    "                try:\n",
    "                    cookie_popup = WebDriverWait(driver, 10).until(\n",
    "                        lambda driver: driver.find_element_by_id(\n",
    "                            \"onetrust-button-group\"\n",
    "                        )\n",
    "                    )\n",
    "                    if (cookie_popup.is_displayed()) and (cookie_popup.is_enabled()):\n",
    "                        # print('Cookie detected.')\n",
    "                        close_cookie_popup = driver.find_element_by_id(\n",
    "                            \"onetrust-accept-btn-handler\"\n",
    "                        )\n",
    "                        if str(close_cookie_popup.text) == str(\"Accepteer cookies\"):\n",
    "                            close_cookie_popup.click()\n",
    "                            # print('Cookie accepted.')\n",
    "                        # WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'SearchForm')))\n",
    "                except errors:\n",
    "                    pass\n",
    "                except:\n",
    "                    try:\n",
    "                        print(\n",
    "                            f\"Cookie popup caused unexpected error: {sys.exc_info()[0]}\"\n",
    "                        )\n",
    "                    except (NoSuchWindowException, urllib3.exceptions.MaxRetryError):\n",
    "                        pass\n",
    "            except:\n",
    "                try:\n",
    "                    print(f\"Alert caused unexpected error: {sys.exc_info()[0]}\")\n",
    "                except (NoSuchWindowException, urllib3.exceptions.MaxRetryError):\n",
    "                    pass\n",
    "        except:\n",
    "            try:\n",
    "                print(f\"Feedback popup caused unexpected error: {sys.exc_info()[0]}\")\n",
    "            except (NoSuchWindowException, urllib3.exceptions.MaxRetryError):\n",
    "                pass\n",
    "    except:\n",
    "        try:\n",
    "            print(f\"Signup popup caused unexpected error: {sys.exc_info()[0]}\")\n",
    "        except (NoSuchWindowException, urllib3.exceptions.MaxRetryError):\n",
    "            pass\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Find selected country and make sure country is Netherlands\n",
    "def find_country(driver, soup, country):\n",
    "\n",
    "    # Get html of page with beautifulsoup4\n",
    "    # html = driver.page_source\n",
    "    # print('Feeding html driver to BeautifulSoup.')\n",
    "    # soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    try:\n",
    "        country_container = (\n",
    "            soup.find(\"article\", id=\"MainCol\")\n",
    "            .find(\"div\", class_=\"css-1efvhw7 ee8wdg0\")\n",
    "            .find(\"div\")\n",
    "            .find(\"div\", class_=\"pt pb-sm css-1ohf0ui\")\n",
    "            .find(\"div\", class_=\"css-qo68e2\")\n",
    "        )\n",
    "    except (NoSuchElementException, TimeoutException):\n",
    "        print(f\"No country selector found.\")\n",
    "    except errors:\n",
    "        print(f\"Unexpected error choosing {str(country)}: {sys.exc_info()[0]}.\")\n",
    "    else:\n",
    "        country_chooser_soup = xpath_soup(\n",
    "            country_container.find(\"div\", class_=\"dropDownOptionsContainer\").find(\"ul\")\n",
    "        )\n",
    "        country_chooser = WebDriverWait(driver, 10).until(\n",
    "            lambda driver: driver.find_element_by_xpath(country_chooser_soup)\n",
    "        )\n",
    "\n",
    "        # Get html of page with beautifulsoup4\n",
    "        # html = driver.page_source\n",
    "        # print('Feeding html driver to BeautifulSoup.')\n",
    "        # soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        # country_selected = soup.find('option', selected = re.compile(r'.*')).text\n",
    "        dropdown_container = country_container.find(\"div\", class_=\"selectedLabel\")\n",
    "        country_selected = dropdown_container.text\n",
    "\n",
    "        if str(country) != (country_selected):\n",
    "            print(\n",
    "                f\"{str(country)} is not selected. {str(country_selected)} is selected.\"\n",
    "            )\n",
    "            dropdown_arrow_soup = xpath_soup(\n",
    "                dropdown_container.find(\"span\", class_=\"SVGInline arrowDown\")\n",
    "            )\n",
    "            dropdown_arrow = WebDriverWait(driver, 10).until(\n",
    "                lambda driver: driver.find_element_by_xpath(dropdown_arrow_soup)\n",
    "            )\n",
    "            dropdown_arrow.click()\n",
    "            time.sleep(5)\n",
    "\n",
    "            if dropdown_container.parent[\"aria-expanded\"] == \"true\":\n",
    "\n",
    "                # Get html of page with beautifulsoup4\n",
    "                # html = driver.page_source\n",
    "                # print('Feeding html driver to BeautifulSoup.')\n",
    "                # soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "                countries_select_soup = [\n",
    "                    countries_soup.find(\"span\", class_=\"dropdownOptionLabel\")\n",
    "                    for countries_soup in country_chooser_soup.find_all(\"li\")\n",
    "                ]\n",
    "                if any(str(x.text) == str(country) for x in countries_select_soup):\n",
    "                    if str(x.text for x in countries_select_soup) == str(country):\n",
    "                        country_select_soup = x\n",
    "\n",
    "                elif all(str(x.text) != str(country) for x in countries_select_soup):\n",
    "                    print(f\"No match found for country {str(country)}.\")\n",
    "                    country = str(\n",
    "                        input(\n",
    "                            f\"Please type one of the following country options in the input box:\\n{str(countries_select_soup)}.\"\n",
    "                        )\n",
    "                    )\n",
    "                    soup = find_country(driver, soup, country)\n",
    "\n",
    "                country_select_xpath = xpath_soup(country_select_soup)\n",
    "                country_select = WebDriverWait(driver, 10).until(\n",
    "                    lambda driver: driver.find_element_by_xpath(country_select_xpath)\n",
    "                )\n",
    "                country_select.click()\n",
    "                print(f\"Selecting {str(country)}.\")\n",
    "\n",
    "                # Get html of page with beautifulsoup4\n",
    "                # html = driver.page_source\n",
    "                # print('Feeding html driver to BeautifulSoup.')\n",
    "                # soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "            elif dropdown_container.parent[\"aria-expanded\"] == \"false\":\n",
    "                soup = find_country(driver, soup, country)\n",
    "\n",
    "            else:\n",
    "                print(f\"Unexpected error choosing {str(country)}: {sys.exc_info()[0]}.\")\n",
    "\n",
    "        elif str(country) == str(country_selected):\n",
    "            pass\n",
    "        print(f\"Searching for job ads in {str(country)}.\")\n",
    "        # WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'SearchForm')))\n",
    "\n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find page number\n",
    "def find_page_number(driver, soup, page_counter, keyword):\n",
    "    # Get html of page with beautifulsoup4\n",
    "    # html = driver.page_source\n",
    "    # print('Feeding html driver to BeautifulSoup.')\n",
    "    # soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    try:\n",
    "        page_footer = WebDriverWait(driver, 10).until(\n",
    "            EC.visibility_of_element_located(\n",
    "                (By.CSS_SELECTOR, \"#MainCol > div.tbl.fill.px.my.d-flex\")\n",
    "            )\n",
    "        )\n",
    "        page_number_text = soup.find(\"div\", class_=\"cell middle hideHH py-sm\").text\n",
    "        page_number_length = [int(s) for s in page_number_text.split() if s.isdigit()][\n",
    "            1\n",
    "        ]\n",
    "        if int(page_number_length) == 1:\n",
    "            print(f\"Only 1 page found for {str(keyword)} jobs.\")\n",
    "            page_number = int(page_number_length)\n",
    "            print(f\"Current page number: {page_number}.\")\n",
    "        elif int(page_number_length) != 1:\n",
    "            print(f\"Total of {page_number_length} pages found for {str(keyword)} jobs.\")\n",
    "            page_number = [int(s) for s in page_number_text.split() if s.isdigit()][0]\n",
    "            print(f\"Current page number: {page_number}.\")\n",
    "    except errors:\n",
    "        page_number = page_counter\n",
    "        page_number_length = 99\n",
    "        print(\n",
    "            f\"No page number indicator found. Page number set to counter: {page_number}\"\n",
    "        )\n",
    "\n",
    "    return page_number_length, page_number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Function to click tabs\n",
    "def click_tab(driver, soup, tab_name_key, tab_name_value):\n",
    "\n",
    "    time.sleep(3)\n",
    "    # Get html of page with beautifulsoup4\n",
    "    html = driver.page_source\n",
    "    print(\"Feeding html driver to BeautifulSoup.\")\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    try:\n",
    "        tab_exists = False\n",
    "        pan_not_error = WebDriverWait(driver, 10).until_not(\n",
    "            EC.visibility_of_element_located(\n",
    "                (By.XPATH, '//div[contains(@class, \"noPad opened transformNone\")]')\n",
    "            )\n",
    "        )\n",
    "        tabs_not_error = WebDriverWait(driver, 10).until_not(\n",
    "            EC.visibility_of_element_located(\n",
    "                (By.XPATH, '//*[@id=\"JDCol\"]/div/div[2]/h3')\n",
    "            )\n",
    "        )\n",
    "        if (pan_not_error) and (tabs_not_error):\n",
    "            # Get current active tab\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.ID, \"SerpFixedHeader\"))\n",
    "            )\n",
    "            tab_scroll = soup.find(\"div\", attrs={\"data-test\": \"scrollable-tabs\"})\n",
    "            print(f\"Target tab: {str(tab_name_key)} ({str(tab_name_value)}).\")\n",
    "            try:\n",
    "                tab_soup = tab_scroll.find(\n",
    "                    \"div\", attrs={\"data-test\": str(tab_name_key).strip()}\n",
    "                )\n",
    "            except:\n",
    "                print(f\"{str(tab_name_key)} ({tab_name_value}) tab does not exist.\")\n",
    "                tab_exists = False\n",
    "            else:\n",
    "                tab_exists = True\n",
    "\n",
    "                if tab_soup != None:\n",
    "                    xpath = xpath_soup(tab_soup)\n",
    "                    try:\n",
    "                        tab = WebDriverWait(driver, 10).until(\n",
    "                            lambda driver: driver.find_element_by_xpath(xpath)\n",
    "                        )\n",
    "                        tab.click()\n",
    "                    except:\n",
    "                        print(\n",
    "                            f\"Could not click {str(tab_name_key)} ({str(tab_name_value)}) tab.\"\n",
    "                        )\n",
    "                        tab = WebDriverWait(driver, 10).until(\n",
    "                            lambda driver: driver.find_element_by_xpath(xpath)\n",
    "                        )\n",
    "                        time.sleep(2)\n",
    "                        tab.click()\n",
    "                    else:\n",
    "                        print(\n",
    "                            f\"Clicked {str(tab_name_key)} ({str(tab_name_value)}) tab.\"\n",
    "                        )\n",
    "                elif tab_soup is None:\n",
    "                    print(\n",
    "                        f\"No {str(tab_name_key)} ({str(tab_name_value)}) tab on page.\"\n",
    "                    )\n",
    "                    tab_exists = False\n",
    "        elif not tabs_not_error:\n",
    "            if tabs_not_error.text == \"Fout bij het laden\":\n",
    "                print(f\"No tabs found.\")\n",
    "            else:\n",
    "                if pan_not_error:\n",
    "                    print(\n",
    "                        f\"Waiting for {str(tab_name_key)} ({tab_name_value}) tab. Resuming in 10 seconds: {sys.exc_info()[0]}.\"\n",
    "                    )\n",
    "                    soup, tab_exists = click_tab(\n",
    "                        driver, soup, tab_name_key, tab_name_value\n",
    "                    )\n",
    "        elif not pan_not_error:\n",
    "            print(f\"No pan found.\")\n",
    "    except errors:\n",
    "        print(\n",
    "            f\"Unexpected error going to {str(tab_name_key)} ({tab_name_value}) tab: {sys.exc_info()[0]}.\"\n",
    "        )\n",
    "\n",
    "    return soup, tab_exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get job data from company tab\n",
    "def company_tab(driver, soup, info_key, info_value):\n",
    "\n",
    "    pattern = re.compile(r\"[^A-Za-z0-9, ]+\")\n",
    "\n",
    "    # Get html of page with beautifulsoup4\n",
    "    html = driver.page_source\n",
    "    print(\"Feeding html driver to BeautifulSoup.\")\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.CSS_SELECTOR, '#CompanyContainer')))\n",
    "    try:\n",
    "        print(\"-\" * 20)\n",
    "        info_row = soup.find(\"div\", id=\"EmpBasicInfo\").find(\"div\").find(\"div\")\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No {info_key} ({info_value}) found.\")\n",
    "        info_row = -1\n",
    "    except errors:\n",
    "        print(\n",
    "            f\"Unexpected error getting {info_key} ({info_value}): {sys.exc_info()[0]}\"\n",
    "        )\n",
    "        info_text = -1\n",
    "    else:\n",
    "        for info in info_row:\n",
    "            try:\n",
    "                info_label = info.find(\"span\").text\n",
    "            except:\n",
    "                pass\n",
    "            else:\n",
    "                if str(info_label) == str(info_value):\n",
    "                    info_text = str(info.find_all(\"span\")[1].text).strip()\n",
    "                    print(f\"{info_key}: {info_text}\")\n",
    "                    print(\"-\" * 20)\n",
    "                else:\n",
    "                    pass\n",
    "    try:\n",
    "        return info_text\n",
    "    except:\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect individual job links\n",
    "def get_each(\n",
    "    driver,\n",
    "    main_window,\n",
    "    window_before,\n",
    "    keyword,\n",
    "    keyword_file,\n",
    "    site,\n",
    "    jobs,\n",
    "    job_soup,\n",
    "    job_id,\n",
    "    jobs_count,\n",
    "    page_counter,\n",
    "    page_number,\n",
    "    page_number_length,\n",
    "    save_path,\n",
    "    json_file_name,\n",
    "):\n",
    "    jobs = remove_dupe_dicts(jobs)\n",
    "\n",
    "    # If results open in new window, make sure everything loads\n",
    "    new_window = check_window(driver, main_window, window_before)\n",
    "\n",
    "    # Get html of page with beautifulsoup4\n",
    "    html = driver.page_source\n",
    "    print(\"Feeding html driver to BeautifulSoup.\")\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # Get job id, url and age\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Job ID: {job_id}\")\n",
    "    print(\"-\" * 20)\n",
    "    job_url = \"https://www.glassdoor.nl\" + str(\n",
    "        job_soup.find(\"div\", class_=\"d-flex flex-column css-x75kgh e1rrn5ka3\").find(\n",
    "            \"a\"\n",
    "        )[\"href\"]\n",
    "    )\n",
    "    print(f\"Job URL: {job_url}\")\n",
    "    print(\"-\" * 20)\n",
    "    job_age = job_soup.find(\n",
    "        \"div\",\n",
    "        class_=\"d-flex align-items-end pl-std css-mi55ob\",\n",
    "        attrs={\"data-test\": \"job-age\"},\n",
    "    ).text\n",
    "    for s in job_age.split():\n",
    "        job_age_number = int(s) if s.isdigit() else job_age\n",
    "    for i in re.findall(\"[\\w +/.]\", job_age):\n",
    "        job_age_unit = str(i) if i.isalpha() else job_age\n",
    "    print(f\"Job Age: {job_age}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    #########################################################\n",
    "    #########################################################\n",
    "    ####################### TAB INFO #######################\n",
    "    #########################################################\n",
    "    #########################################################\n",
    "\n",
    "    tab_name = {\"job\": \"Vacature\", \"overview\": \"Bedrijf\", \"rating\": \"Beoordeling\"}\n",
    "\n",
    "    #########################################################\n",
    "    ##################### JOB OFFER TAB #####################\n",
    "    #########################################################\n",
    "\n",
    "    # Click job offer tab\n",
    "    try:\n",
    "        soup, job_offer_tab_exists = click_tab(\n",
    "            driver, soup, list(tab_name.keys())[0], list(tab_name.values())[0]\n",
    "        )\n",
    "    except NoSuchElementException:\n",
    "        print(\n",
    "            f\"No {list(tab_name.keys())[0]} tab - {sys.exc_info()[0]}.\\nNo job title, job description, company name, company location, company url.\"\n",
    "        )\n",
    "        job_title, job_description, company_name, location = assign_all(4, -1)\n",
    "    else:\n",
    "        if job_offer_tab_exists:\n",
    "            # Get main data\n",
    "            try:\n",
    "                (\n",
    "                    job_title,\n",
    "                    job_description,\n",
    "                    company_name,\n",
    "                    location,\n",
    "                    company_url,\n",
    "                ) = get_main(driver, soup, job_soup)\n",
    "            # In case there is no company tab, -1 for all variables\n",
    "            except NoSuchElementException:\n",
    "                print(\n",
    "                    f\"No {list(tab_name.keys())[0]} tab - {sys.exc_info()[0]}.\\nNo job title, job description, company name, company location, company url.\"\n",
    "                )\n",
    "                (\n",
    "                    job_title,\n",
    "                    job_description,\n",
    "                    company_name,\n",
    "                    location,\n",
    "                    company_url,\n",
    "                ) = assign_all(5, -1)\n",
    "        elif not job_offer_tab_exists:\n",
    "            print(\n",
    "                f\"{list(tab_name.keys())[0]} tab doesn't exist. Refereshing and trying again.\"\n",
    "            )\n",
    "            driver.refresh()\n",
    "            time.sleep(10)\n",
    "            try:\n",
    "                # Click job offer tab\n",
    "                soup, job_offer_tab_exists = click_tab(\n",
    "                    driver, soup, list(tab_name.keys())[0], list(tab_name.values())[0]\n",
    "                )\n",
    "                if job_offer_tab_exists:\n",
    "                    (\n",
    "                        job_title,\n",
    "                        job_description,\n",
    "                        company_name,\n",
    "                        location,\n",
    "                        company_url,\n",
    "                    ) = get_main(driver, soup, job_soup)\n",
    "                elif not job_offer_tab_exists:\n",
    "                    (\n",
    "                        job_title,\n",
    "                        job_description,\n",
    "                        company_name,\n",
    "                        location,\n",
    "                        company_url,\n",
    "                    ) = assign_all(5, -1)\n",
    "            except:\n",
    "                (\n",
    "                    job_title,\n",
    "                    job_description,\n",
    "                    company_name,\n",
    "                    location,\n",
    "                    company_url,\n",
    "                ) = assign_all(5, -1)\n",
    "\n",
    "    #########################################################\n",
    "    ##################### COMPANY TAB #####################\n",
    "    #########################################################\n",
    "\n",
    "    try:\n",
    "        # Click company tab\n",
    "        soup, company_tab_exists = click_tab(\n",
    "            driver, soup, list(tab_name.keys())[1], list(tab_name.values())[1]\n",
    "        )\n",
    "    except NoSuchElementException:\n",
    "        print(\n",
    "            f\"No {list(tab_name.keys())[1]} tab - {sys.exc_info()[0]}.\\nNo industry, sector, company size, company type.\"\n",
    "        )\n",
    "        industry, sector, company_size, company_type = assign_all(4, -1)\n",
    "    else:\n",
    "        if company_tab_exists:\n",
    "            # Get info in tabs\n",
    "            company_tab_info = {\n",
    "                \"Industry\": \"Bedrijfstak\",\n",
    "                \"Sector\": \"Sector\",\n",
    "                \"Size\": \"Grootte\",\n",
    "                \"Type\": \"Type\",\n",
    "            }\n",
    "\n",
    "            # Getting industry\n",
    "            industry = company_tab(\n",
    "                driver,\n",
    "                soup,\n",
    "                list(company_tab_info.keys())[0],\n",
    "                list(company_tab_info.values())[0],\n",
    "            )\n",
    "\n",
    "            # Getting sector\n",
    "            sector = company_tab(\n",
    "                driver,\n",
    "                soup,\n",
    "                list(company_tab_info.keys())[1],\n",
    "                list(company_tab_info.values())[1],\n",
    "            )\n",
    "\n",
    "            # Getting company size\n",
    "            company_size = company_tab(\n",
    "                driver,\n",
    "                soup,\n",
    "                list(company_tab_info.keys())[2],\n",
    "                list(company_tab_info.values())[2],\n",
    "            )\n",
    "\n",
    "            # Getting company type\n",
    "            company_type = company_tab(\n",
    "                driver,\n",
    "                soup,\n",
    "                list(company_tab_info.keys())[3],\n",
    "                list(company_tab_info.values())[3],\n",
    "            )\n",
    "\n",
    "        elif not company_tab_exists:\n",
    "            print(\n",
    "                f\"{list(tab_name.keys())[1]} tab doesn't exist. Moving on to next tab.\"\n",
    "            )\n",
    "            industry, sector, company_size, company_type = assign_all(4, -1)\n",
    "\n",
    "    #########################################################\n",
    "    ###################### RATINGS TAB #####################\n",
    "    #########################################################\n",
    "\n",
    "    try:\n",
    "        # Click ratings tab\n",
    "        soup, rating_tab_exists = click_tab(\n",
    "            driver, soup, list(tab_name.keys())[2], list(tab_name.values())[2]\n",
    "        )\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No {list(tab_name.keys())[2]} tab - {sys.exc_info()[0]}.\")\n",
    "        rating = -1\n",
    "    else:\n",
    "        if rating_tab_exists:\n",
    "            try:\n",
    "                print(\"-\" * 20)\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"#RatingContainer\"))\n",
    "                )\n",
    "                try:\n",
    "                    rating = (\n",
    "                        driver.find_element_by_xpath(\n",
    "                            '//*[@id=\"RatingContainer\"]/div[1]/div/div[1]/div[1]'\n",
    "                        ).text.split()\n",
    "                    )[0]\n",
    "                except:\n",
    "                    rating = (job_soup.find(\"div\").find(\"span\").text.split())[0]\n",
    "                print(f\"Rating average: {rating}\")\n",
    "                print(\"-\" * 20)\n",
    "            except NoSuchElementException:\n",
    "                print(f\"No job rating found on {list(tab_name.keys())[2]} tab.\")\n",
    "                rating = -1\n",
    "            except errors:\n",
    "                print(\n",
    "                    f\"Unexpected error getting {list(tab_name.keys())[2]}: {sys.exc_info()[0]}.\"\n",
    "                )\n",
    "                rating = -1\n",
    "\n",
    "        elif not rating_tab_exists:\n",
    "            print(\n",
    "                f\"{list(tab_name.keys())[1]} tab doesn't exist. Moving on to next tab.\"\n",
    "            )\n",
    "            rating = -1\n",
    "\n",
    "    # Save to dict\n",
    "    print(\"Saving to dict...\")\n",
    "    today = datetime.date.today()\n",
    "    jobs.append(\n",
    "        {\n",
    "            \"Search Keyword\": str(keyword),\n",
    "            \"Platform\": site,\n",
    "            \"Job ID\": int(job_id),\n",
    "            \"Job Title\": str(job_title),\n",
    "            \"Company Name\": str(company_name),\n",
    "            \"Location\": str(location),\n",
    "            \"Industry\": str(industry),\n",
    "            \"Sector\": str(sector),\n",
    "            \"Job Description\": str(job_description),\n",
    "            \"Type of ownership\": str(company_type),\n",
    "            \"Rating\": float(rating),\n",
    "            \"Company URL\": company_url,\n",
    "            \"Job URL\": job_url,\n",
    "            \"Job Age\": job_age,\n",
    "            \"Job Age Number\": job_age_number,\n",
    "            \"Collection Date\": (today.strftime(\"%Y-%m-%d\")),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Save dict as json file\n",
    "    jobs = remove_dupe_dicts(jobs)\n",
    "    try:\n",
    "        print(\n",
    "            f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)} / {int(jobs_count)*int(page_number_length)}\"\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)}\")\n",
    "    print(f\"Saving dict to json file as {json_file_name} in location {save_path}\")\n",
    "    with open(save_path + json_file_name, \"w\") as f:\n",
    "        json.dump(jobs, f)\n",
    "\n",
    "    # Save page counter\n",
    "    if page_counter > 0:\n",
    "        with open(\n",
    "            save_path + f\"{site.lower()}_page_counter_{keyword_file}.txt\", \"w\"\n",
    "        ) as f:\n",
    "            f.write(str(page_counter))\n",
    "\n",
    "    # Check current window and go back if required\n",
    "    check_window_back(driver, main_window, window_before, new_window)\n",
    "\n",
    "    return jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get main job data\n",
    "def get_main(driver, soup, job_soup):\n",
    "\n",
    "    pattern = re.compile(r\"[^A-Za-z0-9, ]+\")\n",
    "    WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.ID, \"JDCol\")))\n",
    "\n",
    "    try:\n",
    "        job_title = driver.find_element_by_xpath(\n",
    "            '//*[@id=\"MainCol\"]/div[1]/ul/li[1]/div[2]/a/span'\n",
    "        ).text.strip()\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job title found.\")\n",
    "        job_title = -1\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting job title: {sys.exc_info()[0]}.\")\n",
    "        job_title = -1\n",
    "    else:\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Job Title: {str(job_title)}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    try:\n",
    "        job_description_list = []\n",
    "        job_desc = (\n",
    "            soup.find(\"div\", id=\"JobDescriptionContainer\")\n",
    "            .find(\"div\")\n",
    "            .find(\"div\", class_=\"jobDescriptionContent desc\")\n",
    "            .find_all(\"div\")\n",
    "        )\n",
    "        if len(job_desc) == 0:\n",
    "            job_desc_pattern = re.compile(r\"JobDesc\\d+\")\n",
    "            job_desc = driver.find_element_by_xpath(\n",
    "                '//*[@id=\"JobDescriptionContainer\"]'\n",
    "            ).find_elements_by_xpath(f'//*[@id=\"{job_desc_pattern}\"]/div')\n",
    "        for job_d in job_desc:\n",
    "            job_description_list.append(job_d.text.strip())\n",
    "        job_description = \"\\n\".join(job_description_list)\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job description found.\")\n",
    "        job_description = -1\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting job description: {sys.exc_info()[0]}.\")\n",
    "        job_description = -1\n",
    "    else:\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Job Description: {str(job_description[:50])}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    try:\n",
    "        company_name = (\n",
    "            job_soup.find(\n",
    "                \"div\", class_=\"d-flex flex-column pl-sm css-1d3xmk8 e1rrn5ka4\"\n",
    "            )\n",
    "            .find(\"div\", class_=\"d-flex justify-content-between align-items-start\")\n",
    "            .find(\"a\", class_=\"css-l2wjgv e1n63ojh0 jobLink\")\n",
    "            .find(\"span\")\n",
    "            .text.strip()\n",
    "        )\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job company name found.\")\n",
    "        company_name = -1\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting company name: {sys.exc_info()[0]}.\")\n",
    "        company_name = -1\n",
    "    else:\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Company Name: {company_name}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    try:\n",
    "        location = (\n",
    "            job_soup.find(\n",
    "                \"div\", class_=\"d-flex flex-column pl-sm css-1d3xmk8 e1rrn5ka4\"\n",
    "            )\n",
    "            .find(\"div\", class_=\"d-flex flex-wrap css-11d3uq0 e1rrn5ka2\")\n",
    "            .find(\"span\")\n",
    "            .text.strip()\n",
    "        )\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job company location found.\")\n",
    "        location = -1\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting company location: {sys.exc_info()[0]}.\")\n",
    "        location = -1\n",
    "    else:\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Company Location: {str(location)}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    try:\n",
    "        company_url = \"https://www.glassdoor.nl/\" + str(\n",
    "            driver.find_element_by_xpath(\n",
    "                '//*[@id=\"JDCol\"]/div/article/div/div[1]/div/div/div[1]/div[3]/div[2]/div/div[1]/div[1]/a'\n",
    "            ).get_attribute(\"href\")\n",
    "        )\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job company url found.\")\n",
    "        company_url = -1\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting company url: {sys.exc_info()[0]}.\")\n",
    "        company_url = -1\n",
    "    else:\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Company URL: {company_url}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    # Check if collection successful\n",
    "    imp_collected = [job_title, job_description, company_name, location, company_url]\n",
    "    if all((var != int(-1)) or (var != str(\"-1\")) for var in imp_collected):\n",
    "        print(\"Main data collected successfully.\")\n",
    "\n",
    "    elif any((var == int(-1)) or (var == str(\"-1\")) for var in imp_collected):\n",
    "        print(\n",
    "            f\"Main data NOT collected successfully. Sleeping for 10 seconds then trying again.\"\n",
    "        )\n",
    "        time.sleep(10)\n",
    "        try:\n",
    "            job_title, job_description, company_name, location, company_url = get_main(\n",
    "                driver, soup\n",
    "            )\n",
    "        except:\n",
    "            print(\n",
    "                f\"Main data NOT collected successfully. Not attempting again. Collecting remaining data: {sys.exc_info()[0]}.\"\n",
    "            )\n",
    "\n",
    "    return job_title, job_description, company_name, location, company_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_page_jobs(\n",
    "    driver,\n",
    "    soup,\n",
    "    keyword,\n",
    "    keyword_file,\n",
    "    site,\n",
    "    main_window,\n",
    "    window_before,\n",
    "    page_counter,\n",
    "    page_number,\n",
    "    page_number_length,\n",
    "    jobs,\n",
    "    df_old_jobs,\n",
    "    save_path,\n",
    "    json_file_name,\n",
    "):\n",
    "\n",
    "    # Get html of page with beautifulsoup4\n",
    "    html = driver.page_source\n",
    "    print(\"Feeding html driver to BeautifulSoup.\")\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # try:\n",
    "    # WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'SearchForm')))\n",
    "    try:\n",
    "        jobs_on_page = (\n",
    "            soup.find(\"div\", id=\"JobResults\")\n",
    "            .find(\"article\", id=\"MainCol\")\n",
    "            .find(\"ul\", class_=\"hover p-0 css-7ry9k1 exy0tjh5\")\n",
    "            .find_all(\"li\")\n",
    "        )\n",
    "        jobs_count = len(jobs_on_page)\n",
    "    except AttributeError as e:\n",
    "        print(\n",
    "            f\"Unexpected error with job links: {e} - {sys.exc_info()[0]}.\\nAttempting .children method.\"\n",
    "        )\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable(\n",
    "                (\n",
    "                    By.CSS_SELECTOR,\n",
    "                    \"#MainCol > div:nth-child(1) > ul > li.react-job-listing.css-7x0jr.eigr9kq3\",\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        jobs_grid = soup.find_all(\"ul\", class_=\"hover p-0  css-7ry9k1 exy0tjh5\")\n",
    "        jobs_on_page = [job.children for job in jobs_grid]\n",
    "        jobs_count = cardinality.count(jobs_on_page)\n",
    "\n",
    "    if int(jobs_count) != 0:\n",
    "        print(\n",
    "            f\"{int(jobs_count)} job(s) found on page {page_number} out of {page_number_length} page(s).\"\n",
    "        )\n",
    "        try:\n",
    "            print(\n",
    "                f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)} / {int(jobs_count)*int(page_number_length)}\"\n",
    "            )\n",
    "        except:\n",
    "            print(f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)}\")\n",
    "\n",
    "        for job_soup in jobs_on_page:\n",
    "            print(\"-\" * 20)\n",
    "            print(\n",
    "                f\"JOB NUMBER: {int(jobs_on_page.index(job_soup)) + 1} OUT OF {int(jobs_count)} ON PAGE {page_number} OUT OF {page_number_length}.\"\n",
    "            )\n",
    "            page_counter = page_number\n",
    "            if page_counter > 0:\n",
    "                with open(\n",
    "                    save_path + f\"{site.lower()}_page_counter_{keyword_file}.txt\", \"w\"\n",
    "                ) as f:\n",
    "                    f.write(str(page_counter))\n",
    "            print(\"-\" * 20)\n",
    "            job_xpath = xpath_soup(job_soup)\n",
    "            job_button = WebDriverWait(driver, 10).until(\n",
    "                lambda driver: driver.find_element_by_xpath(job_xpath)\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                job_button.click()\n",
    "            except errors:\n",
    "                print(\n",
    "                    f\"Click did not work. Trying again to get job soup for {str(keyword)}.\"\n",
    "                )\n",
    "                # Get html of page with beautifulsoup4\n",
    "                # html = driver.page_source\n",
    "                # print('Feeding html driver to BeautifulSoup.')\n",
    "                # soup = BeautifulSoup(html, 'lxml')\n",
    "                job_button.click()\n",
    "\n",
    "            # Get job id\n",
    "            job_id = int(job_soup[\"data-id\"])\n",
    "\n",
    "            # Check if already collected\n",
    "            jobs, job_present = id_check(\n",
    "                driver,\n",
    "                main_window,\n",
    "                window_before,\n",
    "                keyword,\n",
    "                site,\n",
    "                df_old_jobs,\n",
    "                jobs,\n",
    "                job_soup,\n",
    "                job_id,\n",
    "                jobs_count,\n",
    "                save_path,\n",
    "                json_file_name,\n",
    "            )\n",
    "            if job_present is False:\n",
    "                jobs = get_each(\n",
    "                    driver,\n",
    "                    main_window,\n",
    "                    window_before,\n",
    "                    keyword,\n",
    "                    keyword_file,\n",
    "                    site,\n",
    "                    jobs,\n",
    "                    job_soup,\n",
    "                    job_id,\n",
    "                    jobs_count,\n",
    "                    page_counter,\n",
    "                    page_number,\n",
    "                    page_number_length,\n",
    "                    save_path,\n",
    "                    json_file_name,\n",
    "                )\n",
    "            elif job_present is True:\n",
    "                pass\n",
    "\n",
    "    elif int(jobs_count) == 0:\n",
    "        print(\n",
    "            f\"No jobs results found for {str(keyword)}. No error detected. Moving on to next search.\"\n",
    "        )\n",
    "\n",
    "    # except:\n",
    "    #     print(f'No jobs results found for {str(keyword)}: {sys.exc_info()[0]}. Moving on to next search.')\n",
    "\n",
    "    # Save dict as json file\n",
    "    jobs = remove_dupe_dicts(jobs)\n",
    "    print(\"-\" * 20)\n",
    "    try:\n",
    "        print(\n",
    "            f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)} / {int(jobs_count)*int(page_number_length)}\"\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)}\")\n",
    "    print(f\"Saving dict to json file as {json_file_name} in location {save_path}\")\n",
    "    print(\"-\" * 20)\n",
    "    with open(save_path + json_file_name, \"w\") as f:\n",
    "        json.dump(jobs, f)\n",
    "\n",
    "    # End of page\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"REACHED END OF JOBS FOR PAGE: {page_number}.\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # Save page counter\n",
    "    page_counter += 1\n",
    "    if page_counter > 0:\n",
    "        with open(\n",
    "            save_path + f\"{site.lower()}_page_counter_{keyword_file}.txt\", \"w\"\n",
    "        ) as f:\n",
    "            f.write(str(page_counter))\n",
    "\n",
    "    # Save dict as json file\n",
    "    jobs = remove_dupe_dicts(jobs)\n",
    "    try:\n",
    "        print(\n",
    "            f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)} / {int(jobs_count)*int(page_number_length)}\"\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)}\")\n",
    "    print(f\"Saving dict to json file as {json_file_name} in location {save_path}\")\n",
    "    with open(save_path + json_file_name, \"w\") as f:\n",
    "        json.dump(jobs, f)\n",
    "\n",
    "    # Save the old page counter\n",
    "    if page_counter > 0:\n",
    "        with open(\n",
    "            save_path + f\"{site.lower()}_page_counter_{keyword_file}.txt\", \"w\"\n",
    "        ) as f:\n",
    "            f.write(str(page_counter))\n",
    "\n",
    "    print(f\"End of data collection for {keyword}.\\nGot {len(jobs)} jobs.\")\n",
    "\n",
    "    # Save dict as json file\n",
    "    jobs = remove_dupe_dicts(jobs)\n",
    "    print(\"-\" * 20)\n",
    "    try:\n",
    "        print(\n",
    "            f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)} / {int(jobs_count)*int(page_number_length)}\"\n",
    "        )\n",
    "    except:\n",
    "        jobs_count = len(jobs)\n",
    "        print(\n",
    "            f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)} / {int(jobs_count)*int(page_number_length)}\"\n",
    "        )\n",
    "    print(f\"Saving dict to json file as {json_file_name} in location {save_path}\")\n",
    "    print(\"-\" * 20)\n",
    "    with open(save_path + json_file_name, \"w\") as f:\n",
    "        json.dump(jobs, f)\n",
    "\n",
    "    return jobs, jobs_count, page_counter, page_number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get-jobs function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# MAIN FUNCTION TO GET JOB ADS\n",
    "def get_jobs(\n",
    "    driver,\n",
    "    keyword,\n",
    "    keyword_url,\n",
    "    keyword_file,\n",
    "    site,\n",
    "    country,\n",
    "    page_counter,\n",
    "    main_window,\n",
    "    df_old_jobs,\n",
    "    jobs,\n",
    "    save_path,\n",
    "    json_file_name,\n",
    "    df_file_name,\n",
    "):\n",
    "\n",
    "    url = f\"https://www.glassdoor.nl/sitedirectory/title-jobs.htm\"\n",
    "    jobs_count = len(jobs)\n",
    "\n",
    "    # Go to page and establish main windows\n",
    "    print(f\"Going to url: {url}\")\n",
    "    driver.get(url)\n",
    "    window_before = driver.window_handles[0]\n",
    "    print(f\"First Window: {window_before}\")\n",
    "    print(f\"Current page title: {driver.title}\")\n",
    "\n",
    "    # Get html of page with beautifulsoup4\n",
    "    html = driver.page_source\n",
    "    print(\"Feeding html driver to BeautifulSoup.\")\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # Input keyword in search bar\n",
    "    try:\n",
    "        print(f\"Searching for {keyword.upper()} job ads.\")\n",
    "        # Get html of page with beautifulsoup4\n",
    "        # html = driver.page_source\n",
    "        # print('Feeding html driver to BeautifulSoup.')\n",
    "        # soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        search_bar = WebDriverWait(driver, 10).until(\n",
    "            lambda driver: driver.find_element_by_id(\"sc.keyword\")\n",
    "        )\n",
    "        search_bar.send_keys([keyword])\n",
    "        search_bar.submit()\n",
    "        search_button = WebDriverWait(\n",
    "            driver, 10, ignored_exceptions=StaleElementReferenceException\n",
    "        ).until(EC.presence_of_element_located((By.ID, \"HeroSearchButton\")))\n",
    "        driver.execute_script(\"arguments[0].click();\", search_button)\n",
    "    except errors:\n",
    "        print(\n",
    "            f\"Unexpected error with search: {sys.exc_info()[0]}.\\nAttempting search again...\"\n",
    "        )\n",
    "\n",
    "        # Get html of page with beautifulsoup4\n",
    "        # html = driver.page_source\n",
    "        # print('Feeding html driver to BeautifulSoup.')\n",
    "        # soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        print(\"Clicking search button manually.\")\n",
    "        try:\n",
    "            search_bar.send_keys(Keys.ENTER)\n",
    "        except TimeoutException:\n",
    "            search_button = WebDriverWait(\n",
    "                driver, 10, ignored_exceptions=StaleElementReferenceException\n",
    "            ).until(EC.presence_of_element_located((By.ID, \"HeroSearchButton\")))\n",
    "            driver.execute_script(\"arguments[0].click();\", search_button)\n",
    "        except StaleElementReferenceException:\n",
    "            pyautogui.click(1170, 230)\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.ID, \"PageContent\"))\n",
    "            )\n",
    "        except errors:\n",
    "            print(\n",
    "                f\"Unexpected error with clicking the search button: {sys.exc_info()[0]}.\"\n",
    "            )\n",
    "            pyautogui.click(1170, 190)\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.ID, \"PageContent\"))\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if page_counter > 1:\n",
    "        url = (f\"_IP{page_counter}\" + str(\".htm\")).join(\n",
    "            str(driver.current_url).split(str(\".htm\"))\n",
    "        )\n",
    "        # Go to page and establish main windows\n",
    "        print(f\"Going to url: {url}\")\n",
    "        driver.get(url)\n",
    "        window_before = driver.window_handles[0]\n",
    "        print(f\"First Window: {window_before}\")\n",
    "        print(f\"Current page title: {driver.title}\")\n",
    "\n",
    "        # Get html of page with beautifulsoup4\n",
    "        # html = driver.page_source\n",
    "        # print('Feeding html driver to BeautifulSoup.')\n",
    "        # soup = BeautifulSoup(html, 'lxml')\n",
    "    elif page_counter <= 1:\n",
    "        pass\n",
    "\n",
    "    # Find selected country and make sure country is Netherlands\n",
    "    soup = find_country(driver, soup, country)\n",
    "\n",
    "    # Find page number\n",
    "    page_number_length, page_number = find_page_number(\n",
    "        driver, soup, page_counter, keyword\n",
    "    )\n",
    "\n",
    "    # Check for errors\n",
    "    try:\n",
    "        not_page_error = WebDriverWait(driver, 10).until_not(\n",
    "            lambda driver: driver.find_element_by_class_name(\"noResults\")\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        print(f\"No jobs results found for {str(keyword)}. Moving on to next search.\")\n",
    "    else:\n",
    "        while not_page_error:\n",
    "            # Find job button next siblings with bs4\n",
    "            print(\"Finding job links in page.\")\n",
    "            jobs, jobs_count, page_counter, page_number = get_page_jobs(\n",
    "                driver,\n",
    "                soup,\n",
    "                keyword,\n",
    "                keyword_file,\n",
    "                site,\n",
    "                main_window,\n",
    "                window_before,\n",
    "                page_counter,\n",
    "                page_number,\n",
    "                page_number_length,\n",
    "                jobs,\n",
    "                df_old_jobs,\n",
    "                save_path,\n",
    "                json_file_name,\n",
    "            )\n",
    "\n",
    "    #             # Click on next page button\n",
    "    #             try:\n",
    "    #                 if int(page_number_length) == int(page_number):\n",
    "    #                     print(f'Last page reached at page {page_number}. End of data collection for {keyword}.\\nGot {len(jobs)} jobs.')\n",
    "    #                     break\n",
    "    #                 elif int(page_number_length) != int(page_number):\n",
    "    #                     try:\n",
    "    #                         driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "    #                         next_page_button = WebDriverWait(driver, 10).until_not(lambda driver: driver.find_element_by_xpath('//*[@id=\"FooterPageNav\"]/div/ul/li[7]/a'))\n",
    "    #                     except (AttributeError, TimeoutException):\n",
    "    #                         driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "    #                         # Get html of page with beautifulsoup4\n",
    "    #                         html = driver.page_source\n",
    "    #                         print('Feeding html driver to BeautifulSoup.')\n",
    "    #                         soup = BeautifulSoup(html, 'lxml')\n",
    "    #                         next_page_button = WebDriverWait(driver, 10).until_not(lambda driver: driver.find_element_by_xpath(xpath_soup(soup.find('a', attrs={'data-test': 'pagination-next'}))))\n",
    "    #                     except:\n",
    "    #                         next_page_button = WebDriverWait(driver, 10).until_not(lambda driver: driver.find_element_by_xpath(f'//*[@id=\"FooterPageNav\"]/div/ul/li[{str(int(page_number) + 2)}]/a'))\n",
    "    #                     finally:\n",
    "    #                         if not isinstance(next_page_button, bool):\n",
    "    #                             pass\n",
    "    #                         elif isinstance(next_page_button, bool):\n",
    "    #                             print('Next button is boolean. Clicking using page number.')\n",
    "    #                             next_page_button = WebDriverWait(driver, 10).until_not(lambda driver: driver.find_element_by_xpath(f'//*[@id=\"FooterPageNav\"]/div/ul/li[{str(int(page_number) + 1)}]/a'))\n",
    "    #                         next_page_button.click()\n",
    "    #                         print(f'Going to next page {int(page_number)+1}.\\nGot {len(jobs)} jobs.')\n",
    "    #                     # Find page number\n",
    "    #                     page_number_length, page_number = find_page_number(driver, soup, page_counter, keyword)\n",
    "\n",
    "    #                     # Get jobs\n",
    "    #                     jobs, jobs_count, page_counter, page_number = get_page_jobs(driver, soup, keyword, keyword_file, site, main_window, window_before, page_counter, page_number, page_number_length, jobs, df_old_jobs, save_path, json_file_name)\n",
    "    #             except (NoSuchWindowException, NoSuchElementException):\n",
    "    #                 print(f'Could not go to next page. End of data collection for {keyword}.\\nGot {len(jobs)} jobs.')\n",
    "\n",
    "    ############################################### FINAL SAVING AND CLEANUP ###############################################\n",
    "\n",
    "    # Save dict as json file\n",
    "    print(\"Final save.\")\n",
    "    jobs = remove_dupe_dicts(jobs)\n",
    "    try:\n",
    "        print(\n",
    "            f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)} / {int(jobs_count)*int(page_number_length)}\"\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)}\")\n",
    "    print(f\"Saving dict to json file as {json_file_name} in location {save_path}\")\n",
    "    with open(save_path + json_file_name, \"w\") as f:\n",
    "        json.dump(jobs, f)\n",
    "\n",
    "    # Save jobs to df\n",
    "    print(f\"Saving {keyword} jobs data to df...\")\n",
    "    if jobs:\n",
    "        df_jobs = pd.DataFrame(jobs)\n",
    "    elif not jobs:\n",
    "        print(f\"Jobs dict for {keyword} is empty.\")\n",
    "        df_jobs = pd.DataFrame()\n",
    "\n",
    "    # Save df as csv\n",
    "    if not df_jobs.empty:\n",
    "        df_jobs.append(df_old_jobs, ignore_index=True)\n",
    "        df_jobs = clean_df(df_jobs)\n",
    "        df_jobs = save_df(\n",
    "            keyword, df_jobs, save_path, keyword_file.lower(), df_file_name.lower()\n",
    "        )\n",
    "    elif df_jobs.empty:\n",
    "        df_jobs = df_old_jobs\n",
    "\n",
    "    return jobs, jobs_count, df_jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run get_jobs function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "def run_all_glassdoor(\n",
    "    site=\"Glassdoor\",\n",
    "    language=\"English\",\n",
    "    country=\"Nederland\",\n",
    "    incognito_enabled=True,\n",
    "    headless_enabled=False,\n",
    "    proxy_enabled=False,\n",
    "    logging_enabled=False,\n",
    "    print_enabled=True,\n",
    "):\n",
    "\n",
    "    # Get keywords\n",
    "    (\n",
    "        keywords_list,\n",
    "        keywords_sector,\n",
    "        keywords_womenvocc,\n",
    "        keywords_menvocc,\n",
    "        keywords_genvsect,\n",
    "        keywords_oldvocc,\n",
    "        keywords_youngvocc,\n",
    "        keywords_agevsect,\n",
    "    ) = get_keyword_list(print_enabled=print_enabled)\n",
    "    # remove_list = ['primary school teacher', 'selling real estate', 'physiotherapist']\n",
    "    # for k in remove_list:\n",
    "    #     keywords_list.remove(k)\n",
    "\n",
    "    # Start driver\n",
    "    driver = get_driver(select_driver)\n",
    "\n",
    "    # Start for-loop\n",
    "    #     keywords_oldvocc = ['industry']\n",
    "    keywords_list = keywords_oldvocc + keywords_youngvocc\n",
    "    for keyword in keywords_youngvocc:\n",
    "        recollect = False\n",
    "\n",
    "        # Check internet connection\n",
    "        if not is_connected(driver):\n",
    "            is_connected(driver)\n",
    "\n",
    "        # Get main info\n",
    "        (\n",
    "            keyword_url,\n",
    "            keyword_file,\n",
    "            save_path,\n",
    "            json_file_name,\n",
    "            df_file_name,\n",
    "            logs_file_name,\n",
    "            filemode,\n",
    "        ) = main_info(keyword, site)\n",
    "\n",
    "        # Set up log\n",
    "        if logging_enabled is True:\n",
    "            print(\"Logging enabled.\")\n",
    "            writer = MyWriter(logs_file_name.lower(), filemode)\n",
    "            sys.stdout = writer\n",
    "            logger = logging.getLogger(__name__)\n",
    "            logging.basicConfig(\n",
    "                filename=writer.LOGS_PATH + logs_file_name.lower(),\n",
    "                filemode=filemode,\n",
    "                level=logging.CRITICAL,\n",
    "                format=\"%(asctime)s:%(levelname)s: %(message)s\",\n",
    "            )\n",
    "        elif logging_enabled is False:\n",
    "            print(\"No logging enabled.\")\n",
    "\n",
    "        # Load and merge existing dict and df\n",
    "        print(f\"All files will be saved in folder: {save_path}{json_file_name}\")\n",
    "        jobs, df_old_jobs = load_merge_dict_df(\n",
    "            keyword, save_path, df_file_name, json_file_name\n",
    "        )\n",
    "\n",
    "        # Beging data collection\n",
    "        print(\n",
    "            \"-\" * 20,\n",
    "            f\"BEGINING OF DATA COLLECTION FOR {keyword.upper()} JOB ADS\",\n",
    "            \"-\" * 20,\n",
    "        )\n",
    "        main_window = driver.current_window_handle\n",
    "        print(f\"Main Window: {main_window}\")\n",
    "\n",
    "        # Load and save page_counter\n",
    "        page_counter_path = (\n",
    "            save_path + f\"{site.lower()}_page_counter_{keyword_file}.txt\"\n",
    "            if recollect is False\n",
    "            else save_path + f\"{site.lower()}_page_counter_done_{keyword_file}.txt\"\n",
    "        )\n",
    "\n",
    "        if is_non_zero_file(page_counter_path) is True:\n",
    "            with open(page_counter_path) as f:\n",
    "                page_counter = int(f.read())\n",
    "        elif is_non_zero_file(page_counter_path) is False:\n",
    "            page_counter = 1\n",
    "        with open(page_counter_path, \"w\") as f:\n",
    "            f.write(str(page_counter))\n",
    "\n",
    "        print(f\"Starting collection from page number: {page_counter}\")\n",
    "\n",
    "        # Start threads\n",
    "        popup_thread(driver, popup, popup_checker)\n",
    "        # stale_element_thread(driver, stale_element, stale_element_checker)\n",
    "        # act_cool_thread(driver, act_cool, act_cool_checker)\n",
    "\n",
    "        # Get jobs df\n",
    "        jobs, jobs_count, df_jobs = get_jobs(\n",
    "            driver,\n",
    "            keyword.lower(),\n",
    "            keyword_url,\n",
    "            keyword_file,\n",
    "            site,\n",
    "            country,\n",
    "            page_counter,\n",
    "            main_window,\n",
    "            df_old_jobs,\n",
    "            jobs,\n",
    "            save_path.lower(),\n",
    "            json_file_name.lower(),\n",
    "            df_file_name.lower(),\n",
    "        )\n",
    "\n",
    "        # Save df as csv\n",
    "        df_jobs = save_df(\n",
    "            keyword, df_jobs, save_path, keyword_file.lower(), df_file_name.lower()\n",
    "        )\n",
    "        print(\n",
    "            \"-\" * 20, f\"END OF DATA COLLECTION FOR {keyword.upper()} JOB ADS\", \"-\" * 20\n",
    "        )\n",
    "\n",
    "    # Close and quit driver\n",
    "    print(f\"Closing driver.\")\n",
    "    driver.close()\n",
    "    driver.quit()\n",
    "    # thread_popup_checker.terminate()\n",
    "    # thread_stale_element_checker.terminate()\n",
    "    # thread_act_cool_checker.terminate()\n",
    "    # sys.stdout.close()\n",
    "    # sys.exit()\n",
    "\n",
    "    return df_jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_jobs_glassdoor = run_all_glassdoor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword = 'artsen'\n",
    "# site = 'Glassdoor'\n",
    "# keyword_from_list = True\n",
    "# site_from_list = False\n",
    "# df_jobs = post_cleanup(keyword, site)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "metadata": {
   "interpreter": {
    "hash": "e774977668b7c0ae8309835a5187aa7fbf7669e7d0bb59755bc63e573643edcd"
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
