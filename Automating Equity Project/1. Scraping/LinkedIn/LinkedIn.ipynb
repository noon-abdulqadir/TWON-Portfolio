{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "if code_dir_name not in str(Path.cwd()).split('/')[-1]:\n",
    "    for _ in range(5):\n",
    "\n",
    "        parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "        if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "            code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "            if code_dir is not None:\n",
    "                break\n",
    "else:\n",
    "    code_dir = str(Path.cwd())\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Function to check for pop-ups\n",
    "def popup(driver):\n",
    "    # Get html of page with beautifulsoup4\n",
    "    html = driver.page_source\n",
    "    print(\"Feeding html driver to BeautifulSoup.\")\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # Messaging popup\n",
    "    try:\n",
    "        messaging_soup = soup.find(\"li-icon\", attrs={\"type\": \"chevron-down-icon\"})\n",
    "        messaging_xpath = xpath_soup(messaging_soup)\n",
    "        message_button = WebDriverWait(driver, 10).until(\n",
    "            lambda driver: driver.find_element_by_xpath(messaging_soup)\n",
    "        )\n",
    "        if (message_button.is_displayed()) and (message_button.is_enabled()):\n",
    "            print(\"Message popup detected.\")\n",
    "            message_button.click()\n",
    "    except errors:\n",
    "\n",
    "        # Accept cookies\n",
    "        try:\n",
    "            cookie_button = WebDriverWait(driver, 10).until(\n",
    "                lambda driver: driver.find_element_by_css_selector(\n",
    "                    \"#artdeco-global-alert-container > div.artdeco-global-alert.artdeco-global-alert--NOTICE.artdeco-global-alert--COOKIE_CONSENT > section > div > div.artdeco-global-alert-action__wrapper > button:nth-child(2)\"\n",
    "                )\n",
    "            )\n",
    "            if (cookie_button.is_displayed()) and (cookie_button.is_enabled()):\n",
    "                cookie_button.click()\n",
    "        except errors:\n",
    "\n",
    "            # Accept Alert\n",
    "            try:\n",
    "                click_alert = driver.switch_to.alert()\n",
    "                print(\"Alert found.\")\n",
    "                click_alert.accept()\n",
    "            except errors:\n",
    "                pass\n",
    "            except:\n",
    "                try:\n",
    "                    print(f\"Alert caused unexpected error: {sys.exc_info()[0]}\")\n",
    "                except (NoSuchWindowException, urllib3.exceptions.MaxRetryError):\n",
    "                    pass\n",
    "            else:\n",
    "                print(\"Alert accepted.\")\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable(\n",
    "                        (By.CSS_SELECTOR, \"body > div.cta-modal.show > button\")\n",
    "                    )\n",
    "                )\n",
    "        except:\n",
    "            try:\n",
    "                print(f\"Cookies alert caused unexpected error: {sys.exc_info()[0]}\")\n",
    "            except (NoSuchWindowException, urllib3.exceptions.MaxRetryError):\n",
    "                pass\n",
    "        else:\n",
    "            print(\"Cookies accepted.\")\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable(\n",
    "                    (\n",
    "                        By.CSS_SELECTOR,\n",
    "                        \"body > header > nav > section > section:nth-child(4) > form > button\",\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "    except:\n",
    "        try:\n",
    "            print(f\"Message popup caused unexpected error: {sys.exc_info()[0]}\")\n",
    "        except (NoSuchWindowException, urllib3.exceptions.MaxRetryError):\n",
    "            pass\n",
    "    else:\n",
    "        print(\"Message popup dismissed.\")\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable(\n",
    "                (\n",
    "                    By.CSS_SELECTOR,\n",
    "                    \"body > header > nav > section > section:nth-child(4) > form > button\",\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sign in\n",
    "def sign_in(driver, username=None, password=None):\n",
    "    if username is None:\n",
    "        os.environ.get('YAHOO_EMAIL')\n",
    "    if password is None:\n",
    "        os.environ.get('YAHOO_PASSWORD')\n",
    "    print('Entering username and password.')\n",
    "\n",
    "    try:\n",
    "        username_textbox = WebDriverWait(driver, 10).until(\n",
    "            lambda driver: driver.find_element_by_id(\"login-email\")\n",
    "        )\n",
    "    except:\n",
    "        username_textbox = WebDriverWait(driver, 10).until(\n",
    "            lambda driver: driver.find_element_by_id(\"username\")\n",
    "        )\n",
    "    try:\n",
    "        password_textbox = WebDriverWait(driver, 10).until(\n",
    "            lambda driver: driver.find_element_by_id(\"login-password\")\n",
    "        )\n",
    "    except:\n",
    "        password_textbox = WebDriverWait(driver, 10).until(\n",
    "            lambda driver: driver.find_element_by_id(\"password\")\n",
    "        )\n",
    "\n",
    "    username_textbox.send_keys(username)\n",
    "    password_textbox.send_keys(password)\n",
    "    password_textbox.send_keys(Keys.ENTER)\n",
    "\n",
    "    # Get html of page with beautifulsoup4\n",
    "    html = driver.page_source\n",
    "    print(\"Feeding html driver to BeautifulSoup.\")\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    time.sleep(2)\n",
    "    print(f\"Current page title: {driver.title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find page number\n",
    "def find_page_number(driver, soup, page_counter, keyword):\n",
    "    # Get html of page with beautifulsoup4\n",
    "    html = driver.page_source\n",
    "    print(\"Feeding html driver to BeautifulSoup.\")\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            page_footer = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (By.CLASS_NAME, \"jobs-search-two-pane__pagination\")\n",
    "                )\n",
    "            )\n",
    "        except:\n",
    "            page_footer_soup = (\n",
    "                soup.find(\"section\", {\"aria-label\": \"pagination\"})\n",
    "                .find(\"div\", class_=\"pv5 artdeco-pagination ember-view\")\n",
    "                .find(\n",
    "                    \"ul\",\n",
    "                    class_=\"artdeco-pagination__pages artdeco-pagination__pages--number\",\n",
    "                )\n",
    "            )\n",
    "            page_footer_xpath = xpath_soup(page_footer_soup)\n",
    "            page_footer = WebDriverWait(driver, 10).until(\n",
    "                lambda driver: driver.find_element_by_class_name(page_footer_xpath)\n",
    "            )\n",
    "        if page_footer:\n",
    "            page_number_length = driver.find_elements_by_class_name(\n",
    "                \"artdeco-pagination__pages artdeco-pagination__pages--number\"\n",
    "            )\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "            time.sleep(2)\n",
    "            page_number_length = soup.find(\n",
    "                \"section\", class_=\"jobs-search-two-pane__pagination\"\n",
    "            )  # .find('div', class_ = 'pv5 artdeco-pagination ember-view').find('ul', class_ = 'artdeco-pagination__pages artdeco-pagination__pages--number')\n",
    "            print(\"page_number_length: \", page_number_length)\n",
    "            page_number_length = (\n",
    "                page_number_length[-1].find_element_by_class_name(\"button\").text.strip()\n",
    "            )\n",
    "            if int(page_number_length) == 1:\n",
    "                print(f\"Only 1 page found for {str(keyword)} jobs.\")\n",
    "                page_number = page_number_length\n",
    "                page_number = [int(s) for s in page_number.split() if s.isdigit()][0]\n",
    "                print(f\"Current page number: {page_number}.\")\n",
    "            elif int(page_number_length) != 1:\n",
    "                print(\n",
    "                    f\"Total of {page_number_length} pages found for {str(keyword)} jobs.\"\n",
    "                )\n",
    "                try:\n",
    "                    page_number = (\n",
    "                        soup.find(\n",
    "                            \"li\",\n",
    "                            class_=\"artdeco-pagination__indicator artdeco-pagination__indicator--number active selected ember-view\",\n",
    "                        )\n",
    "                        .find(\"button\", {\"aria-current\": \"true\"})\n",
    "                        .text.strip()\n",
    "                    )\n",
    "                except:\n",
    "                    page_number = (\n",
    "                        soup.find(\n",
    "                            \"li\",\n",
    "                            class_=\"artdeco-pagination__indicator artdeco-pagination__indicator--number active selected ember-view\",\n",
    "                        )\n",
    "                        .find(\n",
    "                            \"button\", attrs={\"aria-label\": re.compile(\"*current page\")}\n",
    "                        )\n",
    "                        .text.strip()\n",
    "                    )\n",
    "                page_number = [int(s) for s in page_number.split() if s.isdigit()][0]\n",
    "                print(f\"Current page number: {page_number}.\")\n",
    "\n",
    "        elif not page_footer:\n",
    "            page_number = page_counter\n",
    "            page_number_length = 1\n",
    "            print(\n",
    "                f\"No page number indicator found. Page number set to counter: {page_number}\"\n",
    "            )\n",
    "    except (NoSuchElementException, TimeoutException):\n",
    "        page_number = page_counter\n",
    "        page_number_length = 1\n",
    "        print(\n",
    "            f\"No page number indicator found. Page number set to counter: {page_number}\"\n",
    "        )\n",
    "    except errors:\n",
    "        page_number = page_counter\n",
    "        page_number_length = -1\n",
    "        print(f\"Unexpected error getting page number: {sys.exc_info()[0]}.\")\n",
    "\n",
    "    return page_number_length, page_number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find and click load more button\n",
    "def load_more(\n",
    "    driver,\n",
    "    soup,\n",
    "    keyword,\n",
    "    keyword_file,\n",
    "    site,\n",
    "    main_window,\n",
    "    window_before,\n",
    "    counter,\n",
    "    page_counter,\n",
    "    page_number,\n",
    "    page_number_length,\n",
    "    jobs,\n",
    "    jobs_count,\n",
    "    df_old_jobs,\n",
    "    save_path,\n",
    "    json_file_name,\n",
    "):\n",
    "    try:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "        time.sleep(2)\n",
    "        load_more_xpath = xpath_soup(\n",
    "            soup.find(\n",
    "                \"section\", class_=\"m3\", attrs={\"aria-label\": \"Load more results\"}\n",
    "            ).find(\"button\")\n",
    "        )\n",
    "        load_more_button = WebDriverWait(driver, 10).until(\n",
    "            lambda driver: driver.find_element_by_xpath(load_more_xpath)\n",
    "        )\n",
    "        if (load_more_button.is_displayed()) and (load_more_button.is_enabled()):\n",
    "            try:\n",
    "                load_more_button.click()\n",
    "            except:\n",
    "                time.sleep(2)\n",
    "                load_more_button.click()\n",
    "            finally:\n",
    "                page_counter += 1\n",
    "                page_number = page_counter\n",
    "                page_number_length = page_number + 1\n",
    "                print(\n",
    "                    '\"Load More\" button found and clicked so no next button or page number.'\n",
    "                )\n",
    "                # Get jobs\n",
    "                jobs, jobs_count, counter = get_page_jobs(\n",
    "                    driver,\n",
    "                    soup,\n",
    "                    keyword,\n",
    "                    keyword_file,\n",
    "                    site,\n",
    "                    main_window,\n",
    "                    window_before,\n",
    "                    counter,\n",
    "                    page_counter,\n",
    "                    page_number,\n",
    "                    page_number_length,\n",
    "                    jobs,\n",
    "                    df_old_jobs,\n",
    "                    save_path,\n",
    "                    json_file_name,\n",
    "                )\n",
    "        elif (not load_more_button.is_displayed()) and (\n",
    "            not load_more_button.is_enabled()\n",
    "        ):\n",
    "            page_number = page_counter\n",
    "            page_number_length = 1\n",
    "            print(\n",
    "                'Could not get page number. No \"Load More\" button or next button found.'\n",
    "            )\n",
    "    except (NoSuchWindowException, NoSuchElementException):\n",
    "        print(\n",
    "            f\"Could not go to next page. End of data collection for {keyword}.\\nGot {int(jobs_count)} jobs.\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get main job data\n",
    "def get_main(driver, soup):\n",
    "\n",
    "    pattern = re.compile(r\"[^A-Za-z0-9, ]+\")\n",
    "    upper_pattern = re.compile(\"[A-Z][^A-Z\\s]+(?:\\s+\\S[^A-Z\\s]*)*\")\n",
    "    #     WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.CLASS_NAME, 'jobs-search-results')))\n",
    "    job_container = soup.find(\"div\", class_=\"jobs-details-top-card__container\")\n",
    "    job_container2 = soup.find(\"div\", id=\"job-details\").find(\n",
    "        \"div\", class_=\"jobs-description-details pt4\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        job_title = (\n",
    "            job_container.find(\n",
    "                \"a\", class_=\"jobs-details-top-card__job-title-link ember-view\"\n",
    "            )\n",
    "            .find(\"h2\", class_=\"jobs-details-top-card__job-title t-20 t-black t-normal\")\n",
    "            .text.strip()\n",
    "        )\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job title found.\")\n",
    "        job_title = -1\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting job title: {sys.exc_info()[0]}.\")\n",
    "        job_title = -1\n",
    "    else:\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Job Title: {job_title}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    try:\n",
    "        job_description_list = []\n",
    "        job_desc = soup.find_all(\"div\", id=\"job-details\")\n",
    "        for job_d in job_desc:\n",
    "            job_description_list.append(job_d.text.strip())\n",
    "        job_description = \"\\n\".join(job_description_list)\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job description found.\")\n",
    "        job_description = -1\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting job description: {sys.exc_info()[0]}.\")\n",
    "        job_description = -1\n",
    "    else:\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Job Description: {job_description[:50]}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    try:\n",
    "        company_name = (\n",
    "            job_container.find(\"div\", class_=\"jobs-details-top-card__content-container\")\n",
    "            .find(\"a\", class_=re.compile(\"^jobs-details-top-card__company-url*\"))\n",
    "            .text.strip()\n",
    "        )\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job company name found.\")\n",
    "        company_name = -1\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting company name: {sys.exc_info()[0]}.\")\n",
    "        company_name = -1\n",
    "    else:\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Company Name: {company_name}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    try:\n",
    "        location = soup.find(\n",
    "            \"li\", class_=\"job-card-container__metadata-item\"\n",
    "        ).text.strip()\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job company location found.\")\n",
    "        location = -1\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting company location: {sys.exc_info()[0]}.\")\n",
    "        location = -1\n",
    "    else:\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Company Location: {location}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            industry_list = (\n",
    "                job_container2.find(\"div\", class_=\"jobs-box__group\")\n",
    "                .find(\"h3\", text=re.compile(\"Industry\"), attrs={\"class\": \"t-14 t-bold\"})\n",
    "                .find_parent(\"div\", class_=\"jobs-box__group\")\n",
    "                .find_all(\"ul\", class_=\"jobs-box__list jobs-description-details__list\")\n",
    "            )\n",
    "        except:\n",
    "            industry_list = (\n",
    "                job_container2.find(\n",
    "                    \"h3\", text=re.compile(\"Industry\"), attrs={\"class\": \"t-14 t-bold\"}\n",
    "                )\n",
    "                .find_parent(\"div\", class_=\"jobs-box__group\")\n",
    "                .find_all(\"ul\", class_=\"jobs-box__list jobs-description-details__list\")\n",
    "            )\n",
    "        for ind in industry_list:\n",
    "            ind = ind.text.strip().replace(\"  \", \"\").split(\"\\n\")\n",
    "            industry = [i for i in ind if i]\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job company industry found.\")\n",
    "        industry = -1\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting industry: {sys.exc_info()[0]}.\")\n",
    "        industry = -1\n",
    "    else:\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Industry: {industry}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            sector_list = (\n",
    "                job_container2.find(\"div\", class_=\"jobs-box__group\")\n",
    "                .find(\n",
    "                    \"h3\",\n",
    "                    text=re.compile(\"Job Functions\"),\n",
    "                    attrs={\"class\": \"t-14 t-bold\"},\n",
    "                )\n",
    "                .find_parent(\"div\", class_=\"jobs-box__group\")\n",
    "                .find_all(\"ul\", class_=\"jobs-box__list jobs-description-details__list\")\n",
    "            )\n",
    "        except:\n",
    "            sector_list = (\n",
    "                job_container2.find(\n",
    "                    \"h3\",\n",
    "                    text=re.compile(\"Job Functions\"),\n",
    "                    attrs={\"class\": \"t-14 t-bold\"},\n",
    "                )\n",
    "                .find_parent(\"div\", class_=\"jobs-box__group\")\n",
    "                .find_all(\"ul\", class_=\"jobs-box__list jobs-description-details__list\")\n",
    "            )\n",
    "        for sec in sector_list:\n",
    "            sec = sec.text.strip().replace(\"  \", \"\").split(\"\\n\")\n",
    "            sector = [s for s in sec if s]\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job sector found.\")\n",
    "        sector = -1\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting sector: {sys.exc_info()[0]}.\")\n",
    "        sector = -1\n",
    "    else:\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Sector: {sector}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            level = (\n",
    "                job_container2.find(\"div\", class_=\"jobs-box__group\")\n",
    "                .find(\n",
    "                    \"h3\",\n",
    "                    text=re.compile(\"Seniority Level\"),\n",
    "                    attrs={\"class\": \"t-14 t-bold\"},\n",
    "                )\n",
    "                .find_parent(\"div\", class_=\"jobs-box__group\")\n",
    "                .find(\"p\", class_=\"t-14 mb3\")\n",
    "                .text.strip()\n",
    "            )\n",
    "        except:\n",
    "            level = (\n",
    "                job_container2.find(\n",
    "                    \"h3\",\n",
    "                    text=re.compile(\"Seniority Level\"),\n",
    "                    attrs={\"class\": \"t-14 t-bold\"},\n",
    "                )\n",
    "                .find_parent(\"div\", class_=\"jobs-box__group\")\n",
    "                .find(\"p\", class_=\"t-14 mb3\")\n",
    "                .text.strip()\n",
    "            )\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job seniority level found.\")\n",
    "        level = -1\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting seniority level: {sys.exc_info()[0]}.\")\n",
    "        level = -1\n",
    "    else:\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Seniority Level: {level}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    try:\n",
    "        company_url = (\n",
    "            \"https://www.linkedin.com/\"\n",
    "            + job_container.find(\n",
    "                \"a\", class_=re.compile(\"^jobs-details-top-card__company-url*\")\n",
    "            )[\"href\"]\n",
    "        )\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job company url found.\")\n",
    "        company_url = -1\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting company url: {sys.exc_info()[0]}.\")\n",
    "        company_url = -1\n",
    "    else:\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Company URL: {company_url}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            emp_type = (\n",
    "                job_container2.find(\"div\", class_=\"jobs-box__group\")\n",
    "                .find(\n",
    "                    \"h3\",\n",
    "                    text=re.compile(\"Employment Type\"),\n",
    "                    attrs={\"class\": \"t-14 t-bold\"},\n",
    "                )\n",
    "                .find_parent(\"div\", class_=\"jobs-box__group\")\n",
    "                .find(\"p\", class_=\"t-14 mb3\")\n",
    "                .text.strip()\n",
    "            )\n",
    "        except:\n",
    "            emp_type = (\n",
    "                job_container2.find(\n",
    "                    \"h3\",\n",
    "                    text=re.compile(\"Employment Type\"),\n",
    "                    attrs={\"class\": \"t-14 t-bold\"},\n",
    "                )\n",
    "                .find_parent(\"div\", class_=\"jobs-box__group\")\n",
    "                .find(\"p\", class_=\"t-14 mb3\")\n",
    "                .text.strip()\n",
    "            )\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job employment type found.\")\n",
    "        emp_type = -1\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting employment type: {sys.exc_info()[0]}.\")\n",
    "        emp_type = -1\n",
    "    else:\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Employment Type: {emp_type}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    # Check if collection successful\n",
    "    imp_collected = [job_title, job_description, company_name, location]\n",
    "    if all((var != int(-1)) or (var != str(\"-1\")) for var in imp_collected):\n",
    "        print(\"Main data collected successfully.\")\n",
    "\n",
    "    elif any((var == int(-1)) or (var == str(\"-1\")) for var in imp_collected):\n",
    "        print(\n",
    "            f\"Main data NOT collected successfully. Sleeping for 10 seconds then trying again.\"\n",
    "        )\n",
    "        time.sleep(10)\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable(\n",
    "                    (By.CSS_SELECTOR, \"#main-content > div > section > ul\")\n",
    "                )\n",
    "            )\n",
    "            (\n",
    "                job_title,\n",
    "                job_description,\n",
    "                company_name,\n",
    "                location,\n",
    "                industry,\n",
    "                sector,\n",
    "                level,\n",
    "                company_url,\n",
    "                emp_type,\n",
    "            ) = get_main(driver, soup)\n",
    "        except:\n",
    "            print(\n",
    "                f\"Data NOT collected successfully. Not attempting again: {sys.exc_info()[0]}.\"\n",
    "            )\n",
    "\n",
    "    return (\n",
    "        job_title,\n",
    "        job_description,\n",
    "        company_name,\n",
    "        location,\n",
    "        industry,\n",
    "        sector,\n",
    "        level,\n",
    "        company_url,\n",
    "        emp_type,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect individual job links\n",
    "def get_each(\n",
    "    driver,\n",
    "    main_window,\n",
    "    window_before,\n",
    "    keyword,\n",
    "    site,\n",
    "    jobs,\n",
    "    job_soup,\n",
    "    job_id,\n",
    "    jobs_count,\n",
    "    counter,\n",
    "    page_counter,\n",
    "    page_number,\n",
    "    page_number_length,\n",
    "    save_path,\n",
    "    json_file_name,\n",
    "):\n",
    "    jobs = remove_dupe_dicts(jobs)\n",
    "\n",
    "    # If results open in new window, ke sure everything loads\n",
    "    new_window = check_window(driver, main_window, window_before)\n",
    "\n",
    "    # Get html of page with beautifulsoup4\n",
    "    html = driver.page_source\n",
    "    print(\"Feeding html driver to BeautifulSoup.\")\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # try:\n",
    "    # Get job id, url and age\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Job ID: {job_id}\")\n",
    "    print(\"-\" * 20)\n",
    "    job_url = (\n",
    "        \"https://www.linkedin.com/\"\n",
    "        + job_soup.find(\n",
    "            \"div\", class_=\"full-width artdeco-entity-lockup__title ember-view\"\n",
    "        ).find(\n",
    "            \"a\",\n",
    "            class_=\"disabled ember-view job-card-container__link job-card-list__title\",\n",
    "        )[\n",
    "            \"href\"\n",
    "        ]\n",
    "    )\n",
    "    print(f\"Job URL: {job_url}\")\n",
    "    print(\"-\" * 20)\n",
    "    try:\n",
    "        job_age_datetime = job_soup.find(\n",
    "            \"li\",\n",
    "            class_=re.compile(\"^job-card-container__listed-time job-card-container*\"),\n",
    "        ).find(\"time\")\n",
    "    except:\n",
    "        print('No date \"Job Age\" or \"Job Date\" found.')\n",
    "        job_age, job_age_number, job_date, job_datetime = assign_all(4, -1)\n",
    "    else:\n",
    "        job_age = job_age_datetime.text.strip()\n",
    "        for s in job_age.split():\n",
    "            job_age_number = int(s) if s.isdigit() else job_age\n",
    "        for i in re.findall(\"[\\w +/.]\", job_age):\n",
    "            job_age_unit = str(i) if i.isalpha() else job_age\n",
    "        job_date = job_age_datetime[\"datetime\"].strip()\n",
    "        job_datetime = datetime.datetime.strptime(job_date, \"%Y-%m-%d\")\n",
    "\n",
    "    print(f\"Job Age: {job_age}\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Job Date: {job_date}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    #########################################################\n",
    "    #########################################################\n",
    "    ####################### TAB INFO #######################\n",
    "    #########################################################\n",
    "    #########################################################\n",
    "\n",
    "    try:\n",
    "        # Get main data\n",
    "        (\n",
    "            job_title,\n",
    "            job_description,\n",
    "            company_name,\n",
    "            location,\n",
    "            industry,\n",
    "            sector,\n",
    "            level,\n",
    "            company_url,\n",
    "            emp_type,\n",
    "        ) = get_main(driver, soup)\n",
    "\n",
    "        # # In case there is no company tab, -1 for all variables\n",
    "        # except NoSuchElementException:\n",
    "        #     print(f'No data collected: {sys.exc_info()[0]}.')\n",
    "        #     job_title, job_description, company_name, location, industry, sector, level, company_url, emp_type = assign_all(9, -1)\n",
    "    except:\n",
    "        print(\n",
    "            f\"Problems with getting comment elements.\\nNo data collected: {sys.exc_info()[0]}.\"\n",
    "        )\n",
    "        (\n",
    "            job_url,\n",
    "            job_age,\n",
    "            job_age_number,\n",
    "            job_date,\n",
    "            job_title,\n",
    "            job_description,\n",
    "            company_name,\n",
    "            location,\n",
    "            industry,\n",
    "            sector,\n",
    "            level,\n",
    "            company_url,\n",
    "            emp_type,\n",
    "        ) = assign_all(13, -1)\n",
    "\n",
    "    # Save to dict\n",
    "    print(\"Saving to dict...\")\n",
    "    today = datetime.date.today()\n",
    "    jobs.append(\n",
    "        {\n",
    "            \"Search Keyword\": str(keyword),\n",
    "            \"Platform\": site,\n",
    "            \"Job ID\": int(job_id),\n",
    "            \"Job Title\": str(job_title),\n",
    "            \"Company Name\": str(company_name),\n",
    "            \"Location\": str(location),\n",
    "            \"Industry\": str(industry),\n",
    "            \"Sector\": str(sector),\n",
    "            \"Seniority Level\": str(level),\n",
    "            \"Job Description\": str(job_description),\n",
    "            \"Employment Type\": str(emp_type),\n",
    "            \"Company URL\": company_url,\n",
    "            \"Job URL\": job_url,\n",
    "            \"Job Age\": job_age,\n",
    "            \"Job Age Number\": job_age_number,\n",
    "            \"Job Date\": job_date,\n",
    "            \"Collection Date\": (today.strftime(\"%Y-%m-%d\")),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Save dict as json file\n",
    "    jobs = remove_dupe_dicts(jobs)\n",
    "    print(f\"Progress for {keyword} job ads: {len(jobs)} / {int(jobs_count)}\")\n",
    "    print(f\"Saving dict to json file as {json_file_name} in location {save_path}\")\n",
    "    with open(save_path + json_file_name, \"w\") as f:\n",
    "        json.dump(jobs, f)\n",
    "\n",
    "    # Check current window and go back if required\n",
    "    check_window_back(driver, main_window, window_before, new_window)\n",
    "\n",
    "    return jobs, counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_page_jobs(\n",
    "    driver,\n",
    "    soup,\n",
    "    keyword,\n",
    "    keyword_file,\n",
    "    site,\n",
    "    main_window,\n",
    "    window_before,\n",
    "    counter,\n",
    "    page_counter,\n",
    "    page_number,\n",
    "    page_number_length,\n",
    "    jobs,\n",
    "    df_old_jobs,\n",
    "    save_path,\n",
    "    json_file_name,\n",
    "):\n",
    "\n",
    "    # Get html of page with beautifulsoup4\n",
    "    html = driver.page_source\n",
    "    print(\"Feeding html driver to BeautifulSoup.\")\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    old_url = driver.title\n",
    "    # try:\n",
    "    # WebDriverWait(driver, 100).until(EC.presence_of_element_located((By.CLASS_NAME, 'jobs-search-results display-flex flex-column')))\n",
    "    try:\n",
    "        jobs_on_page = soup.find(\n",
    "            \"ul\", class_=\"jobs-search-results__list list-style-none\"\n",
    "        ).find_all(\n",
    "            \"li\",\n",
    "            class_=re.compile(\"^jobs-search-results__list-item occludable-update*\"),\n",
    "        )\n",
    "        jobs_count = len(jobs_on_page)\n",
    "    except AttributeError as e:\n",
    "        print(\n",
    "            f\"Unexpected error with job links: {e} - {sys.exc_info()[0]}.\\nAttempting .children method.\"\n",
    "        )\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable(\n",
    "                (\n",
    "                    By.CLASS_NAME,\n",
    "                    \"jobs-search-results__list-item occludable-update p0 relative ember-view\",\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        jobs_grid = soup.find_all(\n",
    "            \"ul\", class_=\"jobs-search-results__list list-style-none\"\n",
    "        )\n",
    "        jobs_on_page = [job.children for job in jobs_grid]\n",
    "        jobs_count = cardinality.count(jobs_on_page)\n",
    "\n",
    "    if int(jobs_count) != 0:\n",
    "        for job_soup in jobs_on_page:\n",
    "            try:\n",
    "                last_collected_job_id_soup = int(\n",
    "                    job_soup.find(\n",
    "                        \"div\",\n",
    "                        class_=re.compile(\n",
    "                            \"^job-card-container relative job-card-list job-card-container--clickable*\"\n",
    "                        ),\n",
    "                    )[\"data-job-id\"]\n",
    "                )\n",
    "            except:\n",
    "                last_collected_job_id_soup = int(\n",
    "                    str(job_soup[\"data-occludable-entity-urn\"]).split(\":\")[-1]\n",
    "                )\n",
    "            for job in jobs:\n",
    "                if int(job[\"Job ID\"]) == last_collected_job_id_soup:\n",
    "                    last_collected_job_index = int(jobs_on_page.index(job_soup))\n",
    "                #                     last_collected_job_index = int(index)\n",
    "                elif int(job[\"Job ID\"]) != last_collected_job_id_soup:\n",
    "                    last_collected_job_index = 0\n",
    "\n",
    "        print(f\"{int(jobs_count)} job(s) found on page.\")\n",
    "        print(f\"Progress for {keyword} job ads: {len(jobs)} / {int(jobs_count)}\")\n",
    "\n",
    "        for job_soup in jobs_on_page:\n",
    "            try:\n",
    "                print(\"-\" * 20)\n",
    "                print(\n",
    "                    f\"JOB NUMBER: {int(jobs_on_page.index(job_soup)) + 1} OUT OF {int(jobs_count)} ON PAGE {int(page_number)}.\"\n",
    "                )\n",
    "                page_counter = page_number - 1\n",
    "                if page_counter > 0:\n",
    "                    with open(\n",
    "                        save_path + f\"{site.lower()}_page_counter_{keyword_file}.txt\",\n",
    "                        \"w\",\n",
    "                    ) as f:\n",
    "                        f.write(str(page_counter))\n",
    "                print(\"-\" * 20)\n",
    "                job_button = WebDriverWait(driver, 10).until(\n",
    "                    lambda driver: driver.find_element_by_xpath(xpath_soup(job_soup))\n",
    "                )\n",
    "            except:\n",
    "                # If results open in new window, make sure everything loads\n",
    "                new_window = check_window(driver, main_window, window_before)\n",
    "                if (new_window is True) or (old_url != driver.title):\n",
    "                    # Get jobs\n",
    "                    jobs, jobs_count, df_jobs, page_counter, counter = get_jobs(\n",
    "                        keyword.lower(),\n",
    "                        keyword_url,\n",
    "                        site,\n",
    "                        country,\n",
    "                        driver,\n",
    "                        counter,\n",
    "                        page_counter,\n",
    "                        main_window,\n",
    "                        df_old_jobs,\n",
    "                        jobs,\n",
    "                        save_path.lower(),\n",
    "                        json_file_name.lower(),\n",
    "                        df_file_name.lower(),\n",
    "                    )\n",
    "                elif (new_window is False) or (old_url == driver.title):\n",
    "                    print(\n",
    "                        f\"Problem getting job soup for {str(keyword)}: {sys.exc_info()[0]}. Trying againg.\"\n",
    "                    )\n",
    "                    last_job_soup_xpath = xpath_soup(\n",
    "                        jobs_on_page[int(jobs_on_page.index(job_soup)) - 1]\n",
    "                    )\n",
    "                    last_job_soup = WebDriverWait(driver, 10).until(\n",
    "                        lambda driver: driver.find_element_by_xpath(last_job_soup_xpath)\n",
    "                    )\n",
    "                    job_soup = WebDriverWait(driver, 10).until(\n",
    "                        lambda last_job_soup: last_job_soup.find_element_by_xpath(\n",
    "                            \"following-sibling::*[1]\"\n",
    "                        )\n",
    "                    )\n",
    "                    job_xpath = xpath_soup(job_soup)\n",
    "                    job_button = WebDriverWait(driver, 10).until(\n",
    "                        lambda driver: driver.find_element_by_xpath(job_xpath)\n",
    "                    )\n",
    "\n",
    "            # If results open in new window, make sure everything loads\n",
    "            new_window = check_window(driver, main_window, window_before)\n",
    "            if (new_window is False) or (old_url == driver.title):\n",
    "                try:\n",
    "                    job_button.click()\n",
    "                except:\n",
    "                    print(\n",
    "                        f\"Click did not work. Trying again to get job soup for {str(keyword)}.\"\n",
    "                    )\n",
    "                    # If results open in new window, make sure everything loads\n",
    "                    new_window = check_window(driver, main_window, window_before)\n",
    "                    if (new_window is True) or (old_url != driver.title):\n",
    "                        # Get jobs\n",
    "                        jobs, jobs_count, df_jobs, page_counter, counter = get_jobs(\n",
    "                            keyword.lower(),\n",
    "                            keyword_url,\n",
    "                            site,\n",
    "                            country,\n",
    "                            driver,\n",
    "                            counter,\n",
    "                            page_counter,\n",
    "                            main_window,\n",
    "                            df_old_jobs,\n",
    "                            jobs,\n",
    "                            save_path.lower(),\n",
    "                            json_file_name.lower(),\n",
    "                            df_file_name.lower(),\n",
    "                        )\n",
    "                    elif (new_window is False) or (old_url == driver.title):\n",
    "                        print(\n",
    "                            f\"Problem clicking job button for {str(keyword)}: {sys.exc_info()[0]}. Trying againg.\"\n",
    "                        )\n",
    "                        job_button.click()\n",
    "\n",
    "                # Get job id\n",
    "                try:\n",
    "                    job_id = job_soup.find(\n",
    "                        \"div\",\n",
    "                        class_=re.compile(\n",
    "                            \"^job-card-container relative job-card-list job-card-container--clickable*\"\n",
    "                        ),\n",
    "                    )[\"data-job-id\"]\n",
    "                except TypeError:\n",
    "                    job_id = int(\n",
    "                        str(job_soup[\"data-occludable-entity-urn\"]).split(\":\")[-1]\n",
    "                    )\n",
    "\n",
    "                # Check if already collected\n",
    "                jobs, job_present = id_check(\n",
    "                    driver,\n",
    "                    main_window,\n",
    "                    window_before,\n",
    "                    keyword,\n",
    "                    site,\n",
    "                    df_old_jobs,\n",
    "                    jobs,\n",
    "                    job_soup,\n",
    "                    job_id,\n",
    "                    jobs_count,\n",
    "                    save_path,\n",
    "                    json_file_name,\n",
    "                )\n",
    "                if job_present is False:\n",
    "                    if int(jobs_on_page.index(job_soup)) != int(jobs_count):\n",
    "                        jobs, counter = get_each(\n",
    "                            driver,\n",
    "                            main_window,\n",
    "                            window_before,\n",
    "                            keyword,\n",
    "                            site,\n",
    "                            jobs,\n",
    "                            job_soup,\n",
    "                            job_id,\n",
    "                            jobs_count,\n",
    "                            counter,\n",
    "                            page_counter,\n",
    "                            page_number,\n",
    "                            page_number_length,\n",
    "                            save_path,\n",
    "                            json_file_name,\n",
    "                        )\n",
    "                    elif int(jobs_on_page.index(job_soup)) == int(jobs_count):\n",
    "                        pass\n",
    "                elif job_present is True:\n",
    "                    pass\n",
    "\n",
    "                # If results open in new window, make sure everything loads\n",
    "                new_window = check_window(driver, main_window, window_before)\n",
    "            elif (new_window is True) or (old_url != driver.title):\n",
    "                driver.back()\n",
    "                jobs, jobs_count, counter = get_page_jobs(\n",
    "                    driver,\n",
    "                    soup,\n",
    "                    keyword,\n",
    "                    keyword_file,\n",
    "                    site,\n",
    "                    main_window,\n",
    "                    window_before,\n",
    "                    counter,\n",
    "                    page_counter,\n",
    "                    page_number,\n",
    "                    page_number_length,\n",
    "                    jobs,\n",
    "                    df_old_jobs,\n",
    "                    save_path,\n",
    "                    json_file_name,\n",
    "                )\n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "\n",
    "    elif int(jobs_count) == 0:\n",
    "        print(\n",
    "            f\"No jobs results found for {str(keyword)}. No error detected. Moving on to next search.\"\n",
    "        )\n",
    "    #     except:\n",
    "    #         print(f'No jobs results found for {str(keyword)}: {sys.exc_info()[0]}. Moving on to next search.')\n",
    "    #         job_id, job_url, job_age, job_age_number, job_date, job_title, job_description, company_name, location, industry, sector, level, company_url, emp_type = assign_all(14, -1)\n",
    "\n",
    "    # Save dict as json file\n",
    "    jobs = remove_dupe_dicts(jobs)\n",
    "    try:\n",
    "        print(f\"Progress for {keyword} job ads: {len(jobs)} / {int(jobs_count)}\")\n",
    "    except UnboundLocalError:\n",
    "        jobs_count = len(jobs)\n",
    "        print(f\"Progress for {keyword} job ads: {len(jobs)} / {int(jobs_count)}\")\n",
    "    print(f\"Saving dict to json file as {json_file_name} in location {save_path}\")\n",
    "    with open(save_path + json_file_name, \"w\") as f:\n",
    "        json.dump(jobs, f)\n",
    "\n",
    "    return jobs, jobs_count, counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get-jobs function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# MAIN FUNCTION TO GET JOB ADS\n",
    "def get_jobs(\n",
    "    driver,\n",
    "    keyword,\n",
    "    keyword_url,\n",
    "    keyword_file,\n",
    "    site,\n",
    "    country,\n",
    "    counter,\n",
    "    page_counter,\n",
    "    main_window,\n",
    "    df_old_jobs,\n",
    "    jobs,\n",
    "    save_path,\n",
    "    json_file_name,\n",
    "    df_file_name,\n",
    "):\n",
    "\n",
    "    url = f\"https://www.linkedin.com/jobs/search?keywords={keyword_url}&location={country}\"\n",
    "\n",
    "    # Go to page and establish main windows\n",
    "    print(f\"Going to url: {url}\")\n",
    "    driver.get(url)\n",
    "    window_before = driver.window_handles[0]\n",
    "    print(f\"First Window: {window_before}\")\n",
    "    print(f\"Current page title: {driver.title}\")\n",
    "    pre_signup_driver_title = driver.title\n",
    "\n",
    "    # Get html of page with beautifulsoup4\n",
    "    html = driver.page_source\n",
    "    print(\"Feeding html driver to BeautifulSoup.\")\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    try:\n",
    "        signup_form = WebDriverWait(driver, 10).until(\n",
    "            lambda driver: driver.find_element_by_class_name(\n",
    "                \"join-form__form-body-agreement\"\n",
    "            )\n",
    "        )\n",
    "        if (signup_form.is_enabled()) or (\n",
    "            \"By clicking Agree & Join, you agree to the LinkedIn\" in signup_form.text\n",
    "        ):\n",
    "            signin_button_general = WebDriverWait(driver, 10).until(\n",
    "                lambda driver: driver.find_element_by_class_name(\"form-toggle\")\n",
    "            )\n",
    "            if (signin_button_general.is_enabled()) or (\n",
    "                \"Sign in\" in signin_button_general.text\n",
    "            ):\n",
    "                try:\n",
    "                    signin_button_general.click()\n",
    "                except (\n",
    "                    ElementNotInteractableException,\n",
    "                    ElementClickInterceptedException,\n",
    "                ):\n",
    "                    time.sleep(1)\n",
    "                    signin_button_general.click()\n",
    "\n",
    "                try:\n",
    "                    sign_in(driver)\n",
    "                except (\n",
    "                    ElementNotInteractableException,\n",
    "                    ElementClickInterceptedException,\n",
    "                ):\n",
    "                    time.sleep(1)\n",
    "                    sign_in(driver)\n",
    "                print(\"Signed in.\")\n",
    "    except TimeoutException:\n",
    "        signin_button = WebDriverWait(driver, 10).until(\n",
    "            lambda driver: driver.find_element_by_class_name(\"nav__button-secondary\")\n",
    "        )\n",
    "        if (signin_button.is_enabled()) or (\"Sign in\" in signin_button.text):\n",
    "            try:\n",
    "                signin_button.click()\n",
    "            except (ElementNotInteractableException, ElementClickInterceptedException):\n",
    "                time.sleep(1)\n",
    "                signin_button.click()\n",
    "            print(\"Signing in.\")\n",
    "\n",
    "            try:\n",
    "                sign_in(driver)\n",
    "            except (ElementNotInteractableException, ElementClickInterceptedException):\n",
    "                time.sleep(1)\n",
    "                sign_in(driver)\n",
    "            print(\"Signed in.\")\n",
    "    #     except:\n",
    "    #         print('Already signed in.')\n",
    "\n",
    "    url = f\"https://www.linkedin.com/jobs/search?keywords={keyword_url}&location={country}\"\n",
    "\n",
    "    # Go to page and establish main windows\n",
    "    print(f\"Going to url: {url}\")\n",
    "    driver.get(url)\n",
    "    window_before = driver.window_handles[0]\n",
    "    print(f\"First Window: {window_before}\")\n",
    "    print(f\"Current page title: {driver.title}\")\n",
    "    pre_signup_driver_title = driver.title\n",
    "\n",
    "    if \"Security Verification\" in str(driver.title):\n",
    "        print(\"!\" * 30, \"CAPTCHA FOUND\", \"!\" * 30)\n",
    "        act_cool(5, 500)\n",
    "        ! openpyn nl\n",
    "        ! funckymonkey89\n",
    "        # Get jobs\n",
    "        jobs, jobs_count, df_jobs, page_counter, counter = get_jobs(\n",
    "            driver,\n",
    "            keyword.lower(),\n",
    "            keyword_url,\n",
    "            keyword_file,\n",
    "            site,\n",
    "            country,\n",
    "            counter,\n",
    "            page_counter,\n",
    "            main_window,\n",
    "            df_old_jobs,\n",
    "            jobs,\n",
    "            save_path.lower(),\n",
    "            json_file_name.lower(),\n",
    "            df_file_name.lower(),\n",
    "        )\n",
    "\n",
    "    # Find page number\n",
    "    try:\n",
    "        page_number_length, page_number = find_page_number(\n",
    "            driver, soup, page_counter, keyword\n",
    "        )\n",
    "    except errors:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            try:\n",
    "                load_more_xpath = xpath_soup(\n",
    "                    soup.find(\n",
    "                        \"section\",\n",
    "                        class_=\"m3\",\n",
    "                        attrs={\"aria-label\": \"Load more results\"},\n",
    "                    )\n",
    "                    .find(\"div\", class_=\"jobs-search-two-pane__load-more\")\n",
    "                    .find(\"button\")\n",
    "                )\n",
    "                load_more_button = WebDriverWait(driver, 10).until(\n",
    "                    lambda driver: driver.find_element_by_xpath(load_more_xpath)\n",
    "                )\n",
    "            except:\n",
    "                load_more_button = WebDriverWait(driver, 10).until(\n",
    "                    lambda driver: driver.find_element_by_xpath('//*[@id=\"ember1142\"]')\n",
    "                )\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        else:\n",
    "            if (load_more_button.is_displayed()) and (load_more_button.is_enabled()):\n",
    "                page_number = page_counter\n",
    "                page_number_length = 1\n",
    "                print('\"Load More\" button found so no next button or page number.')\n",
    "            elif (not load_more_button.is_displayed()) and (\n",
    "                not load_more_button.is_enabled()\n",
    "            ):\n",
    "                page_number = page_counter\n",
    "                page_number_length = 1\n",
    "                print(\n",
    "                    'Could not get page number. No \"Load More\" button or next button found.'\n",
    "                )\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Check for errors\n",
    "    # try:\n",
    "    not_page_error = WebDriverWait(driver, 10).until_not(\n",
    "        lambda driver: driver.find_element_by_class_name(\n",
    "            re.compile(\"^jobs-search-no-results__image\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    while not_page_error:\n",
    "        # Find jobs in page\n",
    "        print(\"Finding job links in page.\")\n",
    "        jobs, jobs_count, counter = get_page_jobs(\n",
    "            driver,\n",
    "            soup,\n",
    "            keyword,\n",
    "            keyword_file,\n",
    "            site,\n",
    "            main_window,\n",
    "            window_before,\n",
    "            counter,\n",
    "            page_counter,\n",
    "            page_number,\n",
    "            page_number_length,\n",
    "            jobs,\n",
    "            df_old_jobs,\n",
    "            save_path,\n",
    "            json_file_name,\n",
    "        )\n",
    "\n",
    "        # Get html of page with beautifulsoup4\n",
    "        html = driver.page_source\n",
    "        print(\"Feeding html driver to BeautifulSoup.\")\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "        try:\n",
    "            not_end_of_load = WebDriverWait(driver, 10).until_not(\n",
    "                lambda driver: driver.find_element_by_class_name(\n",
    "                    \"query-expansion-suggestions\"\n",
    "                )\n",
    "            )\n",
    "        except:\n",
    "            not_end_of_load = WebDriverWait(driver, 10).until_not(\n",
    "                lambda driver: driver.find_element_by_class_name(\n",
    "                    \"occludable-update ember-view\"\n",
    "                )\n",
    "            )\n",
    "        if not_end_of_load:\n",
    "            new_driver_title = driver.title\n",
    "            if new_driver_title == driver.title:\n",
    "                # Find next jobs / page\n",
    "                try:\n",
    "                    try:\n",
    "                        load_more(\n",
    "                            driver,\n",
    "                            soup,\n",
    "                            keyword,\n",
    "                            keyword_file,\n",
    "                            site,\n",
    "                            main_window,\n",
    "                            window_before,\n",
    "                            counter,\n",
    "                            page_counter,\n",
    "                            page_number,\n",
    "                            page_number_length,\n",
    "                            jobs,\n",
    "                            jobs_count,\n",
    "                            df_old_jobs,\n",
    "                            save_path,\n",
    "                            json_file_name,\n",
    "                        )\n",
    "                        # Get jobs\n",
    "                        jobs, jobs_count, counter = get_page_jobs(\n",
    "                            driver,\n",
    "                            soup,\n",
    "                            keyword,\n",
    "                            keyword_file,\n",
    "                            site,\n",
    "                            main_window,\n",
    "                            window_before,\n",
    "                            counter,\n",
    "                            page_counter,\n",
    "                            page_number,\n",
    "                            page_number_length,\n",
    "                            jobs,\n",
    "                            df_old_jobs,\n",
    "                            save_path,\n",
    "                            json_file_name,\n",
    "                        )\n",
    "                    except (AttributeError, TimeoutException):\n",
    "                        page_number_length, page_number = find_page_number(\n",
    "                            driver, soup, page_counter, keyword\n",
    "                        )\n",
    "                        if int(page_number_length) == int(page_number):\n",
    "                            print(\n",
    "                                f\"Last page reached at page {page_number}. End of data collection for {keyword}.\\nGot {int(jobs_count)} jobs.\"\n",
    "                            )\n",
    "                            break\n",
    "                        elif int(page_number_length) != int(page_number):\n",
    "                            page_current = soup.find(\n",
    "                                \"li\",\n",
    "                                class_=\"artdeco-pagination__indicator artdeco-pagination__indicator--number active selected ember-view\",\n",
    "                            ).find(\"button\", {\"aria-current\": \"true\"})\n",
    "                            next_page_button_soup = page_current.find_next_siblings(\n",
    "                                \"li\",\n",
    "                                class_=\"artdeco-pagination__indicator artdeco-pagination__indicator--number ember-view\",\n",
    "                            ).find(\"button\")\n",
    "                            next_page_button_xpath = xpath_soup(next_page_button_soup)\n",
    "                            next_page_button.click()\n",
    "                            print(\n",
    "                                f\"Going to next page {int(page_number)+1}.\\nGot {int(jobs_count)} jobs.\"\n",
    "                            )\n",
    "                        elif int(page_number_length) == -1:\n",
    "                            load_more(\n",
    "                                driver,\n",
    "                                soup,\n",
    "                                keyword,\n",
    "                                keyword_file,\n",
    "                                site,\n",
    "                                main_window,\n",
    "                                window_before,\n",
    "                                counter,\n",
    "                                page_counter,\n",
    "                                page_number,\n",
    "                                page_number_length,\n",
    "                                jobs,\n",
    "                                jobs_count,\n",
    "                                df_old_jobs,\n",
    "                                save_path,\n",
    "                                json_file_name,\n",
    "                            )\n",
    "                            # Get jobs\n",
    "                            jobs, jobs_count, counter = get_page_jobs(\n",
    "                                driver,\n",
    "                                soup,\n",
    "                                keyword,\n",
    "                                keyword_file,\n",
    "                                site,\n",
    "                                main_window,\n",
    "                                window_before,\n",
    "                                counter,\n",
    "                                page_counter,\n",
    "                                page_number,\n",
    "                                page_number_length,\n",
    "                                jobs,\n",
    "                                df_old_jobs,\n",
    "                                save_path,\n",
    "                                json_file_name,\n",
    "                            )\n",
    "                    except:\n",
    "                        # Get html of page with beautifulsoup4\n",
    "                        html = driver.page_source\n",
    "                        print(\"Feeding html driver to BeautifulSoup.\")\n",
    "                        soup = BeautifulSoup(html, \"lxml\")\n",
    "                        next_page_xpath = xpath_soup(\n",
    "                            soup.find(\"a\", attrs={\"data-test\": \"pagination-next\"})\n",
    "                        )\n",
    "                        next_page_button = WebDriverWait(driver, 10).until_not(\n",
    "                            lambda driver: driver.find_element_by_xpath(next_page_xpath)\n",
    "                        )\n",
    "                        if int(page_number_length) == int(page_number):\n",
    "                            print(\n",
    "                                f\"Last page reached at page {page_number}. End of data collection for {keyword}.\\nGot {int(jobs_count)} jobs.\"\n",
    "                            )\n",
    "                            break\n",
    "                    try:\n",
    "                        next_page_button.click()\n",
    "                        print(\n",
    "                            f\"Going to next page {int(page_number)+1}.\\nGot {int(jobs_count)} jobs.\"\n",
    "                        )\n",
    "                        # Get jobs\n",
    "                        jobs, jobs_count, counter = get_page_jobs(\n",
    "                            driver,\n",
    "                            soup,\n",
    "                            keyword,\n",
    "                            keyword_file,\n",
    "                            site,\n",
    "                            main_window,\n",
    "                            window_before,\n",
    "                            counter,\n",
    "                            page_counter,\n",
    "                            page_number,\n",
    "                            page_number_length,\n",
    "                            jobs,\n",
    "                            df_old_jobs,\n",
    "                            save_path,\n",
    "                            json_file_name,\n",
    "                        )\n",
    "                    except TimeoutException:\n",
    "                        load_more(\n",
    "                            driver,\n",
    "                            soup,\n",
    "                            keyword,\n",
    "                            keyword_file,\n",
    "                            site,\n",
    "                            main_window,\n",
    "                            window_before,\n",
    "                            counter,\n",
    "                            page_counter,\n",
    "                            page_number,\n",
    "                            page_number_length,\n",
    "                            jobs,\n",
    "                            jobs_count,\n",
    "                            df_old_jobs,\n",
    "                            save_path,\n",
    "                            json_file_name,\n",
    "                        )\n",
    "                        # Get jobs\n",
    "                        jobs, jobs_count, counter = get_page_jobs(\n",
    "                            driver,\n",
    "                            soup,\n",
    "                            keyword,\n",
    "                            keyword_file,\n",
    "                            site,\n",
    "                            main_window,\n",
    "                            window_before,\n",
    "                            counter,\n",
    "                            page_counter,\n",
    "                            page_number,\n",
    "                            page_number_length,\n",
    "                            jobs,\n",
    "                            df_old_jobs,\n",
    "                            save_path,\n",
    "                            json_file_name,\n",
    "                        )\n",
    "\n",
    "                except:\n",
    "                    if new_driver_title == driver.title:\n",
    "                        load_more(\n",
    "                            driver,\n",
    "                            soup,\n",
    "                            keyword,\n",
    "                            keyword_file,\n",
    "                            site,\n",
    "                            main_window,\n",
    "                            window_before,\n",
    "                            counter,\n",
    "                            page_counter,\n",
    "                            page_number,\n",
    "                            page_number_length,\n",
    "                            jobs,\n",
    "                            jobs_count,\n",
    "                            df_old_jobs,\n",
    "                            save_path,\n",
    "                            json_file_name,\n",
    "                        )\n",
    "                        # Get jobs\n",
    "                        jobs, jobs_count, counter = get_page_jobs(\n",
    "                            driver,\n",
    "                            soup,\n",
    "                            keyword,\n",
    "                            keyword_file,\n",
    "                            site,\n",
    "                            main_window,\n",
    "                            window_before,\n",
    "                            counter,\n",
    "                            page_counter,\n",
    "                            page_number,\n",
    "                            page_number_length,\n",
    "                            jobs,\n",
    "                            df_old_jobs,\n",
    "                            save_path,\n",
    "                            json_file_name,\n",
    "                        )\n",
    "                    elif new_driver_title != driver.title:\n",
    "                        driver.back()\n",
    "                        time.sleep(2)\n",
    "                        load_more(\n",
    "                            driver,\n",
    "                            soup,\n",
    "                            keyword,\n",
    "                            keyword_file,\n",
    "                            site,\n",
    "                            main_window,\n",
    "                            window_before,\n",
    "                            counter,\n",
    "                            page_counter,\n",
    "                            page_number,\n",
    "                            page_number_length,\n",
    "                            jobs,\n",
    "                            jobs_count,\n",
    "                            df_old_jobs,\n",
    "                            save_path,\n",
    "                            json_file_name,\n",
    "                        )\n",
    "                        # Get jobs\n",
    "                        jobs, jobs_count, counter = get_page_jobs(\n",
    "                            driver,\n",
    "                            soup,\n",
    "                            keyword,\n",
    "                            keyword_file,\n",
    "                            site,\n",
    "                            main_window,\n",
    "                            window_before,\n",
    "                            counter,\n",
    "                            page_counter,\n",
    "                            page_number,\n",
    "                            page_number_length,\n",
    "                            jobs,\n",
    "                            df_old_jobs,\n",
    "                            save_path,\n",
    "                            json_file_name,\n",
    "                        )\n",
    "            elif new_driver_title != driver.title:\n",
    "                driver.back()\n",
    "                time.sleep(2)\n",
    "                load_more(\n",
    "                    driver,\n",
    "                    soup,\n",
    "                    keyword,\n",
    "                    keyword_file,\n",
    "                    site,\n",
    "                    main_window,\n",
    "                    window_before,\n",
    "                    counter,\n",
    "                    page_counter,\n",
    "                    page_number,\n",
    "                    page_number_length,\n",
    "                    jobs,\n",
    "                    jobs_count,\n",
    "                    df_old_jobs,\n",
    "                    save_path,\n",
    "                    json_file_name,\n",
    "                )\n",
    "                # Get jobs\n",
    "                jobs, jobs_count, counter = get_page_jobs(\n",
    "                    driver,\n",
    "                    soup,\n",
    "                    keyword,\n",
    "                    keyword_file,\n",
    "                    site,\n",
    "                    main_window,\n",
    "                    window_before,\n",
    "                    counter,\n",
    "                    page_counter,\n",
    "                    page_number,\n",
    "                    page_number_length,\n",
    "                    jobs,\n",
    "                    df_old_jobs,\n",
    "                    save_path,\n",
    "                    json_file_name,\n",
    "                )\n",
    "\n",
    "        elif not not_end_of_load:\n",
    "            print('No jobs on page. Showing page error \"not_end_of_load\".')\n",
    "            break\n",
    "\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"REACHED END OF JOBS.\")\n",
    "        print(\"-\" * 20)\n",
    "    else:\n",
    "        print('No jobs on page. Showing page error \"not_page_error\".')\n",
    "        jobs_count = len(jobs)\n",
    "        print(f\"No job results found for {str(keyword)}. Moving on to next search.\")\n",
    "    # except:\n",
    "    #     jobs_count = len(jobs)\n",
    "    #     print(f'No jobs results found for {str(keyword)}. Moving on to next search.')\n",
    "\n",
    "    ############################################### FINAL SAVING AND CLEANUP ###############################################\n",
    "\n",
    "    # Save dict as json file\n",
    "    print(\"Final save.\")\n",
    "    jobs = remove_dupe_dicts(jobs)\n",
    "    try:\n",
    "        print(f\"Progress for {keyword} job ads: {len(jobs)} / {int(jobs_count)}\")\n",
    "    except:\n",
    "        print(f\"Progress for {keyword} job ads: {len(jobs)}\")\n",
    "    print(f\"Saving dict to json file as {json_file_name} in location {save_path}\")\n",
    "    with open(save_path + json_file_name, \"w\") as f:\n",
    "        json.dump(jobs, f)\n",
    "\n",
    "    # Save jobs to df\n",
    "    print(f\"Saving {keyword} jobs data to df...\")\n",
    "    if jobs:\n",
    "        df_jobs = pd.DataFrame(jobs)\n",
    "    elif not jobs:\n",
    "        print(f\"Jobs dict for {keyword} is empty.\")\n",
    "        df_jobs = pd.DataFrame()\n",
    "\n",
    "    # Save df as csv\n",
    "    if not df_jobs.empty:\n",
    "        df_jobs.append(df_old_jobs, ignore_index=True)\n",
    "        df_jobs = clean_df(df_jobs)\n",
    "        df_jobs = save_df(\n",
    "            keyword, df_jobs, save_path, keyword_file.lower(), df_file_name.lower()\n",
    "        )\n",
    "    elif df_jobs.empty:\n",
    "        df_jobs = df_old_jobs\n",
    "\n",
    "    return jobs, jobs_count, df_jobs, page_counter, counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run get_jobs function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "def run_all_linkedin(\n",
    "    site=\"LinkedIn\",\n",
    "    language=\"English\",\n",
    "    country=\"Netherlands\",\n",
    "    incognito_enabled=True,\n",
    "    headless_enabled=False,\n",
    "    proxy_enabled=False,\n",
    "    logging_enabled=False,\n",
    "    print_enabled=True,\n",
    "):\n",
    "\n",
    "    # Get keywords\n",
    "    (\n",
    "        keywords_list,\n",
    "        keywords_sector,\n",
    "        keywords_womenvocc,\n",
    "        keywords_menvocc,\n",
    "        keywords_genvsect,\n",
    "        keywords_oldvocc,\n",
    "        keywords_youngvocc,\n",
    "        keywords_agevsect,\n",
    "    ) = get_keyword_list(print_enabled=print_enabled)\n",
    "    # remove_list = ['software', 'service', 'storage', 'information', 'sales', 'culture']\n",
    "    # for k in remove_list:\n",
    "    #     keywords_list.remove(k)\n",
    "\n",
    "    # Start driver\n",
    "    driver = get_driver(select_driver)\n",
    "\n",
    "    # Start for-loop\n",
    "    keywords_oldvocc = [\"industry\"]\n",
    "    for keyword in keywords_youngvocc:\n",
    "        counter = 1\n",
    "        page_counter = 0\n",
    "        recollect = False\n",
    "\n",
    "        # Check internet connection\n",
    "        if not is_connected(driver):\n",
    "            is_connected(driver)\n",
    "\n",
    "        # Get main info\n",
    "        (\n",
    "            keyword_url,\n",
    "            keyword_file,\n",
    "            save_path,\n",
    "            json_file_name,\n",
    "            df_file_name,\n",
    "            logs_file_name,\n",
    "            filemode,\n",
    "        ) = main_info(keyword, site)\n",
    "\n",
    "        # Set up log\n",
    "        if logging_enabled is True:\n",
    "            print(\"Logging enabled.\")\n",
    "            writer = MyWriter(logs_file_name.lower(), filemode)\n",
    "            sys.stdout = writer\n",
    "            logger = logging.getLogger(__name__)\n",
    "            logging.basicConfig(\n",
    "                filename=writer.LOGS_PATH + logs_file_name.lower(),\n",
    "                filemode=filemode,\n",
    "                level=logging.CRITICAL,\n",
    "                format=\"%(asctime)s:%(levelname)s: %(message)s\",\n",
    "            )\n",
    "        elif logging_enabled is False:\n",
    "            print(\"No logging enabled.\")\n",
    "\n",
    "        # Load and merge existing dict and df\n",
    "        print(f\"All files will be saved in folder: {save_path}{json_file_name}\")\n",
    "        jobs, df_old_jobs = load_merge_dict_df(\n",
    "            keyword, save_path, df_file_name, json_file_name\n",
    "        )\n",
    "\n",
    "        # Beging data collection\n",
    "        print(\n",
    "            \"-\" * 20,\n",
    "            f\"BEGINING OF DATA COLLECTION FOR {keyword.upper()} JOB ADS\",\n",
    "            \"-\" * 20,\n",
    "        )\n",
    "        main_window = driver.current_window_handle\n",
    "        print(f\"Main Window: {main_window}\")\n",
    "\n",
    "        # Load and save page_counter\n",
    "        page_counter_path = (\n",
    "            save_path + f\"{site.lower()}_page_counter_{keyword_file}.txt\"\n",
    "            if recollect is False\n",
    "            else save_path + f\"{site.lower()}_page_counter_done_{keyword_file}.txt\"\n",
    "        )\n",
    "\n",
    "        if is_non_zero_file(page_counter_path) is True:\n",
    "            with open(page_counter_path) as f:\n",
    "                page_counter = int(f.read())\n",
    "        elif is_non_zero_file(page_counter_path) is False:\n",
    "            page_counter = 1\n",
    "        with open(page_counter_path, \"w\") as f:\n",
    "            f.write(str(page_counter))\n",
    "\n",
    "        print(f\"Starting collection from page number: {page_counter}\")\n",
    "\n",
    "        # Start threads\n",
    "        popup_thread(driver, popup, popup_checker)\n",
    "        # stale_element_thread(driver, stale_element, stale_element_checker)\n",
    "        # act_cool_thread(driver, act_cool, act_cool_checker)\n",
    "\n",
    "        # Get jobs\n",
    "        jobs, jobs_count, df_jobs, page_counter, counter = get_jobs(\n",
    "            driver,\n",
    "            keyword.lower(),\n",
    "            keyword_url,\n",
    "            keyword_file,\n",
    "            site,\n",
    "            country,\n",
    "            counter,\n",
    "            page_counter,\n",
    "            main_window,\n",
    "            df_old_jobs,\n",
    "            jobs,\n",
    "            save_path.lower(),\n",
    "            json_file_name.lower(),\n",
    "            df_file_name.lower(),\n",
    "        )\n",
    "\n",
    "        # Save df as csv\n",
    "        df_jobs = save_df(\n",
    "            keyword, df_jobs, save_path, keyword_file.lower(), df_file_name.lower()\n",
    "        )\n",
    "        print(\n",
    "            \"-\" * 20, f\"END OF DATA COLLECTION FOR {keyword.upper()} JOB ADS\", \"-\" * 20\n",
    "        )\n",
    "\n",
    "    # Close and quit driver\n",
    "    print(f\"Closing driver.\")\n",
    "    driver.close()\n",
    "    driver.quit()\n",
    "    # thread_popup_checker.terminate()\n",
    "    # thread_stale_element_checker.terminate()\n",
    "    # thread_act_cool_checker.terminate()\n",
    "    # sys.stdout.close()\n",
    "    # sys.exit()\n",
    "\n",
    "    return df_jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_jobs_linkedin = run_all_linkedin()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword = 'network specialist'\n",
    "# site = 'LinkedIn'\n",
    "# keyword_from_list = True\n",
    "# site_from_list = False\n",
    "# df_jobs = post_cleanup(keyword, site)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "metadata": {
   "interpreter": {
    "hash": "e774977668b7c0ae8309835a5187aa7fbf7669e7d0bb59755bc63e573643edcd"
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
