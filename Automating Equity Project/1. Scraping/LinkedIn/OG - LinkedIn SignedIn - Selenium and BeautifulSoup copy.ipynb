{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "if code_dir_name not in str(Path.cwd()).split('/')[-1]:\n",
    "    for _ in range(5):\n",
    "\n",
    "        parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "        if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "            code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "            if code_dir is not None:\n",
    "                break\n",
    "else:\n",
    "    code_dir = str(Path.cwd())\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Function to check for pop-ups\n",
    "def popup():\n",
    "    # Get html of page with beautifulsoup4\n",
    "    html = driver.page_source\n",
    "    print(\"Feeding html driver to BeautifulSoup.\")\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # Messaging popup\n",
    "    try:\n",
    "        messaging_soup = soup.find(\"li-icon\", attrs={\"type\": \"chevron-down-icon\"})\n",
    "        messaging_xpath = xpath_soup(messaging_soup)\n",
    "        message_button = WebDriverWait(driver, 10).until(\n",
    "            lambda driver: driver.find_element_by_xpath(messaging_soup)\n",
    "        )\n",
    "        if message_button.is_enabled():\n",
    "            print(\"Message popup detected.\")\n",
    "            message_button.click()\n",
    "            print(\"Message popup dismissed.\")\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable(\n",
    "                    (\n",
    "                        By.CSS_SELECTOR,\n",
    "                        \"body > header > nav > section > section:nth-child(4) > form > button\",\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "    except errors:\n",
    "\n",
    "        # Accept cookies\n",
    "        try:\n",
    "            cookie_button = WebDriverWait(driver, 10).until(\n",
    "                lambda driver: driver.find_element_by_css_selector(\n",
    "                    \"#artdeco-global-alert-container > div.artdeco-global-alert.artdeco-global-alert--NOTICE.artdeco-global-alert--COOKIE_CONSENT > section > div > div.artdeco-global-alert-action__wrapper > button:nth-child(2)\"\n",
    "                )\n",
    "            )\n",
    "            if cookie_button.is_enabled():\n",
    "                cookie_button.click()\n",
    "                print(\"Cookies accepted.\")\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable(\n",
    "                        (\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \"body > header > nav > section > section:nth-child(4) > form > button\",\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "        except errors:\n",
    "\n",
    "            # Accept Alert\n",
    "            try:\n",
    "                click_alert = driver.switch_to.alert()\n",
    "                print(\"Alert found.\")\n",
    "                click_alert.accept()\n",
    "                print(\"Alert accepted.\")\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable(\n",
    "                        (\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \"body > header > nav > section > section:nth-child(4) > form > button\",\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            except errors:\n",
    "                pass\n",
    "            except:\n",
    "                try:\n",
    "                    print(f\"Alert caused unexpected error: {sys.exc_info()[0]}\")\n",
    "                except (NoSuchWindowException, urllib3.exceptions.MaxRetryError):\n",
    "                    pass\n",
    "        except:\n",
    "            try:\n",
    "                print(f\"Cookies alert caused unexpected error: {sys.exc_info()[0]}\")\n",
    "            except (NoSuchWindowException, urllib3.exceptions.MaxRetryError):\n",
    "                pass\n",
    "    except:\n",
    "        try:\n",
    "            print(f\"Message popup caused unexpected error: {sys.exc_info()[0]}\")\n",
    "        except (NoSuchWindowException, urllib3.exceptions.MaxRetryError):\n",
    "            pass\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for popups all the time\n",
    "def popup_checker():\n",
    "    while True:\n",
    "        try:\n",
    "            popup()\n",
    "        except AttributeError:\n",
    "            time.sleep(10)\n",
    "            print(f\"Unexpected error with popup checker: {sys.exc_info()[0]}.\")\n",
    "\n",
    "\n",
    "thread_popup_checker = multiprocessing.Process(target=popup_checker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find page number\n",
    "def find_page_number(driver, soup, counter, keyword):\n",
    "    # Get html of page with beautifulsoup4\n",
    "    html = driver.page_source\n",
    "    print(\"Feeding html driver to BeautifulSoup.\")\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    try:\n",
    "        page_footer_soup = soup.find(\n",
    "            \"ul\", class_=\"artdeco-pagination__pages artdeco-pagination__pages--number\"\n",
    "        )\n",
    "        page_footer_xpath = xpath_soup(page_footer_soup)\n",
    "        page_footer = WebDriverWait(driver, 10).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, page_footer_xpath))\n",
    "        )\n",
    "        if page_footer:\n",
    "            page_number_length = (\n",
    "                soup.find_all(\n",
    "                    \"li\",\n",
    "                    class_=\"artdeco-pagination__indicator artdeco-pagination__indicator--number ember-view\",\n",
    "                )[-1]\n",
    "                .find(\"button\")\n",
    "                .text.strip()\n",
    "            )\n",
    "            if int(page_number_length) == 1:\n",
    "                print(f\"Only 1 page found for {str(keyword)} jobs.\")\n",
    "                page_number = page_number_length\n",
    "                page_number = [int(s) for s in page_number.split() if s.isdigit()][0]\n",
    "                print(f\"Current page number: {page_number}.\")\n",
    "            elif int(page_number_length) != 1:\n",
    "                print(\n",
    "                    f\"Total of {page_number_length} pages found for {str(keyword)} jobs.\"\n",
    "                )\n",
    "                try:\n",
    "                    page_number = (\n",
    "                        soup.find(\n",
    "                            \"li\",\n",
    "                            class_=\"artdeco-pagination__indicator artdeco-pagination__indicator--number active selected ember-view\",\n",
    "                        )\n",
    "                        .find(\"button\", {\"aria-current\": \"true\"})\n",
    "                        .text.strip()\n",
    "                    )\n",
    "                except:\n",
    "                    page_number = (\n",
    "                        soup.find(\n",
    "                            \"li\",\n",
    "                            class_=\"artdeco-pagination__indicator artdeco-pagination__indicator--number active selected ember-view\",\n",
    "                        )\n",
    "                        .find(\n",
    "                            \"button\", attrs={\"aria-label\": re.compile(\"*current page\")}\n",
    "                        )\n",
    "                        .text.strip()\n",
    "                    )\n",
    "                page_number = [int(s) for s in page_number.split() if s.isdigit()][0]\n",
    "                print(f\"Current page number: {page_number}.\")\n",
    "\n",
    "        elif not page_footer:\n",
    "            page_number = counter\n",
    "            print(\n",
    "                f\"No page number indicator found. Page number set to counter: {page_number}\"\n",
    "            )\n",
    "            pass\n",
    "    except (NoSuchElementException, TimeoutException):\n",
    "        page_number = counter\n",
    "        page_number_length = 1\n",
    "        print(\n",
    "            f\"No page number indicator found. Page number set to counter: {page_number}\"\n",
    "        )\n",
    "        pass\n",
    "    except errors:\n",
    "        page_number = counter\n",
    "        page_number_length = 1\n",
    "        print(f\"Unexpected error getting page number: {sys.exc_info()[0]}.\")\n",
    "        pass\n",
    "\n",
    "    return page_number_length, page_number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_page_jobs(\n",
    "    driver,\n",
    "    soup,\n",
    "    counter,\n",
    "    keyword,\n",
    "    site,\n",
    "    page_number,\n",
    "    page_number_length,\n",
    "    jobs,\n",
    "    df_old_jobs,\n",
    "    save_path,\n",
    "    json_file_name,\n",
    "):\n",
    "\n",
    "    # Get html of page with beautifulsoup4\n",
    "    html = driver.page_source\n",
    "    print(\"Feeding html driver to BeautifulSoup.\")\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    try:\n",
    "        WebDriverWait(driver, 100).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"jobs-search-results\"))\n",
    "        )\n",
    "        try:\n",
    "            jobs_on_page = soup.find(\n",
    "                \"ul\", class_=\"jobs-search-results__list list-style-none\"\n",
    "            ).find_all(\n",
    "                \"li\",\n",
    "                class_=re.compile(\"^jobs-search-results__list-item occludable-update*\"),\n",
    "            )\n",
    "            jobs_count = len(jobs_on_page)\n",
    "        except AttributeError as e:\n",
    "            print(\n",
    "                f\"Unexpected error with job links: {e} - {sys.exc_info()[0]}.\\nAttempting .children method.\"\n",
    "            )\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable(\n",
    "                    (\n",
    "                        By.CLASS_NAME,\n",
    "                        \"jobs-search-results__list-item occludable-update p0 relative ember-view\",\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            jobs_grid = soup.find_all(\n",
    "                \"ul\", class_=\"jobs-search-results__list list-style-none\"\n",
    "            )\n",
    "            jobs_on_page = [job.children for job in jobs_grid]\n",
    "            jobs_count = cardinality.count(jobs_on_page)\n",
    "\n",
    "        if int(jobs_count) != 0:\n",
    "            print(\n",
    "                f\"{int(jobs_count)} job(s) found on page {page_number} out of {page_number_length} page(s).\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)} / {int(jobs_count)*int(page_number_length)}\"\n",
    "            )\n",
    "\n",
    "            for job_soup in jobs_on_page:\n",
    "                try:\n",
    "                    job_xpath = xpath_soup(job_soup)\n",
    "                    job_button = WebDriverWait(driver, 10).until(\n",
    "                        lambda driver: driver.find_element_by_xpath(job_xpath)\n",
    "                    )\n",
    "\n",
    "                    try:\n",
    "                        job_button.click()\n",
    "                    except errors:\n",
    "                        time.sleep(5)\n",
    "                        job_button.click()\n",
    "\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located(\n",
    "                            (\n",
    "                                By.CLASS_NAME,\n",
    "                                \"jobs-search-results__list-item occludable-update p0 relative ember-view\",\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # Get job id\n",
    "                    job_id = job_soup.find(\n",
    "                        \"div\",\n",
    "                        class_=re.compile(\n",
    "                            \"^job-card-container relative job-card-list job-card-container--clickable*\"\n",
    "                        ),\n",
    "                    )[\"data-job-id\"]\n",
    "\n",
    "                    # Check if already collected\n",
    "                    jobs, counter, job_present = id_check(\n",
    "                        driver,\n",
    "                        counter,\n",
    "                        keyword,\n",
    "                        site,\n",
    "                        df_old_jobs,\n",
    "                        jobs,\n",
    "                        job_soup,\n",
    "                        job_id,\n",
    "                        save_path,\n",
    "                        json_file_name,\n",
    "                    )\n",
    "\n",
    "                    if job_present is False:\n",
    "                        jobs, counter = get_each(\n",
    "                            driver,\n",
    "                            counter,\n",
    "                            keyword,\n",
    "                            site,\n",
    "                            jobs,\n",
    "                            job_soup,\n",
    "                            job_id,\n",
    "                            jobs_count,\n",
    "                            page_number,\n",
    "                            page_number_length,\n",
    "                            save_path,\n",
    "                            json_file_name,\n",
    "                        )\n",
    "                    elif job_present is True:\n",
    "                        print(f\"Increase counter\\n  FROM: {counter}\")\n",
    "                        counter += 1\n",
    "                        print(f\"  TO: {counter}.\")\n",
    "                        pass\n",
    "\n",
    "                    # If results open in new window, make sure everything loads\n",
    "                    new_window = check_window(\n",
    "                        driver, main_window, window_before, counter\n",
    "                    )\n",
    "\n",
    "                except:\n",
    "                    print(\n",
    "                        f\"Problem getting job soup for {str(keyword)}: {sys.exc_info()[0]}. Moving on to next search.\"\n",
    "                    )\n",
    "                    # Get html of page with beautifulsoup4\n",
    "                    html = driver.page_source\n",
    "                    print(\"Feeding html driver to BeautifulSoup.\")\n",
    "                    soup = BeautifulSoup(html, \"lxml\")\n",
    "                    jobs, jobs_count, counter = get_page_jobs(\n",
    "                        driver,\n",
    "                        soup,\n",
    "                        counter,\n",
    "                        keyword,\n",
    "                        site,\n",
    "                        page_number,\n",
    "                        page_number_length,\n",
    "                        jobs,\n",
    "                        df_old_jobs,\n",
    "                        save_path,\n",
    "                        json_file_name,\n",
    "                    )\n",
    "\n",
    "        elif int(jobs_count) == 0:\n",
    "            print(\n",
    "                f\"No jobs results found for {str(keyword)}. No error detected. Moving on to next search.\"\n",
    "            )\n",
    "    except:\n",
    "        print(\n",
    "            f\"No jobs results found for {str(keyword)}: {sys.exc_info()[0]}. Moving on to next search.\"\n",
    "        )\n",
    "        pass\n",
    "    finally:\n",
    "        # Save dict as json file\n",
    "        jobs = remove_dupe_dicts(jobs)\n",
    "        print(\n",
    "            f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)} / {int(jobs_count)*int(page_number_length)}\"\n",
    "        )\n",
    "        print(f\"Saving dict to json file as {json_file_name} in location {save_path}\")\n",
    "        with open(save_path + json_file_name, \"w\") as f:\n",
    "            json.dump(jobs, f)\n",
    "\n",
    "    return jobs, jobs_count, counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect individual job links\n",
    "def get_each(\n",
    "    driver,\n",
    "    counter,\n",
    "    keyword,\n",
    "    site,\n",
    "    jobs,\n",
    "    job_soup,\n",
    "    job_id,\n",
    "    jobs_count,\n",
    "    page_number,\n",
    "    page_number_length,\n",
    "    save_path,\n",
    "    json_file_name,\n",
    "):\n",
    "    jobs = remove_dupe_dicts(jobs)\n",
    "\n",
    "    #     # If results open in new window, make sure everything loads\n",
    "    #     new_window = check_window(driver, main_window, window_before, counter)\n",
    "\n",
    "    # Get html of page with beautifulsoup4\n",
    "    html = driver.page_source\n",
    "    print(\"Feeding html driver to BeautifulSoup.\")\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # Get job id, url and age\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Job ID: {job_id}\")\n",
    "    print(\"-\" * 20)\n",
    "    job_url = (\n",
    "        \"https://www.linkedin.com/\"\n",
    "        + job_soup.find(\n",
    "            \"div\", class_=\"full-width artdeco-entity-lockup__title ember-view\"\n",
    "        ).find(\n",
    "            \"a\",\n",
    "            class_=\"disabled ember-view job-card-container__link job-card-list__title\",\n",
    "        )[\n",
    "            \"href\"\n",
    "        ]\n",
    "    )\n",
    "    print(f\"Job URL: {job_url}\")\n",
    "    print(\"-\" * 20)\n",
    "    job_age = (\n",
    "        job_soup.find(\n",
    "            \"li\",\n",
    "            class_=re.compile(\"^job-card-container__listed-time job-card-container*\"),\n",
    "        )\n",
    "        .find(\"time\")\n",
    "        .text.strip()\n",
    "    )\n",
    "    for s in job_age.split():\n",
    "        job_age_number = int(s) if s.isdigit() else job_age\n",
    "    for i in re.findall(\"[\\w +/.]\", job_age):\n",
    "        job_age_unit = str(i) if i.isalpha() else job_age\n",
    "    print(f\"Job Age: {job_age}\")\n",
    "    print(\"-\" * 20)\n",
    "    job_date = (\n",
    "        job_soup.find(\n",
    "            \"li\",\n",
    "            class_=re.compile(\"^job-card-container__listed-time job-card-container*\"),\n",
    "        )\n",
    "        .find(\"time\")[\"datetime\"]\n",
    "        .strip()\n",
    "    )\n",
    "    print(f\"Job Date: {job_date}\")\n",
    "    job_datetime = datetime.strptime(job_date, \"%Y-%m-%d\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    #########################################################\n",
    "    #########################################################\n",
    "    ####################### TAB INFO #######################\n",
    "    #########################################################\n",
    "    #########################################################\n",
    "\n",
    "    try:\n",
    "        # Get main data\n",
    "        (\n",
    "            job_title,\n",
    "            job_description,\n",
    "            company_name,\n",
    "            location,\n",
    "            industry,\n",
    "            sector,\n",
    "            level,\n",
    "            company_url,\n",
    "            emp_type,\n",
    "        ) = get_main(driver, soup)\n",
    "\n",
    "    # In case there is no company tab, -1 for all variables\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No data collected: {sys.exc_info()[0]}.\")\n",
    "        job_title = -1\n",
    "        job_description = -1\n",
    "        company_name = -1\n",
    "        location = -1\n",
    "        industry = -1\n",
    "        sector = -1\n",
    "        level = -1\n",
    "        company_url = -1\n",
    "        emp_type = -1\n",
    "        pass\n",
    "\n",
    "    # Save to dict\n",
    "    print(\"Saving to dict...\")\n",
    "    today = date.today()\n",
    "    jobs.append(\n",
    "        {\n",
    "            \"Search Keyword\": str(keyword),\n",
    "            \"Platform\": site,\n",
    "            \"Job ID\": int(job_id),\n",
    "            \"Job Title\": str(job_title),\n",
    "            \"Company Name\": str(company_name),\n",
    "            \"Location\": str(location),\n",
    "            \"Industry\": str(industry),\n",
    "            \"Sector\": str(sector),\n",
    "            \"Seniority Level\": str(level),\n",
    "            \"Job Description\": str(job_description),\n",
    "            \"Employment Type\": str(emp_type),\n",
    "            \"Company URL\": company_url,\n",
    "            \"Job URL\": job_url,\n",
    "            \"Job Age\": job_age,\n",
    "            \"Job Age Number\": job_age_number,\n",
    "            \"Job Date\": job_date,\n",
    "            \"Collection Date\": (today.strftime(\"%Y-%m-%d\")),\n",
    "        }\n",
    "    )\n",
    "    jobs = remove_dupe_dicts(jobs)\n",
    "\n",
    "    # Save dict as json file\n",
    "    print(\n",
    "        f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)} / {int(jobs_count)*int(page_number_length)}\"\n",
    "    )\n",
    "    print(f\"Saving dict to json file as {json_file_name} in location {save_path}\")\n",
    "    with open(save_path + json_file_name, \"w\") as f:\n",
    "        json.dump(jobs, f)\n",
    "\n",
    "    # Increase counter by 1\n",
    "    print(f\"Increase counter\\n  FROM: {counter}\")\n",
    "    counter += 1\n",
    "    print(f\"  TO: {counter}.\")\n",
    "    with open(f\"{os.getcwd()}/Data/Logs/counter_{keyword}.txt\", \"w\") as f:\n",
    "        f.write(str(counter))\n",
    "\n",
    "    #     # Check current window and go back if required\n",
    "    #     check_window_back(driver, main_window, window_before, new_window, counter)\n",
    "\n",
    "    return jobs, counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get main job data\n",
    "def get_main(driver, soup):\n",
    "\n",
    "    pattern = re.compile(r\"[^A-Za-z0-9, ]+\")\n",
    "    upper_pattern = re.compile(\"[A-Z][^A-Z\\s]+(?:\\s+\\S[^A-Z\\s]*)*\")\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.visibility_of_element_located((By.CLASS_NAME, \"jobs-search-results\"))\n",
    "    )\n",
    "    job_container = soup.find(\"div\", class_=\"jobs-details-top-card__container\")\n",
    "    job_container2 = soup.find(\"div\", id=\"job-details\").find(\n",
    "        \"div\", class_=\"jobs-description-details pt4\"\n",
    "    )\n",
    "    try:\n",
    "        print(\"-\" * 20)\n",
    "        job_title = (\n",
    "            job_container.find(\n",
    "                \"a\", class_=\"jobs-details-top-card__job-title-link ember-view\"\n",
    "            )\n",
    "            .find(\"h2\", class_=\"jobs-details-top-card__job-title t-20 t-black t-normal\")\n",
    "            .text.strip()\n",
    "        )\n",
    "        print(f\"Job Title: {job_title}\")\n",
    "        print(\"-\" * 20)\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job title found.\")\n",
    "        job_title = None\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting job title: {sys.exc_info()[0]}.\")\n",
    "        job_title = -1\n",
    "\n",
    "    try:\n",
    "        print(\"-\" * 20)\n",
    "        job_description = soup.find_all(\"div\", id=\"job-details\")\n",
    "        #         for job_d in job_desc:\n",
    "        # #             job_description = pattern.sub('', str(job_d.text.splitlines()))\n",
    "        #             job_description = str(job_d.text.splitlines())\n",
    "        #             job_description = job_description.strip().replace('  ', '').replace(', ,', '').replace(',,', '').replace('xa0', ' ').replace('&nbsp;', '\\n').replace('Posted by', '')\n",
    "        print(f\"Job Description: {job_description[:50]}\")\n",
    "        print(\"-\" * 20)\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job description found.\")\n",
    "        job_description = None\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting job description: {sys.exc_info()[0]}.\")\n",
    "        job_description = -1\n",
    "\n",
    "    try:\n",
    "        print(\"-\" * 20)\n",
    "        #         company_name = pattern.sub('', job_container.find('div', class_ = 'jobs-details-top-card__content-container').find('a', class_ = re.compile('^jobs-details-top-card__company-url*')).text.strip())\n",
    "        company_name = (\n",
    "            job_container.find(\"div\", class_=\"jobs-details-top-card__content-container\")\n",
    "            .find(\"a\", class_=re.compile(\"^jobs-details-top-card__company-url*\"))\n",
    "            .text.strip()\n",
    "        )\n",
    "        print(f\"Company Name: {company_name}\")\n",
    "        print(\"-\" * 20)\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job company name found.\")\n",
    "        company_name = None\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting company name: {sys.exc_info()[0]}.\")\n",
    "        company_name = -1\n",
    "\n",
    "    try:\n",
    "        print(\"-\" * 20)\n",
    "        #         location = pattern.sub('', soup.find('li', class_ = 'job-card-container__metadata-item').text.strip())\n",
    "        location = soup.find(\n",
    "            \"li\", class_=\"job-card-container__metadata-item\"\n",
    "        ).text.strip()\n",
    "        print(f\"Company Location: {location}\")\n",
    "        print(\"-\" * 20)\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job company location found.\")\n",
    "        location = None\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting company location: {sys.exc_info()[0]}.\")\n",
    "        location = -1\n",
    "\n",
    "    try:\n",
    "        print(\"-\" * 20)\n",
    "        industry_list = (\n",
    "            job_container2.find(\n",
    "                \"h3\", text=re.compile(\"Industry\"), attrs={\"class\": \"t-14 t-bold\"}\n",
    "            )\n",
    "            .find_parent(\"div\", class_=\"jobs-box__group\")\n",
    "            .find_all(\"ul\", class_=\"jobs-box__list jobs-description-details__list\")\n",
    "        )\n",
    "        for ind in industry_list:\n",
    "            ind = ind.text.strip().replace(\"  \", \"\").split(\"\\n\")\n",
    "            industry = [i for i in ind if i]\n",
    "        print(f\"Industry: {industry}\")\n",
    "        print(\"-\" * 20)\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job company industry found.\")\n",
    "        industry = None\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting industry: {sys.exc_info()[0]}.\")\n",
    "        industry = -1\n",
    "\n",
    "    try:\n",
    "        print(\"-\" * 20)\n",
    "        sector_list = (\n",
    "            job_container2.find(\n",
    "                \"h3\", text=re.compile(\"Job Functions\"), attrs={\"class\": \"t-14 t-bold\"}\n",
    "            )\n",
    "            .find_parent(\"div\", class_=\"jobs-box__group\")\n",
    "            .find_all(\"ul\", class_=\"jobs-box__list jobs-description-details__list\")\n",
    "        )\n",
    "        for sec in sector_list:\n",
    "            sec = sec.text.strip().replace(\"  \", \"\").split(\"\\n\")\n",
    "            sector = [s for s in sec if s]\n",
    "        print(f\"Sector: {sector}\")\n",
    "        print(\"-\" * 20)\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job sector found.\")\n",
    "        sector = None\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting sector: {sys.exc_info()[0]}.\")\n",
    "        sector = -1\n",
    "\n",
    "    try:\n",
    "        print(\"-\" * 20)\n",
    "        level = (\n",
    "            job_container2.find(\n",
    "                \"h3\", text=re.compile(\"Seniority Level\"), attrs={\"class\": \"t-14 t-bold\"}\n",
    "            )\n",
    "            .find_parent(\"div\", class_=\"jobs-box__group\")\n",
    "            .find(\"p\", class_=\"t-14 mb3\")\n",
    "            .text.strip()\n",
    "        )\n",
    "        print(f\"Seniority Level: {level}\")\n",
    "        print(\"-\" * 20)\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job seniority level found.\")\n",
    "        level = None\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting seniority level: {sys.exc_info()[0]}.\")\n",
    "        level = -1\n",
    "\n",
    "    try:\n",
    "        print(\"-\" * 20)\n",
    "        company_url = (\n",
    "            \"https://www.linkedin.com/\"\n",
    "            + job_container.find(\n",
    "                \"a\", class_=re.compile(\"^jobs-details-top-card__company-url*\")\n",
    "            )[\"href\"]\n",
    "        )\n",
    "        print(f\"Company URL: {company_url}\")\n",
    "        print(\"-\" * 20)\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job company url found.\")\n",
    "        company_url = None\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting company url: {sys.exc_info()[0]}.\")\n",
    "        company_url = -1\n",
    "\n",
    "    try:\n",
    "        print(\"-\" * 20)\n",
    "        emp_type = (\n",
    "            job_container2.find(\n",
    "                \"h3\", text=re.compile(\"Employment Type\"), attrs={\"class\": \"t-14 t-bold\"}\n",
    "            )\n",
    "            .find_parent(\"div\", class_=\"jobs-box__group\")\n",
    "            .find(\"p\", class_=\"t-14 mb3\")\n",
    "            .text.strip()\n",
    "        )\n",
    "        print(f\"Employment Type: {emp_type}\")\n",
    "        print(\"-\" * 20)\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No job employment type found.\")\n",
    "        emp_type = None\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting employment type: {sys.exc_info()[0]}.\")\n",
    "        emp_type = -1\n",
    "\n",
    "    if (\n",
    "        all(\n",
    "            [\n",
    "                job_title,\n",
    "                job_description,\n",
    "                company_name,\n",
    "                location,\n",
    "                industry,\n",
    "                sector,\n",
    "                level,\n",
    "                company_url,\n",
    "                emp_type,\n",
    "            ]\n",
    "        )\n",
    "        != -1\n",
    "    ):\n",
    "        print(\"Data collected successfully.\")\n",
    "        pass\n",
    "    elif (\n",
    "        any(\n",
    "            [\n",
    "                job_title,\n",
    "                job_description,\n",
    "                company_name,\n",
    "                location,\n",
    "                industry,\n",
    "                sector,\n",
    "                level,\n",
    "                company_url,\n",
    "                emp_type,\n",
    "            ]\n",
    "        )\n",
    "        == -1\n",
    "    ):\n",
    "        print(\n",
    "            f\"Data NOT collected successfully. Sleeping for 10 seconds then trying again: {sys.exc_info()[0]}.\"\n",
    "        )\n",
    "        time.sleep(10)\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable(\n",
    "                    (By.CSS_SELECTOR, \"#main-content > div > section > ul\")\n",
    "                )\n",
    "            )\n",
    "            (\n",
    "                job_title,\n",
    "                job_description,\n",
    "                company_name,\n",
    "                location,\n",
    "                industry,\n",
    "                sector,\n",
    "                level,\n",
    "                company_url,\n",
    "                emp_type,\n",
    "            ) = get_main(driver, soup)\n",
    "        except:\n",
    "            print(\n",
    "                f\"Data NOT collected successfully. Not attempting again: {sys.exc_info()[0]}.\"\n",
    "            )\n",
    "            pass\n",
    "\n",
    "    return (\n",
    "        job_title,\n",
    "        job_description,\n",
    "        company_name,\n",
    "        location,\n",
    "        industry,\n",
    "        sector,\n",
    "        level,\n",
    "        company_url,\n",
    "        emp_type,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get-jobs function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# MAIN FUNCTION TO GET JOB ADS\n",
    "def get_jobs(\n",
    "    keyword,\n",
    "    keyword_url,\n",
    "    site,\n",
    "    driver,\n",
    "    url,\n",
    "    df_old_jobs,\n",
    "    save_path,\n",
    "    json_file_name,\n",
    "    run_all,\n",
    "):\n",
    "\n",
    "    print(\n",
    "        \"-\" * 20, f\"BEGINING OF DATA COLLECTION FOR {keyword.upper()} JOB ADS\", \"-\" * 20\n",
    "    )\n",
    "    main_window = driver.current_window_handle\n",
    "    print(f\"Main Window: {main_window}\")\n",
    "    driver.implicitly_wait(0)\n",
    "    driver.set_window_size(1120, 1000)\n",
    "    counter = 1\n",
    "    see_more_counter = 0\n",
    "    skipped_counters = []\n",
    "\n",
    "    if is_non_zero_file(save_path + json_file_name) is True:\n",
    "        print(\n",
    "            f'A file with the name \"{json_file_name}\" already exists.\\nNew data will be appended to this file.'\n",
    "        )\n",
    "        with open(save_path + json_file_name) as f:\n",
    "            jobs = json.load(f)\n",
    "            jobs = remove_dupe_dicts(jobs)\n",
    "    elif is_non_zero_file(save_path + json_file_name) is False:\n",
    "        jobs = []\n",
    "\n",
    "    # Check internet connection\n",
    "    if not is_connected():\n",
    "        is_connected()\n",
    "\n",
    "    # Go to page and establish main windows\n",
    "    print(f\"Going to url: {url}\")\n",
    "    driver.get(url)\n",
    "\n",
    "    signed_out = WebDriverWait(driver, 10).until(\n",
    "        lambda driver: driver.find_element_by_class_name(\"nav__cta-container\")\n",
    "    )\n",
    "    if signed_out:\n",
    "        # Sign in\n",
    "        print(\"Signing in.\")\n",
    "        signin_button = WebDriverWait(driver, 10).until(\n",
    "            lambda driver: driver.find_element_by_class_name(\"nav__button-secondary\")\n",
    "        )\n",
    "        try:\n",
    "            signin_button.click()\n",
    "        except (ElementNotInteractableException, ElementClickInterceptedException):\n",
    "            time.sleep(1)\n",
    "            signin_button.click()\n",
    "\n",
    "        username = os.environ.get('YAHOO_EMAIL')\n",
    "        password = os.environ.get('YAHOO_PASSWORD')\n",
    "        username_textbox = WebDriverWait(driver, 10).until(\n",
    "            lambda driver: driver.find_element_by_id(\"username\")\n",
    "        )\n",
    "        password_textbox = WebDriverWait(driver, 10).until(\n",
    "            lambda driver: driver.find_element_by_id(\"password\")\n",
    "        )\n",
    "        username_textbox.send_keys(username)\n",
    "        password_textbox.send_keys(password)\n",
    "        password_textbox.send_keys(Keys.ENTER)\n",
    "    else:\n",
    "        print(\"Already signed in.\")\n",
    "        pass\n",
    "\n",
    "    time.sleep(1)\n",
    "    # Get html of page with beautifulsoup4\n",
    "    html = driver.page_source\n",
    "    print(\"Feeding html driver to BeautifulSoup.\")\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    app_content = soup.find(\"main\", class_=\"app__content\")\n",
    "    if app_content != None:\n",
    "        captcha = app_content.find(\"h1\").text\n",
    "        if captcha == \"Let's do a quick security check\":\n",
    "            driver.close()\n",
    "            driver.quit()\n",
    "            initialize()\n",
    "            time.sleep(5)\n",
    "        #             run_all()\n",
    "        elif captcha != \"Let's do a quick security check\":\n",
    "            pass\n",
    "    elif app_content is None:\n",
    "        pass\n",
    "\n",
    "    # Find page number\n",
    "    page_number_length, page_number = find_page_number(driver, soup, counter, keyword)\n",
    "\n",
    "    # Check for errors\n",
    "    try:\n",
    "        page_not_error = WebDriverWait(driver, 10).until_not(\n",
    "            lambda driver: driver.find_element_by_class_name(\n",
    "                re.compile(\"^jobs-search-no-results__image\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        while page_not_error:\n",
    "            # Find job button next siblings with bs4\n",
    "            print(\"Finding job links in page.\")\n",
    "            jobs, jobs_count, counter = get_page_jobs(\n",
    "                driver,\n",
    "                soup,\n",
    "                counter,\n",
    "                keyword,\n",
    "                site,\n",
    "                page_number,\n",
    "                page_number_length,\n",
    "                jobs,\n",
    "                df_old_jobs,\n",
    "                save_path,\n",
    "                json_file_name,\n",
    "            )\n",
    "\n",
    "            # Click on next page button\n",
    "            try:\n",
    "                # Get html of page with beautifulsoup4\n",
    "                html = driver.page_source\n",
    "                print(\"Feeding html driver to BeautifulSoup.\")\n",
    "                soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "                if int(page_number_length) == int(page_number):\n",
    "                    print(\n",
    "                        f\"Last page reached at page {page_number}. End of data collection for {keyword}.\\nGot {len(jobs)} jobs.\"\n",
    "                    )\n",
    "                    break\n",
    "                elif int(page_number_length) != int(page_number):\n",
    "                    next_page_soup = soup.find(\n",
    "                        \"li\",\n",
    "                        class_=\"artdeco-pagination__indicator artdeco-pagination__indicator--number active selected ember-view\",\n",
    "                    ).find_next_sibling(\n",
    "                        \"li\",\n",
    "                        attrs={\n",
    "                            \"class\": re.compile(\n",
    "                                \"^artdeco-pagination__indicator artdeco-pagination__indicator--number*\"\n",
    "                            )\n",
    "                        },\n",
    "                    )\n",
    "                    next_page_xpath = xpath_soup(next_page_soup)\n",
    "                    next_page_button = driver.find_element_by_xpath(next_page_xpath)\n",
    "                    next_page_button.click()\n",
    "                    print(\n",
    "                        f\"Going to next page {int(page_number)+1}.\\nGot {len(jobs)} jobs.\"\n",
    "                    )\n",
    "                    # Find page number\n",
    "                    page_number_length, page_number = find_page_number(\n",
    "                        driver, soup, counter\n",
    "                    )\n",
    "                    # Get jobs\n",
    "                    jobs, jobs_count, counter = get_page_jobs(\n",
    "                        driver,\n",
    "                        soup,\n",
    "                        counter,\n",
    "                        keyword,\n",
    "                        site,\n",
    "                        page_number,\n",
    "                        page_number_length,\n",
    "                        jobs,\n",
    "                        df_old_jobs,\n",
    "                        save_path,\n",
    "                        json_file_name,\n",
    "                    )\n",
    "            except (NoSuchWindowException, NoSuchElementException):\n",
    "                jobs_count = len(jobs)\n",
    "                print(\n",
    "                    f\"Could not go to next page. End of data collection for {keyword}.\\nGot {len(jobs)} jobs.\"\n",
    "                )\n",
    "        else:\n",
    "            jobs_count = len(jobs)\n",
    "            print(f\"No job results found for {str(keyword)}. Moving on to next search.\")\n",
    "    except TimeoutException:\n",
    "        jobs_count = len(jobs)\n",
    "        print(f\"No jobs results found for {str(keyword)}. Moving on to next search.\")\n",
    "\n",
    "    ############################################### FINAL SAVING AND CLEANUP ###############################################\n",
    "\n",
    "    # Save dict as json file\n",
    "    print(\"Final save.\")\n",
    "    jobs = remove_dupe_dicts(jobs)\n",
    "    try:\n",
    "        print(\n",
    "            f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)} / {int(jobs_count)*int(page_number_length)}\"\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)}\")\n",
    "    print(f\"Saving dict to json file as {json_file_name} in location {save_path}\")\n",
    "    with open(save_path + json_file_name, \"w\") as f:\n",
    "        json.dump(jobs, f)\n",
    "\n",
    "    # Save jobs to df\n",
    "    print(f\"Saving {keyword} jobs data to df...\")\n",
    "    if jobs:\n",
    "        df_jobs = pd.DataFrame(jobs)\n",
    "        df_jobs.append(df_old_jobs)\n",
    "        df_jobs = clean_df(df_jobs)\n",
    "    elif not jobs:\n",
    "        print(f\"Jobs dict for {keyword} is empty.\")\n",
    "        df_jobs = pd.DataFrame()\n",
    "        df_jobs.append(df_old_jobs)\n",
    "        try:\n",
    "            df_jobs = clean_df(df_jobs)\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    # Save df as csv\n",
    "    try:\n",
    "        df_jobs, done = save_df(\n",
    "            keyword,\n",
    "            keywords_womenvocc,\n",
    "            keywords_menvocc,\n",
    "            keywords_sector,\n",
    "            df_jobs,\n",
    "            keyword_file.lower(),\n",
    "            save_path.lower(),\n",
    "            df_file_name.lower(),\n",
    "        )\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    return jobs, jobs_count, df_jobs, counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run get_jobs function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    site = \"LinkedIn\"\n",
    "    language = \"English\"\n",
    "    country = \"Netherlands\"  # in Dutch\n",
    "    incognito_enabled = True\n",
    "    headless_enabled = False\n",
    "    proxy_enabled = False\n",
    "\n",
    "    # Get keywords\n",
    "    try:\n",
    "        (\n",
    "            keywords_list,\n",
    "            keywords_womenvocc,\n",
    "            keywords_menvocc,\n",
    "            keywords_sector,\n",
    "        ) = get_keyword_list()\n",
    "\n",
    "    except AttributeError:\n",
    "        time.sleep(1)\n",
    "        (\n",
    "            keywords_list,\n",
    "            keywords_womenvocc,\n",
    "            keywords_menvocc,\n",
    "            keywords_sector,\n",
    "        ) = get_keyword_list()\n",
    "\n",
    "    # Start driver\n",
    "    driver = get_driver(\n",
    "        select_driver, incognito_enabled, headless_enabled, proxy_enabled\n",
    "    )\n",
    "\n",
    "    # Start threads\n",
    "    # if not thread_popup_checker.is_alive():\n",
    "    #     thread_popup_checker.start()\n",
    "    # if not thread_stale_element_checker.is_alive():\n",
    "    #     thread_stale_element_checker.start()\n",
    "    # if not thread_act_cool_checker.is_alive():\n",
    "    #     thread_act_cool_checker.start()\n",
    "\n",
    "    # keywords_list = ['teacher']\n",
    "    # Start for-loop\n",
    "    for keyword in keywords_list:\n",
    "        counter = 1\n",
    "        page_counter = 0\n",
    "\n",
    "        # Check internet connection\n",
    "        if not is_connected():\n",
    "            is_connected()\n",
    "\n",
    "        # Get main info\n",
    "        (\n",
    "            keyword_url,\n",
    "            keyword_file,\n",
    "            save_path,\n",
    "            json_file_name,\n",
    "            df_file_name,\n",
    "            logs_file_name,\n",
    "            filemode,\n",
    "        ) = main_info(keyword, site)\n",
    "\n",
    "        # Set up log\n",
    "        writer = MyWriter(logs_file_name.lower(), filemode)\n",
    "        sys.stdout = writer\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logging.basicConfig(\n",
    "            filename=writer.LOGS_PATH + logs_file_name.lower(),\n",
    "            filemode=filemode,\n",
    "            level=logging.CRITICAL,\n",
    "            format=\"%(asctime)s:%(levelname)s: %(message)s\",\n",
    "        )\n",
    "\n",
    "        # Load and merge existing dict and df\n",
    "        print(f\"All files will be saved in folder: {save_path}\")\n",
    "        jobs, df_old_jobs = load_merge_dict_df(save_path, df_file_name, json_file_name)\n",
    "\n",
    "        # Beging data collection\n",
    "        print(\n",
    "            \"-\" * 20,\n",
    "            f\"BEGINING OF DATA COLLECTION FOR {keyword.upper()} JOB ADS\",\n",
    "            \"-\" * 20,\n",
    "        )\n",
    "        main_window = driver.current_window_handle\n",
    "        print(f\"Main Window: {main_window}\")\n",
    "\n",
    "        # Get jobs\n",
    "        jobs, jobs_count, df_jobs, counter = get_jobs(\n",
    "            keyword.lower(),\n",
    "            keyword_url,\n",
    "            site,\n",
    "            country,\n",
    "            driver,\n",
    "            counter,\n",
    "            main_window,\n",
    "            df_old_jobs,\n",
    "            jobs,\n",
    "            save_path.lower(),\n",
    "            json_file_name.lower(),\n",
    "        )\n",
    "\n",
    "        # Save df as csv\n",
    "        df_jobs, done = save_df(\n",
    "            keyword,\n",
    "            keywords_womenvocc,\n",
    "            keywords_menvocc,\n",
    "            keywords_sector,\n",
    "            df_jobs,\n",
    "            keyword_file.lower(),\n",
    "            save_path.lower(),\n",
    "            df_file_name.lower(),\n",
    "        )\n",
    "        print(\n",
    "            \"-\" * 20, f\"END OF DATA COLLECTION FOR {keyword.upper()} JOB ADS\", \"-\" * 20\n",
    "        )\n",
    "\n",
    "    # Close and quit driver\n",
    "    print(f\"Closing driver.\")\n",
    "    driver.close()\n",
    "    driver.quit()\n",
    "    # sys.stdout.close()\n",
    "    # sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword = 'retail trade'\n",
    "# site = 'Indeed'\n",
    "# from_list = False\n",
    "# df_jobs = post_cleanup(keyword, site, from_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
