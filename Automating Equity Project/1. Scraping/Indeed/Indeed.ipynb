{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "if code_dir_name not in str(Path.cwd()).split('/')[-1]:\n",
    "    for _ in range(5):\n",
    "\n",
    "        parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "        if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "            code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "            if code_dir is not None:\n",
    "                break\n",
    "else:\n",
    "    code_dir = str(Path.cwd())\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some Funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for pop-ups\n",
    "def popup(driver):\n",
    "\n",
    "    # Signup popup\n",
    "    try:\n",
    "        signup_popup = WebDriverWait(driver, 10).until(\n",
    "            lambda driver: driver.find_element_by_id(\"popover-form-container\")\n",
    "        )\n",
    "        if (signup_popup.is_displayed()) and (signup_popup.is_enabled()):\n",
    "            # print('Signup popup detected.')\n",
    "            close_signup_popup = driver.find_element_by_xpath(\n",
    "                '//*[@id=\"popover-x\"]/button'\n",
    "            )\n",
    "            close_signup_popup.click()\n",
    "            # print('Signup popup dismissed.')\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.CLASS_NAME, \"input_submit\"))\n",
    "            )\n",
    "    except errors:\n",
    "\n",
    "        # Accept Alert\n",
    "        try:\n",
    "            click_alert = driver.switch_to.alert()\n",
    "            # print('Alert found.')\n",
    "            click_alert.accept()\n",
    "            # print('Alert accepted.')\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.CLASS_NAME, \"input_submit\"))\n",
    "            )\n",
    "        except errors:\n",
    "\n",
    "            # Cookie popup\n",
    "            try:\n",
    "                cookie_popup = WebDriverWait(driver, 10).until(\n",
    "                    lambda driver: driver.find_element_by_id(\n",
    "                        \"onetrust-accept-btn-handler\"\n",
    "                    )\n",
    "                )\n",
    "                if (cookie_popup.is_displayed()) and (cookie_popup.is_enabled()):\n",
    "                    # print('Cookie detected.')\n",
    "                    cookie_popup.click()\n",
    "                    # print('Cookie accepted.')\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.element_to_be_clickable((By.CLASS_NAME, \"input_submit\"))\n",
    "                    )\n",
    "            except errors:\n",
    "                pass\n",
    "            # Cookie popup\n",
    "            except:\n",
    "                try:\n",
    "                    print(f\"Cookie popup caused unexpected error: {sys.exc_info()[0]}\")\n",
    "                except (NoSuchWindowException, urllib3.exceptions.MaxRetryError):\n",
    "                    pass\n",
    "        # Accept Alert\n",
    "        except:\n",
    "            try:\n",
    "                print(f\"Alert caused unexpected error: {sys.exc_info()[0]}\")\n",
    "            except (NoSuchWindowException, urllib3.exceptions.MaxRetryError):\n",
    "                pass\n",
    "    # Signup popup\n",
    "    except:\n",
    "        try:\n",
    "            print(f\"Signup popup caused unexpected error: {sys.exc_info()[0]}\")\n",
    "        except (NoSuchWindowException, urllib3.exceptions.MaxRetryError):\n",
    "            pass\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find selected language and make sure it's English if available\n",
    "def find_language(driver, soup, language):\n",
    "    language_chooser_list = WebDriverWait(driver, 10).until(\n",
    "        lambda driver: driver.find_elements_by_xpath(\n",
    "            '//*[@id=\"filter-language\"]/button'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for language_chooser in language_chooser_list:\n",
    "        if language_chooser.text == \"Vacaturetaal\":\n",
    "            print(\"Language chooser found.\")\n",
    "            language_chooser.click()\n",
    "\n",
    "            driver.execute_script(\n",
    "                \"arguments[0].setAttribute('aria-expanded', arguments[1])\",\n",
    "                language_chooser,\n",
    "                \"true\",\n",
    "            )\n",
    "\n",
    "            drop_down = language_chooser.get_attribute(\"aria-expanded\")\n",
    "\n",
    "            if str(drop_down) == \"true\":\n",
    "                print(\"Language dropdown menu activated.\")\n",
    "\n",
    "                # Get html of page with beautifulsoup4\n",
    "                # html = driver.page_source\n",
    "                # print('Feeding html driver to BeautifulSoup.')\n",
    "                # soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "                language_dropdown = soup.find_all(\n",
    "                    \"ul\", class_=\"filter-language-menu\", id=\"filter-language-menu\"\n",
    "                )\n",
    "\n",
    "                for language in language_dropdown:\n",
    "                    language_english = xpath_soup(\n",
    "                        language.find(\"a\").find(\n",
    "                            \"span\", class_=\"rbLabel\", text=re.compile(str(language))\n",
    "                        )\n",
    "                    )\n",
    "                    language_button = WebDriverWait(driver, 10).until(\n",
    "                        lambda driver: driver.find_element_by_xpath(language_english)\n",
    "                    )\n",
    "                    language_button.click()\n",
    "                    print(f\"{str(language)} language chosen.\")\n",
    "\n",
    "            elif str(drop_down) == \"false\":\n",
    "                print(\"Language dropdown menu NOT activated.\")\n",
    "\n",
    "                # Get html of page with beautifulsoup4\n",
    "                # html = driver.page_source\n",
    "                # print('Feeding html driver to BeautifulSoup.')\n",
    "                # soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "                # soup = find_language(driver, soup, language)\n",
    "\n",
    "        elif language_chooser.text != \"Vacaturetaal\":\n",
    "            print(\n",
    "                f\"Language chooser NOT found. {language_chooser.text} chooser found instead.\"\n",
    "            )\n",
    "\n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find page number\n",
    "def find_page_number(driver, soup, page_counter):\n",
    "    try:\n",
    "        try:\n",
    "            page_footer = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"pagination\"))\n",
    "            )\n",
    "        except:\n",
    "            print(\"No page footer found. Trying different method.\")\n",
    "            page_footer = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"secondRow\"))\n",
    "            )\n",
    "            page_number_text = str(\n",
    "                soup.find(\"div\", class_=\"secondRow\")\n",
    "                .find(\"div\", class_=\"searchCountContainer\")\n",
    "                .find(\"div\", id=\"searchCountPages\")\n",
    "                .text\n",
    "            )\n",
    "            page_number_length = [\n",
    "                int(s) for s in page_number_text.split() if s.isdigit()\n",
    "            ][1]\n",
    "            page_number = [int(s) for s in page_number_text.split() if s.isdigit()][0]\n",
    "        else:\n",
    "            page_number_list = soup.find(\"ul\", class_=\"pagination-list\").find_all(\"li\")\n",
    "            page_number_length = len(\n",
    "                soup.find(\"ul\", class_=\"pagination-list\").find_all(\"li\")\n",
    "            )\n",
    "            if int(page_number_length) == 1:\n",
    "                print(f\"Only 1 page found for {str(keyword)} jobs.\")\n",
    "                print(f\"CURRENT PAGE NUMBER: 1.\")\n",
    "            elif int(page_number_length) != 1:\n",
    "                print(\n",
    "                    f\"Total of {page_number_length} pages found for {str(keyword)} jobs.\"\n",
    "                )\n",
    "            try:\n",
    "                page_number = int(\n",
    "                    soup.find(\"ul\", class_=\"pagination-list\")\n",
    "                    .find(attrs={\"aria-current\": \"true\"})\n",
    "                    .text\n",
    "                )\n",
    "            except:\n",
    "                page_number = [int(s) for s in page_number_text.split() if s.isdigit()][\n",
    "                    0\n",
    "                ]\n",
    "\n",
    "    except:\n",
    "        page_number = page_counter + 1\n",
    "        page_number_length = 1\n",
    "        print(\n",
    "            f\"Unexpected error getting page number: {sys.exc_info()[0]}. Page number set to page counter + 1: {page_number}\"\n",
    "        )\n",
    "\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"CURRENT PAGE NUMBER: {page_number}.\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    return page_number_length, page_number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect individual job links\n",
    "def get_each(\n",
    "    driver,\n",
    "    soup,\n",
    "    main_window,\n",
    "    window_before,\n",
    "    keyword,\n",
    "    keyword_file,\n",
    "    site,\n",
    "    jobs,\n",
    "    job_soup,\n",
    "    job_id,\n",
    "    jobs_count,\n",
    "    page_counter,\n",
    "    page_number,\n",
    "    page_number_length,\n",
    "    save_path,\n",
    "    json_file_name,\n",
    "):\n",
    "\n",
    "    # If results open in new window, make sure everything loads\n",
    "    new_window = check_window(driver, main_window, window_before)\n",
    "\n",
    "    # Get html of page with beautifulsoup4\n",
    "    # html = driver.page_source\n",
    "    # print('Feeding html driver to BeautifulSoup.')\n",
    "    # soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    # Get job id, url and age\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Job ID: {job_id}\")\n",
    "    print(\"-\" * 20)\n",
    "    job_url = \"https://nl.indeed.com/vacature-bekijken\" + str(\n",
    "        job_soup.find(\"h2\", class_=\"title\").find(\"a\")[\"href\"]\n",
    "    ).replace(\"/rc/clk\", \"\")\n",
    "    print(f\"Job URL: {job_url}\")\n",
    "    print(\"-\" * 20)\n",
    "    job_age = (\n",
    "        soup.find(\"div\", class_=\"result-link-bar\")\n",
    "        .find(\"span\", class_=\"date\")\n",
    "        .find(string=re.compile(r\".*\"))\n",
    "    )\n",
    "    for s in job_age:\n",
    "        job_age_number = int(s) if s.isdigit() else job_age\n",
    "    print(f\"Job Age: {job_age}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    #########################################################\n",
    "    #########################################################\n",
    "    ####################### TAB INFO #######################\n",
    "    #########################################################\n",
    "    #########################################################\n",
    "\n",
    "    # Get main data\n",
    "    # try:\n",
    "    (\n",
    "        job_title,\n",
    "        job_description,\n",
    "        company_name,\n",
    "        location,\n",
    "        rating,\n",
    "        company_url,\n",
    "        emp_type,\n",
    "    ) = get_main(driver, soup)\n",
    "\n",
    "    # In case there is no data, -1 for all variables\n",
    "    # except NoSuchElementException:\n",
    "    #     print(f'No data collected: {sys.exc_info()[0]}.')\n",
    "    #     job_title, job_description, company_name, location, rating, company_url, emp_type = assign_all(7, -1)\n",
    "\n",
    "    # Save to dict\n",
    "    print(\"Saving to dict...\")\n",
    "    today = datetime.date.today()\n",
    "    jobs.append(\n",
    "        {\n",
    "            \"Search Keyword\": str(keyword),\n",
    "            \"Platform\": site,\n",
    "            \"Job ID\": job_id,\n",
    "            \"Job Title\": str(job_title),\n",
    "            \"Company Name\": str(company_name),\n",
    "            \"Location\": str(location),\n",
    "            \"Job Description\": str(job_description),\n",
    "            \"Rating\": float(rating),\n",
    "            \"Employment Type\": str(emp_type),\n",
    "            \"Company URL\": company_url,\n",
    "            \"Job URL\": job_url,\n",
    "            \"Job Age\": job_age,\n",
    "            \"Job Age Number\": job_age_number,\n",
    "            \"Collection Date\": (today.strftime(\"%Y-%m-%d\")),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Save dict as json file\n",
    "    jobs = remove_dupe_dicts(jobs)\n",
    "    print(f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)}\")\n",
    "    print(f\"Saving dict to json file as {json_file_name} in location {save_path}\")\n",
    "    with open(save_path + json_file_name, \"w\") as f:\n",
    "        json.dump(jobs, f)\n",
    "\n",
    "    # Save page counter\n",
    "    if page_counter > 0:\n",
    "        with open(\n",
    "            save_path + f\"{site.lower()}_page_counter_{keyword_file}.txt\", \"w\"\n",
    "        ) as f:\n",
    "            f.write(str(page_counter))\n",
    "\n",
    "    # Check current window and go back if required\n",
    "    check_window_back(driver, main_window, window_before, new_window)\n",
    "\n",
    "    return jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get main job data\n",
    "def get_main(driver, soup):\n",
    "\n",
    "    pattern = re.compile(r\"[^A-Za-z0-9, /]+\")\n",
    "    # WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\\\"vjs-container\\\"]')))\n",
    "\n",
    "    # Get html of page with beautifulsoup4\n",
    "    html = driver.page_source\n",
    "    print(\"Feeding html driver to BeautifulSoup.\")\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    try:\n",
    "        container = soup.find(\"div\", id=\"vjs-container\")\n",
    "        try:\n",
    "            job_title = str(\n",
    "                container.find(\"div\", id=\"vjs-jobinfo\")\n",
    "                .find(\"div\", id=\"vjs-jobtitle\")\n",
    "                .text\n",
    "            )\n",
    "        except NoSuchElementException:\n",
    "            print(f\"No job title found.\")\n",
    "            job_title = -1\n",
    "        except errors:\n",
    "            print(f\"Unexpected error getting job title: {sys.exc_info()[0]}.\")\n",
    "            job_title = -1\n",
    "        else:\n",
    "            print(\"-\" * 20)\n",
    "            print(f\"Job Title: {job_title}\")\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "        try:\n",
    "            job_desc = container.find(\"div\", id=\"vjs-content\").find_all(\n",
    "                \"div\", id=\"vjs-desc\"\n",
    "            )\n",
    "            job_description = \"\"\n",
    "            for job_d in job_desc:\n",
    "                job_description += str(job_d.text).replace(\"\\\\uf0b7\", \"\\\\n\")\n",
    "        except NoSuchElementException:\n",
    "            print(f\"No job description found.\")\n",
    "            job_description = -1\n",
    "        except errors:\n",
    "            print(f\"Unexpected error getting job description: {sys.exc_info()[0]}.\")\n",
    "            job_description = -1\n",
    "        else:\n",
    "            print(\"-\" * 20)\n",
    "            print(f\"Job Description: {job_description[:50]}\")\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "        try:\n",
    "            company_name = pattern.sub(\n",
    "                \"\",\n",
    "                str(\n",
    "                    container.find(\"div\", id=\"vjs-jobinfo\")\n",
    "                    .find(\"span\", id=\"vjs-cn\")\n",
    "                    .text\n",
    "                ).strip(),\n",
    "            )\n",
    "        except NoSuchElementException:\n",
    "            print(f\"No job company name found.\")\n",
    "            company_name = -1\n",
    "        except errors:\n",
    "            print(f\"Unexpected error getting company name: {sys.exc_info()[0]}.\")\n",
    "            company_name = -1\n",
    "        else:\n",
    "            print(\"-\" * 20)\n",
    "            print(f\"Company Name: {company_name}\")\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "        try:\n",
    "            location = pattern.sub(\n",
    "                \"\", str(container.find(\"span\", id=\"vjs-loc\").text).strip()\n",
    "            )\n",
    "        except NoSuchElementException:\n",
    "            print(f\"No job company location found.\")\n",
    "            location = -1\n",
    "        except errors:\n",
    "            print(f\"Unexpected error getting company location: {sys.exc_info()[0]}.\")\n",
    "            location = -1\n",
    "        else:\n",
    "            print(\"-\" * 20)\n",
    "            print(f\"Company Location: {location}\")\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "        try:\n",
    "            company_url = \"https://indeed.nl\" + str(\n",
    "                container.find(\"div\", id=\"vjs-header\")\n",
    "                .find(\"div\", id=\"apply-button-container\")\n",
    "                .find(\"a\")[\"href\"]\n",
    "            )\n",
    "        except NoSuchElementException:\n",
    "            print(f\"No job company url found.\")\n",
    "            company_url = -1\n",
    "        except errors:\n",
    "            print(f\"Unexpected error getting company url: {sys.exc_info()[0]}.\")\n",
    "            company_url = -1\n",
    "        else:\n",
    "            print(\"-\" * 20)\n",
    "            print(f\"Company URL: {company_url}\")\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "    except errors:\n",
    "        print(f\"Unexpected error getting container: {sys.exc_info()[0]}.\")\n",
    "        job_title, job_description, company_name, location, company_url = assign_all(\n",
    "            5, -1\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        premium = (\n",
    "            driver.find_element_by_xpath(\n",
    "                '//*[@id=\"p_81291054cf8540ca\"]/table/tbody/tr/td/span[2]/font/font'\n",
    "            )\n",
    "        ).text\n",
    "\n",
    "        try:\n",
    "            rating = float(\n",
    "                (\n",
    "                    driver.find_element_by_xpath(\n",
    "                        '//*[@id=\"vjs-jobinfo\"]/div[2]/a/span[1]'\n",
    "                    )\n",
    "                ).text\n",
    "            )\n",
    "        except NoSuchElementException:\n",
    "            print(f\"No job rating found.\")\n",
    "            rating = -1\n",
    "        except errors:\n",
    "            print(f\"Unexpected error getting rating: {sys.exc_info()[0]}.\")\n",
    "            rating = -1\n",
    "        else:\n",
    "            print(\"-\" * 20)\n",
    "            print(f\"Rating: {rating}\")\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "        try:\n",
    "            print(\"-\" * 20)\n",
    "            emp_type = (\n",
    "                driver.find_element_by_xpath('//*[@id=\"vjs-jobinfo\"]/div[3]/span')\n",
    "            ).text\n",
    "            print(f\"Employment Type: {emp_type}\")\n",
    "            print(\"-\" * 20)\n",
    "        except NoSuchElementException:\n",
    "            print(f\"No job employment type found.\")\n",
    "            emp_type = -1\n",
    "        except errors:\n",
    "            print(f\"Unexpected error getting employment type: {sys.exc_info()[0]}.\")\n",
    "            emp_type = -1\n",
    "        else:\n",
    "            print(\"-\" * 20)\n",
    "            print(f\"Employment Type: {emp_type}\")\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No premium tag.\")\n",
    "        rating, emp_type = assign_all(2, -1)\n",
    "\n",
    "    except:\n",
    "        print(\n",
    "            f\"Unexpected error getting ratings and employment type: {sys.exc_info()[0]}.\"\n",
    "        )\n",
    "        rating, emp_type = assign_all(2, -1)\n",
    "\n",
    "    # Check if collection successful\n",
    "    imp_collected = [job_title, job_description, company_name, location]\n",
    "    if all((var != int(-1)) or (var != str(\"-1\")) for var in imp_collected):\n",
    "        print(\"Main data collected successfully.\")\n",
    "\n",
    "    elif any((var == int(-1)) or (var == str(\"-1\")) for var in imp_collected):\n",
    "        print(\n",
    "            f\"Main data NOT collected successfully. Sleeping for 10 seconds then trying again.\"\n",
    "        )\n",
    "        time.sleep(10)\n",
    "        try:\n",
    "            (\n",
    "                job_title,\n",
    "                job_description,\n",
    "                company_name,\n",
    "                location,\n",
    "                rating,\n",
    "                company_url,\n",
    "                emp_type,\n",
    "            ) = get_main(driver, soup)\n",
    "        except:\n",
    "            print(\n",
    "                f\"Data NOT collected successfully. Not attempting again: {sys.exc_info()[0]}.\"\n",
    "            )\n",
    "\n",
    "    return (\n",
    "        job_title,\n",
    "        job_description,\n",
    "        company_name,\n",
    "        location,\n",
    "        rating,\n",
    "        company_url,\n",
    "        emp_type,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on next page button\n",
    "def next_page(\n",
    "    driver,\n",
    "    soup,\n",
    "    keyword,\n",
    "    keyword_file,\n",
    "    site,\n",
    "    main_window,\n",
    "    window_before,\n",
    "    counter,\n",
    "    page_counter,\n",
    "    page_number,\n",
    "    page_number_length,\n",
    "    jobs,\n",
    "    df_old_jobs,\n",
    "    save_path,\n",
    "    json_file_name,\n",
    "):\n",
    "\n",
    "    # Get html of page with beautifulsoup4\n",
    "    # html = driver.page_source\n",
    "    # print('Feeding html driver to BeautifulSoup.')\n",
    "    # soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    try:\n",
    "        page_footer = WebDriverWait(driver, 10).until(\n",
    "            lambda driver: driver.find_element_by_class_name(\"pagination\")\n",
    "        )\n",
    "    except:\n",
    "        done = True\n",
    "        print(\"No page footer found.\")\n",
    "    else:\n",
    "\n",
    "        while page_footer.is_enabled():\n",
    "\n",
    "            page_number = int(\n",
    "                soup.find(\"ul\", class_=\"pagination-list\")\n",
    "                .find(attrs={\"aria-current\": \"true\"})\n",
    "                .text\n",
    "            )\n",
    "            page_number_for_next = soup.find(\"ul\", class_=\"pagination-list\").find(\n",
    "                attrs={\"aria-current\": \"true\"}\n",
    "            )\n",
    "            page_number_next = page_number_for_next.find_next(\"li\")\n",
    "\n",
    "            try:\n",
    "                if page_number_next.find(\"a\").find(\"span\")[\"class\"][0] == \"pn\":\n",
    "                    page_number_next_text = page_number_next.find(\"a\")[\"aria-label\"]\n",
    "                    print(\n",
    "                        f\"More page buttons found. Next page number: {page_number_next_text}.\"\n",
    "                    )\n",
    "                    done = False\n",
    "\n",
    "                    try:\n",
    "                        # Determine current page number\n",
    "                        page_number_for_next = soup.find(\n",
    "                            \"ul\", class_=\"pagination-list\"\n",
    "                        ).find(attrs={\"aria-current\": \"true\"})\n",
    "                        page_number_next = page_number_for_next.find_next(\"li\")\n",
    "                    except:\n",
    "                        done = True\n",
    "                        print(\"No next page button (Volgende) found.\")\n",
    "                        break\n",
    "                    else:\n",
    "                        try:\n",
    "                            page_number_next_text = int(\n",
    "                                page_number_next.find(\"a\")[\"aria-label\"]\n",
    "                            )\n",
    "                            page_counter = page_number\n",
    "                            if page_counter > 0:\n",
    "                                with open(\n",
    "                                    save_path\n",
    "                                    + f\"{site.lower()}_page_counter_{keyword_file}.txt\",\n",
    "                                    \"w\",\n",
    "                                ) as f:\n",
    "                                    f.write(str(page_counter))\n",
    "                            (\n",
    "                                jobs,\n",
    "                                jobs_count,\n",
    "                                df_jobs,\n",
    "                                counter,\n",
    "                                page_counter,\n",
    "                                done,\n",
    "                            ) = get_jobs(\n",
    "                                keyword.lower(),\n",
    "                                keyword_url,\n",
    "                                site,\n",
    "                                language,\n",
    "                                driver,\n",
    "                                counter,\n",
    "                                page_counter,\n",
    "                                main_window,\n",
    "                                df_old_jobs,\n",
    "                                jobs,\n",
    "                                save_path.lower(),\n",
    "                                json_file_name.lower(),\n",
    "                                done,\n",
    "                            )\n",
    "                        except:\n",
    "                            # Get html of page with beautifulsoup4\n",
    "                            # html = driver.page_source\n",
    "                            # print('Feeding html driver to BeautifulSoup.')\n",
    "                            # soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "                            page_number_length = len(\n",
    "                                soup.find(\"ul\", class_=\"pagination-list\").find_all(\"li\")\n",
    "                            )\n",
    "                            page_number = int(\n",
    "                                soup.find(\"ul\", class_=\"pagination-list\")\n",
    "                                .find(attrs={\"aria-current\": \"true\"})\n",
    "                                .text\n",
    "                            )\n",
    "                            page_counter = page_number\n",
    "                            print(f\"Last page reached at page {int(page_number)}.\")\n",
    "                            done = True\n",
    "                            break\n",
    "                        else:\n",
    "                            if int(page_number) == int(page_number_next_text):\n",
    "                                page_number = int(\n",
    "                                    soup.find(\"ul\", class_=\"pagination-list\")\n",
    "                                    .find(attrs={\"aria-current\": \"true\"})\n",
    "                                    .text\n",
    "                                )\n",
    "                                print(\n",
    "                                    f\"Page successfully changed to page {int(page_number)}\"\n",
    "                                )\n",
    "                            elif int(page_number) != int(page_number_next_text):\n",
    "                                page_number = int(\n",
    "                                    soup.find(\"ul\", class_=\"pagination-list\")\n",
    "                                    .find(attrs={\"aria-current\": \"true\"})\n",
    "                                    .text\n",
    "                                )\n",
    "                                print(\n",
    "                                    f\"Page not successfully changed to page {int(page_number_next_text)}. Current page {int(page_number)}.\"\n",
    "                                )\n",
    "\n",
    "                elif page_number_next.find(\"a\").find(\"span\")[\"class\"][0] != \"pn\":\n",
    "                    done = True\n",
    "                    print(f\"Last page reached at page {int(page_number)}.\")\n",
    "                    break\n",
    "                break\n",
    "\n",
    "            except:\n",
    "                done = True\n",
    "                print(\"No more page buttons found.\")\n",
    "                break\n",
    "            break\n",
    "        else:\n",
    "            done = True\n",
    "            print(\"No page footer found.\")\n",
    "\n",
    "    # Save page counter\n",
    "    if page_counter > 0:\n",
    "        with open(\n",
    "            save_path + f\"{site.lower()}_page_counter_{keyword_file}.txt\", \"w\"\n",
    "        ) as f:\n",
    "            f.write(str(page_counter))\n",
    "\n",
    "    return int(page_number), jobs, counter, page_counter, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_jobs(\n",
    "    driver,\n",
    "    soup,\n",
    "    keyword,\n",
    "    keyword_file,\n",
    "    site,\n",
    "    main_window,\n",
    "    window_before,\n",
    "    counter,\n",
    "    page_counter,\n",
    "    page_number,\n",
    "    page_number_length,\n",
    "    jobs,\n",
    "    df_old_jobs,\n",
    "    save_path,\n",
    "    json_file_name,\n",
    "    done,\n",
    "):\n",
    "\n",
    "    # Find job button next siblings with bs4\n",
    "    print(\"Finding job links in page.\")\n",
    "\n",
    "    # Get html of page with beautifulsoup4\n",
    "    # html = driver.page_source\n",
    "    # print('Feeding html driver to BeautifulSoup.')\n",
    "    # soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"resultsCol\"))\n",
    "    )\n",
    "\n",
    "    jobs_on_page = soup.find_all(\n",
    "        \"div\",\n",
    "        class_=[\n",
    "            \"jobsearch-SerpJobCard unifiedRow row result clickcard vjs-highlight\",\n",
    "            \"jobsearch-SerpJobCard unifiedRow row result clickcard\",\n",
    "        ],\n",
    "    )\n",
    "    jobs_count = len(jobs_on_page)\n",
    "\n",
    "    # if int(jobs_count) != 0:\n",
    "    print(\n",
    "        f\"{int(jobs_count)} job(s) found on page {page_number} out of {page_number_length} page(s).\"\n",
    "    )\n",
    "    print(f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)}\")\n",
    "\n",
    "    # Get html of page with beautifulsoup4\n",
    "    # html = driver.page_source\n",
    "    # print('Feeding html driver to BeautifulSoup.')\n",
    "    # soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    for job_soup in jobs_on_page:\n",
    "        print(\"-\" * 20)\n",
    "        print(\n",
    "            f\"JOB NUMBER: {int(jobs_on_page.index(job_soup)) + 1} OUT OF {int(jobs_count)} ON PAGE {page_number} OUT OF {page_number_length}.\"\n",
    "        )\n",
    "        page_counter = page_number - 1\n",
    "        if page_counter > 0:\n",
    "            with open(\n",
    "                save_path + f\"{site.lower()}_page_counter_{keyword_file}.txt\", \"w\"\n",
    "            ) as f:\n",
    "                f.write(str(page_counter))\n",
    "        print(\"-\" * 20)\n",
    "        job_xpath = xpath_soup(job_soup)\n",
    "        job_button = WebDriverWait(\n",
    "            driver, 10, ignored_exceptions=StaleElementReferenceException\n",
    "        ).until(lambda driver: driver.find_element_by_xpath(job_xpath))\n",
    "\n",
    "        try:\n",
    "            job_button.click()\n",
    "        except ElementNotInteractableException:\n",
    "            print(\n",
    "                f\"Click did not work. Trying again to get job soup for {str(keyword)}.\"\n",
    "            )\n",
    "            # Get html of page with beautifulsoup4\n",
    "            html = driver.page_source\n",
    "            print(\"Feeding html driver to BeautifulSoup.\")\n",
    "            soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "        # Get job id\n",
    "        job_id = str(job_soup[\"id\"])\n",
    "\n",
    "        # Check if already collected\n",
    "        jobs, job_present = id_check(\n",
    "            driver,\n",
    "            main_window,\n",
    "            window_before,\n",
    "            keyword,\n",
    "            site,\n",
    "            df_old_jobs,\n",
    "            jobs,\n",
    "            job_soup,\n",
    "            job_id,\n",
    "            jobs_count,\n",
    "            save_path,\n",
    "            json_file_name,\n",
    "        )\n",
    "        if job_present is False:\n",
    "            jobs = get_each(\n",
    "                driver,\n",
    "                soup,\n",
    "                main_window,\n",
    "                window_before,\n",
    "                keyword,\n",
    "                keyword_file,\n",
    "                site,\n",
    "                jobs,\n",
    "                job_soup,\n",
    "                job_id,\n",
    "                jobs_count,\n",
    "                page_counter,\n",
    "                page_number,\n",
    "                page_number_length,\n",
    "                save_path,\n",
    "                json_file_name,\n",
    "            )\n",
    "        elif job_present is True:\n",
    "            print(f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)}\")\n",
    "    # elif int(jobs_count) == 0:\n",
    "    #     print(f'No jobs results found for {str(keyword)}. No error detected. Moving on to next search.')\n",
    "\n",
    "    # End of page\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"REACHED END OF JOBS FOR PAGE: {page_number}.\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # Save page counter\n",
    "    if page_counter > 0:\n",
    "        with open(\n",
    "            save_path + f\"{site.lower()}_page_counter_{keyword_file}.txt\", \"w\"\n",
    "        ) as f:\n",
    "            f.write(str(page_counter))\n",
    "\n",
    "    # Save dict as json file\n",
    "    jobs = remove_dupe_dicts(jobs)\n",
    "    print(f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)}\")\n",
    "    print(f\"Saving dict to json file as {json_file_name} in location {save_path}\")\n",
    "    with open(save_path + json_file_name, \"w\") as f:\n",
    "        json.dump(jobs, f)\n",
    "\n",
    "    # Find next page\n",
    "    page_number, jobs, counter, page_counter, done = next_page(\n",
    "        driver,\n",
    "        soup,\n",
    "        keyword,\n",
    "        keyword_file,\n",
    "        site,\n",
    "        main_window,\n",
    "        window_before,\n",
    "        counter,\n",
    "        page_counter,\n",
    "        page_number,\n",
    "        page_number_length,\n",
    "        jobs,\n",
    "        df_old_jobs,\n",
    "        save_path,\n",
    "        json_file_name,\n",
    "    )\n",
    "\n",
    "    # Save the old page counter\n",
    "    if page_counter > 0:\n",
    "        with open(\n",
    "            save_path + f\"{site.lower()}_page_counter_{keyword_file}.txt\", \"w\"\n",
    "        ) as f:\n",
    "            f.write(str(page_counter))\n",
    "\n",
    "    print(f\"End of data collection for {keyword}.\\nGot {len(jobs)} jobs.\")\n",
    "\n",
    "    try:\n",
    "        page_number_length = len(\n",
    "            soup.find(\"ul\", class_=\"pagination-list\").find_all(\"li\")\n",
    "        )\n",
    "        page_number = int(\n",
    "            soup.find(\"ul\", class_=\"pagination-list\")\n",
    "            .find(attrs={\"aria-current\": \"true\"})\n",
    "            .text\n",
    "        )\n",
    "    except:\n",
    "        page_number = counter\n",
    "\n",
    "    # Save dict as json file\n",
    "    done = True\n",
    "    jobs = remove_dupe_dicts(jobs)\n",
    "    jobs_count = len(jobs)\n",
    "    print(\"-\" * 20)\n",
    "    try:\n",
    "        print(f\"Progress for {keyword} job ads (page {int(page_number)}): {len(jobs)}\")\n",
    "    except:\n",
    "        pass\n",
    "    print(f\"Saving dict to json file as {json_file_name} in location {save_path}\")\n",
    "    print(\"-\" * 20)\n",
    "    with open(save_path + json_file_name, \"w\") as f:\n",
    "        json.dump(jobs, f)\n",
    "\n",
    "    return jobs, jobs_count, counter, page_counter, int(page_number), done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get-jobs function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN FUNCTION TO GET JOB ADS\n",
    "def get_jobs(\n",
    "    driver,\n",
    "    keyword,\n",
    "    keyword_url,\n",
    "    keyword_file,\n",
    "    site,\n",
    "    language,\n",
    "    counter,\n",
    "    page_counter,\n",
    "    main_window,\n",
    "    df_old_jobs,\n",
    "    jobs,\n",
    "    save_path,\n",
    "    json_file_name,\n",
    "    df_file_name,\n",
    "    done,\n",
    "):\n",
    "\n",
    "    #     if done is False:\n",
    "    url = f\"https://nl.indeed.com/jobs?q={keyword_url}&l=Noord-Holland&start={page_counter}0\"\n",
    "\n",
    "    # Go to page and establish main windows\n",
    "    print(f\"Going to url: {url}\")\n",
    "    driver.get(url)\n",
    "    window_before = driver.window_handles[0]\n",
    "    print(f\"First Window: {window_before}\")\n",
    "    print(f\"Current page title: {driver.title}\")\n",
    "\n",
    "    if \"Captcha\" in str(driver.title):\n",
    "        print(\"!\" * 30, \"CAPTCHA FOUND\", \"!\" * 30)\n",
    "        act_cool(5, 500)\n",
    "        ! openpyn nl\n",
    "        ! funckymonkey89\n",
    "        # Get jobs\n",
    "        jobs, jobs_count, df_jobs, counter, page_counter, done = get_jobs(\n",
    "            keyword.lower(),\n",
    "            keyword_url,\n",
    "            site,\n",
    "            language,\n",
    "            driver,\n",
    "            counter,\n",
    "            page_counter,\n",
    "            main_window,\n",
    "            df_old_jobs,\n",
    "            jobs,\n",
    "            save_path.lower(),\n",
    "            json_file_name.lower(),\n",
    "            df_file_name.lower(),\n",
    "            done,\n",
    "        )\n",
    "\n",
    "    # Get html of page with beautifulsoup4\n",
    "    html = driver.page_source\n",
    "    print(\"Feeding html driver to BeautifulSoup.\")\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # Find selected language and make sure it's English if available\n",
    "    # soup = find_language(driver, soup, language)\n",
    "\n",
    "    # Find page number\n",
    "    page_number_length, page_number = find_page_number(driver, soup, page_counter)\n",
    "\n",
    "    # Find job button next siblings with bs4\n",
    "    jobs, jobs_count, counter, page_counter, page_number, done = get_page_jobs(\n",
    "        driver,\n",
    "        soup,\n",
    "        keyword,\n",
    "        keyword_file,\n",
    "        site,\n",
    "        main_window,\n",
    "        window_before,\n",
    "        counter,\n",
    "        page_counter,\n",
    "        page_number,\n",
    "        page_number_length,\n",
    "        jobs,\n",
    "        df_old_jobs,\n",
    "        save_path,\n",
    "        json_file_name,\n",
    "        done,\n",
    "    )\n",
    "\n",
    "    # Save dict as json file\n",
    "    jobs = remove_dupe_dicts(jobs)\n",
    "    print(f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)}\")\n",
    "    print(f\"Saving dict to json file as {json_file_name} in location {save_path}\")\n",
    "    with open(save_path + json_file_name, \"w\") as f:\n",
    "        json.dump(jobs, f)\n",
    "    done = True\n",
    "\n",
    "    #     elif done is True:\n",
    "    #         print('Done is True.')\n",
    "    #         jobs_count = len(jobs)\n",
    "    #         try:\n",
    "    #             page_number = int(soup.find('ul', class_ = 'pagination-list').find(attrs={'aria-current': 'true'}).text)\n",
    "    #             page_counter = page_number\n",
    "    #         except:\n",
    "    #             page_counter = 0\n",
    "\n",
    "    ############################################### FINAL SAVING AND CLEANUP ###############################################\n",
    "\n",
    "    # Save dict as json file\n",
    "    print(\"Final save.\")\n",
    "    jobs = remove_dupe_dicts(jobs)\n",
    "    try:\n",
    "        print(\n",
    "            f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)} / {int(jobs_count)*int(page_number_length)}\"\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Progress for {keyword} job ads (page {page_number}): {len(jobs)}\")\n",
    "\n",
    "    print(f\"Saving dict to json file as {json_file_name} in location {save_path}\")\n",
    "    with open(save_path + json_file_name, \"w\") as f:\n",
    "        json.dump(jobs, f)\n",
    "\n",
    "    # Save jobs to df\n",
    "    print(f\"Saving {keyword} jobs data to df...\")\n",
    "    if jobs:\n",
    "        df_jobs = pd.DataFrame(jobs)\n",
    "    elif not jobs:\n",
    "        print(f\"Jobs dict for {keyword} is empty.\")\n",
    "        df_jobs = pd.DataFrame()\n",
    "\n",
    "    # Save df as csv\n",
    "    if not df_jobs.empty:\n",
    "        df_jobs.append(df_old_jobs, ignore_index=True)\n",
    "        df_jobs = clean_df(df_jobs)\n",
    "        df_jobs = save_df(\n",
    "            keyword, df_jobs, save_path, keyword_file.lower(), df_file_name.lower()\n",
    "        )\n",
    "    elif df_jobs.empty:\n",
    "        df_jobs = df_old_jobs\n",
    "\n",
    "    return jobs, jobs_count, df_jobs, counter, page_counter, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run get_jobs function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "def run_all_indeed(\n",
    "    site=\"Indeed\",\n",
    "    language=\"English\",\n",
    "    country=\"Netherlands\",\n",
    "    incognito_enabled=True,\n",
    "    headless_enabled=False,\n",
    "    proxy_enabled=False,\n",
    "    logging_enabled=False,\n",
    "    print_enabled=True,\n",
    "):\n",
    "\n",
    "    # Get keywords\n",
    "    (\n",
    "        keywords_list,\n",
    "        keywords_sector,\n",
    "        keywords_womenvocc,\n",
    "        keywords_menvocc,\n",
    "        keywords_genvsect,\n",
    "        keywords_oldvocc,\n",
    "        keywords_youngvocc,\n",
    "        keywords_agevsect,\n",
    "    ) = get_keyword_list(print_enabled=print_enabled)\n",
    "    # remove_list = ['software', 'service', 'storage', 'information', 'sales', 'culture']\n",
    "    # remove_list = []\n",
    "    # for k in remove_list:\n",
    "    #     keywords_list.remove(k)\n",
    "\n",
    "    # Start driver\n",
    "    driver = get_driver(select_driver)\n",
    "\n",
    "    keywords_list = keywords_oldvocc + keywords_youngvocc\n",
    "    # Start for-loop\n",
    "    for keyword in keywords_list:\n",
    "        counter = 1\n",
    "        recollect = False\n",
    "\n",
    "        # Check internet connection\n",
    "        if not is_connected(driver):\n",
    "            is_connected(driver)\n",
    "\n",
    "        # Get main info\n",
    "        (\n",
    "            keyword_url,\n",
    "            keyword_file,\n",
    "            save_path,\n",
    "            json_file_name,\n",
    "            df_file_name,\n",
    "            logs_file_name,\n",
    "            filemode,\n",
    "        ) = main_info(keyword, site)\n",
    "        done = False\n",
    "\n",
    "        # Set up log\n",
    "        if logging_enabled is True:\n",
    "            print(\"Logging enabled.\")\n",
    "            writer = MyWriter(logs_file_name.lower(), filemode)\n",
    "            sys.stdout = writer\n",
    "            logger = logging.getLogger(__name__)\n",
    "            logging.basicConfig(\n",
    "                filename=writer.LOGS_PATH + logs_file_name.lower(),\n",
    "                filemode=filemode,\n",
    "                level=logging.CRITICAL,\n",
    "                format=\"%(asctime)s:%(levelname)s: %(message)s\",\n",
    "            )\n",
    "        elif logging_enabled is False:\n",
    "            print(\"No logging enabled.\")\n",
    "\n",
    "        # Load and merge existing dict and df\n",
    "        print(f\"All files will be saved in folder: {save_path}{json_file_name}\")\n",
    "        jobs, df_old_jobs = load_merge_dict_df(\n",
    "            keyword, save_path, df_file_name, json_file_name\n",
    "        )\n",
    "\n",
    "        # Beging data collection\n",
    "        print(\n",
    "            \"-\" * 20,\n",
    "            f\"BEGINING OF DATA COLLECTION FOR {keyword.upper()} JOB ADS\",\n",
    "            \"-\" * 20,\n",
    "        )\n",
    "        main_window = driver.current_window_handle\n",
    "        print(f\"Main Window: {main_window}\")\n",
    "\n",
    "        # Load and save page_counter\n",
    "        page_counter_path = (\n",
    "            save_path + f\"{site.lower()}_page_counter_{keyword_file}.txt\"\n",
    "            if recollect is False\n",
    "            else save_path + f\"{site.lower()}_page_counter_done_{keyword_file}.txt\"\n",
    "        )\n",
    "\n",
    "        if is_non_zero_file(page_counter_path) is True:\n",
    "            with open(page_counter_path) as f:\n",
    "                page_counter = int(f.read())\n",
    "        elif is_non_zero_file(page_counter_path) is False:\n",
    "            page_counter = 0\n",
    "        with open(page_counter_path, \"w\") as f:\n",
    "            f.write(str(page_counter))\n",
    "\n",
    "        #         page_counter = 29\n",
    "        print(f\"Starting collection from page number: {page_counter+1}\")\n",
    "\n",
    "        # Start threads\n",
    "        popup_thread(driver, popup, popup_checker)\n",
    "        # stale_element_thread(driver, stale_element, stale_element_checker)\n",
    "        # act_cool_thread(driver, act_cool, act_cool_checker)\n",
    "\n",
    "        # Get jobs\n",
    "        jobs, jobs_count, df_jobs, counter, page_counter, done = get_jobs(\n",
    "            driver,\n",
    "            keyword.lower(),\n",
    "            keyword_url,\n",
    "            keyword_file,\n",
    "            site,\n",
    "            language,\n",
    "            counter,\n",
    "            page_counter,\n",
    "            main_window,\n",
    "            df_old_jobs,\n",
    "            jobs,\n",
    "            save_path.lower(),\n",
    "            json_file_name.lower(),\n",
    "            df_file_name.lower(),\n",
    "            done,\n",
    "        )\n",
    "\n",
    "        # Save df as csv\n",
    "        df_jobs = save_df(\n",
    "            keyword, df_jobs, save_path, keyword_file.lower(), df_file_name.lower()\n",
    "        )\n",
    "        print(\n",
    "            \"-\" * 20, f\"END OF DATA COLLECTION FOR {keyword.upper()} JOB ADS\", \"-\" * 20\n",
    "        )\n",
    "\n",
    "    # Close and quit driver\n",
    "    print(f\"Closing driver.\")\n",
    "    driver.close()\n",
    "    driver.quit()\n",
    "    # thread_popup_checker.terminate()\n",
    "    # thread_stale_element_checker.terminate()\n",
    "    # thread_act_cool_checker.terminate()\n",
    "    # sys.stdout.close()\n",
    "    # sys.exit()\n",
    "\n",
    "    return df_jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    df_jobs_indeed = run_all_indeed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword = 'artsen'\n",
    "# site = 'Indeed'\n",
    "# keyword_from_list = True\n",
    "# site_from_list = False\n",
    "# df_jobs = post_cleanup(keyword, site)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
