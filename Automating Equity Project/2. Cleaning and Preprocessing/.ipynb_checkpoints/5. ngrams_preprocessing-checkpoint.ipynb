{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1122d883",
   "metadata": {},
   "source": [
    "# ATTN: This script should be run AFTER all POS tagging, lemmatization, and stemming (spacy and nltk) completed.\n",
    "# If BERT POS tagging was done, change pkl file loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54cb736",
   "metadata": {},
   "source": [
    "# Use spacy to create bi and trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206a80dd",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM DF_JOBS_TAGS_LEMMAS_STEMS_SPACY_NLTK\n",
    "### IF BERT POS TAGGING WAS DONE, SOURCING FROM DF_JOBS_TAGS_LEMMAS_STEMS_SPACY_NLTK_BERT\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b3608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "sys.path.append(code_dir)\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fe469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup_module.imports import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405cfdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_make_ngrams(sentence, matcher, gram_type):\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    matches = matcher(doc)\n",
    "    matches_list = []\n",
    "\n",
    "    for idx in range(len(matches)):\n",
    "        for match_id, start, end in matches:\n",
    "            if nlp.vocab.strings[match_id].split('_')[0] == gram_type:\n",
    "                match = doc[matches[idx][1]: matches[idx][2]].text\n",
    "                matches_list.append(match.lower())\n",
    "    \n",
    "    return list(set(matches_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d7a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_tags_lemmas_stems_spacy_nltk.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae54c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_jobs['Job Description spacy_1grams_original_list'] = df_jobs['Job Description spacy_tokenized']\n",
    "df_jobs['Job Description spacy_1grams'] = df_jobs['Job Description spacy_tokenized'].apply(\n",
    "    lambda tokens: [\n",
    "        tuple(token.split())\n",
    "        for token in tokens\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a0c8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Spacy bi and trigrams\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "bigram_rules = [\n",
    "    ['NOUN', 'VERB'],\n",
    "    ['VERB', 'NOUN'],\n",
    "    ['ADJ', 'NOUN'],\n",
    "    ['ADJ', 'PROPN'],\n",
    "    # more rules here...\n",
    "]\n",
    "\n",
    "trigram_rules = [\n",
    "    ['VERB', 'ADJ', 'NOUN'],\n",
    "    ['NOUN', 'VERB', 'ADV'],\n",
    "    ['NOUN', 'ADP', 'NOUN'],\n",
    "    # more rules here...\n",
    "]\n",
    "\n",
    "patters_dict = {\n",
    "    'bigram_patterns': [[{'POS': i} for i in j] for j in bigram_rules],\n",
    "    'trigram_patterns': [[{'POS': i} for i in j] for j in trigram_rules],\n",
    "}\n",
    "\n",
    "ngram_dict = {\n",
    "    'bigram': 2,\n",
    "    'trigram': 3,\n",
    "}\n",
    "\n",
    "for ngram_name, ngram_num in ngram_dict.items():\n",
    "    \n",
    "    \n",
    "    matcher.add(f'{ngram_name}_patterns', patters_dict[f'{ngram_name}_patterns'])\n",
    "\n",
    "    df_jobs[f'Job Description spacy_{str(ngram_num)}grams_original_list'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "        lambda sentence: \n",
    "            [\n",
    "                '_'.join(ngram_.split())\n",
    "                for ngram_ in spacy_make_ngrams(sentence, matcher, ngram_name)\n",
    "            ]\n",
    "    )\n",
    "    \n",
    "    df_jobs[f'Job Description spacy_{str(ngram_num)}grams'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "        lambda sentence: \n",
    "            [\n",
    "                tuple(ngram_.split())\n",
    "                for ngram_ in spacy_make_ngrams(sentence, matcher, ngram_name)\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    df_jobs[f'Job Description spacy_{str(ngram_num)}grams_in_sent'] = df_jobs['Job Description spacy_sentencized'].str.lower().replace(\n",
    "        regex = {\n",
    "            re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
    "            for ngrams_list in df_jobs[f'Job Description spacy_{str(ngram_num)}grams_original_list']\n",
    "            for ngram_ in ngrams_list\n",
    "            if '_' in ngram_\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame):\n",
    "        df_jobs.to_pickle(f'{df_save_dir}df_jobs_ngrams_spacy.pkl')\n",
    "\n",
    "        df_jobs.to_csv(f'{df_save_dir}df_jobs_ngrams_spacy.csv', index=False)\n",
    "    else:\n",
    "        print(f'ERORR: LENGTH OF DF = {len(df_jobs)}')\n",
    "\n",
    "    if f'{ngram_name}_patterns' in matcher:\n",
    "        matcher.remove(f'{ngram_name}_patterns')\n",
    "    assert f'{ngram_name}_patterns' not in matcher\n",
    "\n",
    "if len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame):\n",
    "    df_jobs.to_pickle(f'{df_save_dir}df_jobs_ngrams_spacy.pkl')\n",
    "\n",
    "    df_jobs.to_csv(f'{df_save_dir}df_jobs_ngrams_spacy.csv', index=False)\n",
    "else:\n",
    "    print(f'ERORR: LENGTH OF DF = {len(df_jobs)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc3ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Spacy Allgrams\n",
    "df_jobs['Job Description spacy_123grams_original_list'] = df_jobs['Job Description spacy_tokenized'] + df_jobs['Job Description spacy_2grams_original_list'] + df_jobs['Job Description spacy_3grams_original_list']\n",
    "df_jobs['Job Description spacy_123grams'] = df_jobs['Job Description spacy_1grams'] + df_jobs['Job Description spacy_2grams'] + df_jobs['Job Description spacy_3grams']\n",
    "df_jobs['Job Description spacy_123grams_in_sent'] = df_jobs['Job Description spacy_sentencized'].str.lower().replace(\n",
    "    regex = {\n",
    "        re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
    "        for ngrams_list in df_jobs[f'Job Description spacy_123grams_original_list']\n",
    "        for ngram_ in ngrams_list\n",
    "        if '_' in ngram_\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68848b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
    "df_jobs.to_pickle(f'{df_save_dir}df_jobs_ngrams_spacy.pkl')\n",
    "df_jobs.to_csv(f'{df_save_dir}df_jobs_ngrams_spacy.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d258e491",
   "metadata": {},
   "source": [
    "# Use NLTK to create bi and trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e767712",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM DF_JOBS_NGRAMS_SPACY\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f3b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "sys.path.append(code_dir)\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c11531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup_module.imports import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61db031",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_ngrams_spacy.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afc29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nltk_1grams_original_list'] = df_jobs['Job Description nltk_tokenized']\n",
    "df_jobs['Job Description nltk_1grams'] = df_jobs['Job Description nltk_tokenized'].apply(\n",
    "    lambda tokens: [\n",
    "        tuple(token.split())\n",
    "        for token in tokens\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5e1032",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# NLTK bi and trigrams\n",
    "ngram_dict = {\n",
    "    'bigram': 2,\n",
    "    'trigram': 3\n",
    "}\n",
    "\n",
    "for ngram_name, ngram_num in ngram_dict.items():\n",
    "\n",
    "    df_jobs[f'Job Description nltk_{str(ngram_num)}grams_original_list'] = df_jobs['Job Description nltk_tokenized'].apply(\n",
    "        lambda tokens:\n",
    "            list(\n",
    "                '_'.join(ngram_list)\n",
    "                for ngram_list in nltk.ngrams(tokens, ngram_num)\n",
    "            )\n",
    "    )\n",
    "\n",
    "    df_jobs[f'Job Description nltk_{str(ngram_num)}grams'] = df_jobs['Job Description nltk_tokenized'].apply(\n",
    "        lambda tokens: list(nltk.ngrams(tokens, ngram_num))\n",
    "    )\n",
    "\n",
    "    df_jobs[f'Job Description nltk_{str(ngram_num)}grams_in_sent'] = df_jobs['Job Description spacy_sentencized'].str.lower().replace(\n",
    "        regex = {\n",
    "            re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
    "            for ngrams_list in df_jobs[f'Job Description nltk_{str(ngram_num)}grams_original_list']\n",
    "            for ngram_ in ngrams_list\n",
    "            if '_' in ngram_\n",
    "        }\n",
    "    )\n",
    "\n",
    "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
    "df_jobs.to_pickle(f'{df_save_dir}df_jobs_ngrams_spacy_nltk.pkl')\n",
    "df_jobs.to_csv(f'{df_save_dir}df_jobs_ngrams_spacy_nltk.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f3ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# NLTK Allgrams\n",
    "df_jobs['Job Description nltk_123grams_original_list'] = (\n",
    "    df_jobs['Job Description nltk_tokenized']\n",
    "    + df_jobs['Job Description nltk_2grams_original_list']\n",
    "    + df_jobs['Job Description nltk_3grams_original_list']\n",
    ")\n",
    "df_jobs['Job Description nltk_123grams'] = (\n",
    "    df_jobs['Job Description nltk_1grams']\n",
    "    + df_jobs['Job Description nltk_2grams']\n",
    "    + df_jobs['Job Description nltk_3grams']\n",
    ")\n",
    "df_jobs['Job Description nltk_123grams_in_sent'] = (\n",
    "    df_jobs['Job Description spacy_sentencized']\n",
    "    .str.lower()\n",
    "    .replace(\n",
    "        regex={\n",
    "            re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
    "            for ngrams_list in df_jobs[\n",
    "                'Job Description nltk_123grams_original_list'\n",
    "            ]\n",
    "            for ngram_ in ngrams_list\n",
    "            if '_' in ngram_\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17610cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
    "df_jobs.to_pickle(f'{df_save_dir}df_jobs_ngrams_spacy_nltk.pkl')\n",
    "df_jobs.to_csv(f'{df_save_dir}df_jobs_ngrams_spacy_nltk.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1440bd34",
   "metadata": {},
   "source": [
    "# Use Gensim to create bi and trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0494636f",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM DF_JOBS_NGRAMS_SPACY_NLTK\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad74074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "sys.path.append(code_dir)\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c83ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup_module.imports import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9c33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_ngrams_spacy_nltk.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d29aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_jobs['Job Description gensim_1grams_original_list'] = df_jobs['Job Description gensim_tokenized']\n",
    "df_jobs['Job Description gensim_1grams'] = df_jobs['Job Description gensim_tokenized'].apply(\n",
    "    lambda tokens: [\n",
    "        tuple(token.split())\n",
    "        for token in tokens\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becdbf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Gensim bi and trigrams\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n",
    "\n",
    "# Gensim Bigrams\n",
    "bigram = Phraser(Phrases(df_jobs['Job Description gensim_tokenized'], connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
    "df_jobs['Job Description gensim_2grams_original_list_all'] = bigram[df_jobs['Job Description gensim_tokenized']]\n",
    "df_jobs['Job Description gensim_2grams_original_list'] = df_jobs['Job Description gensim_2grams_original_list_all'].apply(\n",
    "    lambda ngrams_list: [\n",
    "        ngram_\n",
    "        for ngram_ in ngrams_list\n",
    "        if len(re.findall('[a-zA-Z]*\\_[a-zA-Z]*', ngram_)) != 0\n",
    "    ]\n",
    ")\n",
    "df_jobs['Job Description gensim_2grams'] = df_jobs['Job Description gensim_2grams_original_list'].apply(\n",
    "    lambda ngrams: [\n",
    "        tuple(ngram.split('_'))\n",
    "        for ngram in ngrams\n",
    "        if '_' in ngram\n",
    "    ]\n",
    ")\n",
    "df_jobs[f'Job Description gensim_2grams_in_sent'] = df_jobs['Job Description spacy_sentencized'].str.lower().apply(\n",
    "    lambda sentence: ' '.join(preprocess_string(re.sub(pattern, ' ', sentence.strip().lower())))\n",
    ").replace(\n",
    "    regex = {\n",
    "        re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
    "        for ngrams_list in df_jobs[f'Job Description gensim_2grams_original_list']\n",
    "        for ngram_ in ngrams_list\n",
    "        if '_' in ngram_\n",
    "    }\n",
    ")\n",
    "\n",
    "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
    "df_jobs.to_pickle(f'{df_save_dir}df_jobs_ngrams_spacy_nltk_gensim.pkl')\n",
    "df_jobs.to_csv(f'{df_save_dir}df_jobs_ngrams_spacy_nltk_gensim.csv', index=False)\n",
    "\n",
    "# Gensim Trigrams\n",
    "trigram = Phraser(Phrases(df_jobs['Job Description gensim_2grams_original_list_all'], connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
    "df_jobs['Job Description gensim_3grams_original_list_all'] = trigram[df_jobs['Job Description gensim_2grams_original_list_all']]\n",
    "df_jobs['Job Description gensim_3grams_original_list'] = df_jobs['Job Description gensim_3grams_original_list_all'].apply(\n",
    "    lambda ngrams_list: [\n",
    "        ngram_\n",
    "        for ngram_ in ngrams_list\n",
    "        if len(re.findall('[a-zA-Z]*\\_[a-zA-Z]*\\_[a-zA-Z]*', ngram_)) != 0\n",
    "    ]\n",
    ")\n",
    "df_jobs['Job Description gensim_3grams'] = df_jobs['Job Description gensim_3grams_original_list'].apply(\n",
    "    lambda ngrams: [\n",
    "        tuple(ngram.split('_'))\n",
    "        for ngram in ngrams\n",
    "        if '_' in ngram\n",
    "    ]\n",
    ")\n",
    "df_jobs[f'Job Description gensim_3grams_in_sent'] = df_jobs['Job Description spacy_sentencized'].str.lower().apply(\n",
    "    lambda sentence: ' '.join(preprocess_string(re.sub(pattern, ' ', sentence.strip().lower())))\n",
    ").replace(\n",
    "    regex = {\n",
    "        re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
    "        for ngrams_list in df_jobs[f'Job Description gensim_3grams_original_list']\n",
    "        for ngram_ in ngrams_list\n",
    "        if '_' in ngram_\n",
    "    }\n",
    ")\n",
    "\n",
    "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
    "df_jobs.to_pickle(f'{df_save_dir}df_jobs_ngrams_spacy_nltk_gensim.pkl')\n",
    "df_jobs.to_csv(f'{df_save_dir}df_jobs_ngrams_spacy_nltk_gensim.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d89c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Gensim Allgrams\n",
    "df_jobs['Job Description gensim_123grams_original_list'] = (\n",
    "    df_jobs['Job Description gensim_tokenized']\n",
    "    + df_jobs['Job Description gensim_2grams_original_list']\n",
    "    + df_jobs['Job Description gensim_3grams_original_list']\n",
    ")\n",
    "df_jobs['Job Description gensim_123grams'] = (\n",
    "    df_jobs['Job Description gensim_1grams']\n",
    "    + df_jobs['Job Description gensim_2grams']\n",
    "    + df_jobs['Job Description gensim_3grams']\n",
    ")\n",
    "df_jobs['Job Description gensim_123grams_in_sent'] = (\n",
    "    df_jobs['Job Description spacy_sentencized']\n",
    "    .str.lower()\n",
    "    .apply(\n",
    "        lambda sentence: ' '.join(\n",
    "            preprocess_string(re.sub(pattern, ' ', sentence.strip().lower()))\n",
    "        )\n",
    "    )\n",
    "    .replace(\n",
    "        regex={\n",
    "            re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
    "            for ngrams_list in df_jobs[\n",
    "                'Job Description gensim_123grams_original_list'\n",
    "            ]\n",
    "            for ngram_ in ngrams_list\n",
    "            if '_' in ngram_\n",
    "        }\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e46331",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
    "df_jobs.to_pickle(f'{df_save_dir}df_jobs_ngrams_spacy_nltk_gensim.pkl')\n",
    "df_jobs.to_csv(f'{df_save_dir}df_jobs_ngrams_spacy_nltk_gensim.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac58f92",
   "metadata": {},
   "source": [
    "# Create word frequencies for uni, bi, and trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c33d9ab",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM DF_JOBS_NGRAMS_SPACY_NLTK_GENSIM\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735bed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "sys.path.append(code_dir)\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1007a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup_module.imports import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8731dee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abs_frequency(row, text_col, ngram_num, embedding_library):\n",
    "\n",
    "    abs_word_freq = defaultdict(int)\n",
    "    for word in row[f'Job Description {embedding_library}_{ngram_num}grams_original_list']:\n",
    "        abs_word_freq[word] += 1\n",
    "\n",
    "        abs_wtd_df = (\n",
    "            pd.DataFrame.from_dict(abs_word_freq, orient='index')\n",
    "            .rename(columns={0: 'abs_word_freq'})\n",
    "            .sort_values(by=['abs_word_freq'], ascending=False)\n",
    "            )\n",
    "        abs_wtd_df.insert(1, 'abs_word_perc', value=abs_wtd_df['abs_word_freq'] / abs_wtd_df['abs_word_freq'].sum())\n",
    "        abs_wtd_df.insert(2, 'abs_word_perc_cum', abs_wtd_df['abs_word_perc'].cumsum())\n",
    "\n",
    "        row[f'Job Description {embedding_library}_{ngram_num}grams_abs_word_freq'] = str(abs_wtd_df['abs_word_freq'].to_dict())\n",
    "        row[f'Job Description {embedding_library}_{ngram_num}grams_abs_word_perc'] = str(abs_wtd_df['abs_word_perc'].to_dict())\n",
    "        row[f'Job Description {embedding_library}_{ngram_num}grams_abs_word_perc_cum'] = str(abs_wtd_df['abs_word_perc_cum'].to_dict())\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a92a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_ngrams_spacy_nltk_gensim.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43206134",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ngrams_list=[1, 2, 3, 123]\n",
    "embedding_libraries_list = ['spacy', 'nltk', 'gensim']\n",
    "\n",
    "for embedding_library, ngram_num in itertools.product(embedding_libraries_list, ngrams_list):\n",
    "    df_jobs = df_jobs.apply(lambda row: get_abs_frequency(row=row, text_col='Job Description spacy_tokenized', ngram_num=ngram_num, embedding_library=embedding_library), axis='columns')\n",
    "\n",
    "    assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
    "    df_jobs.to_pickle(f'{df_save_dir}df_jobs_ngrams_frequency.pkl')\n",
    "    df_jobs.to_csv(f'{df_save_dir}df_jobs_ngrams_frequency.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30affd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
    "df_jobs.to_pickle(f'{df_save_dir}df_jobs_ngrams_frequency.pkl')\n",
    "df_jobs.to_csv(f'{df_save_dir}df_jobs_ngrams_frequency.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1670f255",
   "metadata": {},
   "source": [
    "# Create BoW dictionary, corpus, and tfidf matrix for uni, bi, and trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16989aa0",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM DF_JOBS_NGRAMS_FREQUENCY\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051454b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "sys.path.append(code_dir)\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7586e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup_module.imports import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57359916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_and_dictionary(row, ngram_num, embedding_library):\n",
    "    \n",
    "    ngrams_original_list = row[f'Job Description {embedding_library}_{ngram_num}grams_original_list']\n",
    "    dictionary = Dictionary([ngrams_original_list])\n",
    "    BoW_corpus = [dictionary.doc2bow(ngrams_original_list)]\n",
    "    tfidf = TfidfModel(BoW_corpus, smartirs='ntc')\n",
    "    tfidf_matrix = [tfidf[doc] for doc in BoW_corpus]\n",
    "\n",
    "    row[f'Job Description {embedding_library}_{ngram_num}grams_dictionary'] = dictionary\n",
    "    row[f'Job Description {embedding_library}_{ngram_num}grams_BoW_corpus'] = BoW_corpus\n",
    "    row[f'Job Description {embedding_library}_{ngram_num}grams_tfidf'] = tfidf\n",
    "    row[f'Job Description {embedding_library}_{ngram_num}grams_tfidf_matrix'] = tfidf_matrix\n",
    "    \n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b00a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_ngrams_frequency.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6a2c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ngrams_list=[1, 2, 3, 123]\n",
    "embedding_libraries_list = ['spacy', 'nltk', 'gensim']\n",
    "\n",
    "for embedding_library, ngram_num in itertools.product(embedding_libraries_list, ngrams_list):\n",
    "    df_jobs = df_jobs.apply(\n",
    "        lambda row: get_corpus_and_dictionary(\n",
    "            row=row, ngram_num=ngram_num, embedding_library=embedding_library\n",
    "        ),\n",
    "        axis='columns'\n",
    "    )\n",
    "\n",
    "    assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
    "    df_jobs.to_pickle(f'{df_save_dir}df_jobs_ngrams_frequency.pkl')\n",
    "    df_jobs.to_csv(f'{df_save_dir}df_jobs_ngrams_BoW.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d8769",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
    "df_jobs.to_pickle(f'{df_save_dir}df_jobs_ngrams_frequency.pkl')\n",
    "df_jobs.to_csv(f'{df_save_dir}df_jobs_ngrams_BoW.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6ab61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "study1_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
