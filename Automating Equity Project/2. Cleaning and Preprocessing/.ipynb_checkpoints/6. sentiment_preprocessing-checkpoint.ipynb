{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1122d883",
   "metadata": {},
   "source": [
    "# ATTN: This script should be run AFTER all bi and trigrams (spacy, nltk, and gensim) completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c976d079",
   "metadata": {},
   "source": [
    "# Use spacy and nltk for sentiment scoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206a80dd",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM DF_JOBS_TAGS_LEMMAS_STEMS_SPACY_NLTK\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b3608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "sys.path.append(code_dir)\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a15a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup_module.imports import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d7a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_ngrams_spacy_nltk_gensim.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Spacy sentiment\n",
    "if 'spacytextblob' not in nlp.pipe_names:\n",
    "    nlp.add_pipe('spacytextblob')\n",
    "\n",
    "df_jobs['Job Description spacy_sentiment'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence: float(nlp(sentence)._.blob.polarity)\n",
    "    if isinstance(sentence, str) else np.nan\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76d5f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
    "df_jobs.to_pickle(f'{df_save_dir}df_jobs_sentiment_spacy_nltk.pkl')\n",
    "df_jobs.to_csv(f'{df_save_dir}df_jobs_sentiment_spacy_nltk.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5e1032",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# NLTK sentiment\n",
    "df_jobs['Job Description nltk_sentiment'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence: float(sentim_analyzer.polarity_scores(sentence)['compound'])\n",
    "    if isinstance(sentence, str) else np.nan\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e46331",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
    "df_jobs.to_pickle(f'{df_save_dir}df_jobs_sentiment_spacy_nltk.pkl')\n",
    "df_jobs.to_csv(f'{df_save_dir}df_jobs_sentiment_spacy_nltk.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e65256",
   "metadata": {},
   "source": [
    "# ATTN: This script should be run AFTER all sentiment scoring (spacy and nltk) completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26883380",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM DF_JOBS_SENTIMENT_SPACY_NLTK\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75534c8",
   "metadata": {},
   "source": [
    "# Word2Vec and FastText embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6a5ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import importlib\n",
    "# from pathlib import Path\n",
    "# import numpy as np\n",
    "\n",
    "# mod = sys.modules[__name__]\n",
    "\n",
    "# code_dir = None\n",
    "# code_dir_name = 'Code'\n",
    "# unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "# for _ in range(5):\n",
    "\n",
    "#     parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "#     if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "#         code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "#         if code_dir is not None:\n",
    "#             break\n",
    "\n",
    "# sys.path.append(code_dir)\n",
    "# # %load_ext autoreload\n",
    "# # %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72bd9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from setup_module.imports import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa416b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_sentiment_spacy_nltk.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d145aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_train_word2vec(df, ngram_number, embedding_library, size = 300, words=None, t = time.time(), cores = multiprocessing.cpu_count()):\n",
    "#     if words is None:\n",
    "#         words = [\n",
    "#             'she',\n",
    "#             'he',\n",
    "#             'support',\n",
    "#             'leader',\n",
    "#             'management',\n",
    "#             'team',\n",
    "#             'business',\n",
    "#             'customer',\n",
    "#             'risk',\n",
    "#             'build',\n",
    "#             'computer',\n",
    "#             'programmer',\n",
    "#         ]\n",
    "#     sentences = df[f'Job Description {embedding_library}_{ngram_number}grams_original_list'].values\n",
    "\n",
    "#     w2v_model = Word2Vec(\n",
    "#         sentences=sentences,\n",
    "#         vector_size=size,\n",
    "#         min_count=0,\n",
    "#         window=2,\n",
    "#         sample=6e-5,\n",
    "#         alpha=0.03,\n",
    "#         min_alpha=0.0007,\n",
    "#         negative=20,\n",
    "#         workers=cores - 1,\n",
    "#         sg = 1,\n",
    "#     )\n",
    "\n",
    "#     w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "#     print(f'Time to train the model for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
    "\n",
    "#     w2v_model.train(\n",
    "#         sentences,\n",
    "#         total_examples=w2v_model.corpus_count,\n",
    "#         epochs=30,\n",
    "#         report_delay=1,\n",
    "#     )\n",
    "\n",
    "#     print(f'Time to build w2v_vocab for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
    "#     w2v_vocab = list(w2v_model.wv.index_to_key)\n",
    "\n",
    "#     print(f'Checking words form list of length {len(words)}')\n",
    "#     print(f'WORDS LIST: {words}')\n",
    "\n",
    "#     for word in words:\n",
    "#         print(f'Checking word:\\n{word.upper()}:')\n",
    "#         try:\n",
    "#             # print(f'{sector} 300: {w2v_model_300.wv[word]}')\n",
    "#             # print(f'{sector} 100: {w2v_model_100.wv[word]}')\n",
    "#             print(f'Length of {size} model vobal: {len(w2v_vocab)}')\n",
    "#             print(f'{size} - Positive most similar to {word}: {w2v_model.wv.most_similar(positive=word, topn=5)}')\n",
    "#             print(f'{size} - Negative most similar to {word}: {w2v_model.wv.most_similar(negative=word, topn=5)}')\n",
    "\n",
    "#         except KeyError as e:\n",
    "#             print(e)\n",
    "\n",
    "#     return w2v_vocab, w2v_model\n",
    "\n",
    "# def word2vec_embeddings(sentences, w2v_vocab, w2v_model, size=300):\n",
    "\n",
    "#     sentences = [word for word in sentences if word in w2v_vocab]\n",
    "\n",
    "#     return (\n",
    "#         np.mean(w2v_model.wv[sentences], axis=0)\n",
    "#         if sentences\n",
    "#         else np.zeros(size)\n",
    "#     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60af39e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_train_fasttext(df, ngram_number, embedding_library, size = 300, words=None, t = time.time(), cores = multiprocessing.cpu_count()):\n",
    "#     if words is None:\n",
    "#         words = [\n",
    "#             'she',\n",
    "#             'he',\n",
    "#             'support',\n",
    "#             'leader',\n",
    "#             'management',\n",
    "#             'team',\n",
    "#             'business',\n",
    "#             'customer',\n",
    "#             'risk',\n",
    "#             'build',\n",
    "#             'computer',\n",
    "#             'programmer',\n",
    "#         ]\n",
    "#     sentences = df[f'Job Description {embedding_library}_{ngram_number}grams_original_list'].values\n",
    "\n",
    "#     ft_model = FastText(\n",
    "#         sentences=sentences,\n",
    "#         vector_size=size,\n",
    "#         min_count=0,\n",
    "#         window=2,\n",
    "#         sample=6e-5,\n",
    "#         alpha=0.03,\n",
    "#         min_alpha=0.0007,\n",
    "#         negative=20,\n",
    "#         workers=cores - 1,\n",
    "#         sg = 1,\n",
    "#     )\n",
    "\n",
    "#     ft_model.build_vocab(sentences, progress_per=10000)\n",
    "#     print(f'Time to train the model for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
    "\n",
    "#     ft_model.train(\n",
    "#         sentences,\n",
    "#         total_examples=ft_model.corpus_count,\n",
    "#         epochs=30,\n",
    "#         report_delay=1,\n",
    "#     )\n",
    "\n",
    "#     print(f'Time to build vocab for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
    "#     ft_vocab = list(ft_model.wv.index_to_key)\n",
    "\n",
    "#     print(f'Checking words form list of length {len(words)}')\n",
    "#     print(f'WORDS LIST: {words}')\n",
    "\n",
    "#     for word in words:\n",
    "#         print(f'Checking word:\\n{word.upper()}:')\n",
    "#         try:\n",
    "#             # print(f'{sector} 300: {ft_model_300.wv[word]}')\n",
    "#             # print(f'{sector} 100: {ft_model_100.wv[word]}')\n",
    "#             print(f'Length of {size} model vobal: {len(ft_vocab)}')\n",
    "#             print(f'{size} - Positive most similar to {word}: {ft_model.wv.most_similar(positive=word, topn=5)}')\n",
    "#             print(f'{size} - Negative most similar to {word}: {ft_model.wv.most_similar(negative=word, topn=5)}')\n",
    "\n",
    "#         except KeyError as e:\n",
    "#             print(e)\n",
    "\n",
    "#     return ft_vocab, ft_model\n",
    "\n",
    "# def fasttext_embeddings(sentences, ft_vocab, ft_model, size=300):\n",
    "\n",
    "#     sentences = [word for word in sentences if word in ft_vocab]\n",
    "\n",
    "#     return np.mean(ft_model.wv[sentences], axis=0) if sentences else np.zeros(size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb05b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_glove(glove_file = f'{llm_path}/gensim/glove/glove.840B.300d.txt'):\n",
    "#     embeddings_index = {}\n",
    "#     with open(glove_file, 'r', encoding='utf8') as glove:\n",
    "\n",
    "#         for line in glove:\n",
    "#             values = line.split()\n",
    "#             word = values[0]\n",
    "\n",
    "#             with contextlib.suppress(ValueError):\n",
    "#                 coefs = np.asarray(values[1:], dtype='float32')\n",
    "#                 embeddings_index[word] = coefs\n",
    "#     print(f'Found {len(embeddings_index)} word vectors.')\n",
    "\n",
    "#     return embeddings_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec52618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sent2vec(sentences, embeddings_index=None, external_glove=True, extra_preprocessing_enabled=False):\n",
    "\n",
    "#     if external_glove is False and embeddings_index is None:\n",
    "#         embeddings_index= get_glove()\n",
    "\n",
    "#     if extra_preprocessing_enabled is False:\n",
    "#         words = sentences\n",
    "\n",
    "#     elif extra_preprocessing_enabled is True:\n",
    "#         stop_words = set(sw.words('english'))\n",
    "#         words = str(sentences).lower()\n",
    "#         words = word_tokenize(words)\n",
    "#         words = [w for w in words if (w not in stop_words) and (w.isalpha())]\n",
    "\n",
    "#     M = []\n",
    "\n",
    "#     try:\n",
    "#         for w in words:\n",
    "#             try:\n",
    "#                 M.append(embeddings_index[w])\n",
    "#             except Exception:\n",
    "#                 continue\n",
    "\n",
    "#         M = np.array(M)\n",
    "#         v = M.sum(axis='index')\n",
    "#         return np.zeros(300) if type(v) != np.ndarray else v / np.sqrt((v ** 2).sum())\n",
    "#     except Exception:\n",
    "#         return np.zeros(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc0e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_models_dict = {\n",
    "#     'w2v': [build_train_word2vec, word2vec_embeddings, Word2Vec],\n",
    "#     'ft': [build_train_fasttext, fasttext_embeddings, FastText],\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a730bf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Make embeddings\n",
    "# ngrams_list=[1, 2, 3, 123]\n",
    "# embedding_libraries_list = ['spacy', 'nltk', 'gensim']\n",
    "\n",
    "# for embedding_library, ngram_number in itertools.product(embedding_libraries_list, ngrams_list):\n",
    "#     print(f'Building {embedding_library}_{ngram_number}grams model and vocabulary.')\n",
    "\n",
    "#     for embed_model_name, embed_func_list in embedding_models_dict.items():\n",
    "\n",
    "#         build_train_func, embed_func, model_loader = embed_func_list\n",
    "#         print(f'Building {embed_model_name} from {embed_func.__name__} function.')\n",
    "\n",
    "#         vocab, model = build_train_func(\n",
    "#             df=df_jobs,\n",
    "#             ngram_number=ngram_number,\n",
    "#             embedding_library=embedding_library,\n",
    "#         )\n",
    "\n",
    "#         print(f'Getting {embed_model_name} embeddings.')\n",
    "\n",
    "#         df_jobs[\n",
    "#             f'Job Description {embedding_library}_{ngram_number}grams_mean_{embed_model_name}_embeddings'\n",
    "#         ] = df_jobs[\n",
    "#             f'Job Description {embedding_library}_{ngram_number}grams_original_list'\n",
    "#         ].apply(\n",
    "#             lambda sentences: embed_func(sentences, vocab, model)\n",
    "#         )\n",
    "#         model.save(f'{data_dir}embeddings models/{embedding_library}_{ngram_number}grams_{embed_model_name}_model.model')\n",
    "\n",
    "#         assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
    "#         df_jobs.to_pickle(f'{df_save_dir}df_jobs_for_trainning.pkl')\n",
    "#         df_jobs.to_csv(f'{df_save_dir}df_jobs_for_trainning.csv', index=False)\n",
    "\n",
    "#     # Sent2Vec\n",
    "#     print('Getting sent2vec embeddings.')\n",
    "#     embeddings_index = get_glove()\n",
    "#     df_jobs[f'Job Description {embedding_library}_{ngram_number}grams_sent2vec_embeddings'] = df_jobs[f'Job Description {embedding_library}_{ngram_number}grams'].apply(lambda sentences: sent2vec(sentences, embeddings_index=embeddings_index, external_glove=True, extra_preprocessing_enabled=False))\n",
    "#     print('Done getting sent2vec embeddings.')\n",
    "\n",
    "#     assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
    "#     df_jobs.to_pickle(f'{df_save_dir}df_jobs_for_trainning.pkl')\n",
    "#     df_jobs.to_csv(f'{df_save_dir}df_jobs_for_trainning.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75806adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
    "# df_jobs.to_pickle(f'{df_save_dir}df_jobs_for_trainning.pkl')\n",
    "# df_jobs.to_csv(f'{df_save_dir}df_jobs_for_trainning.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb13b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "study1_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
