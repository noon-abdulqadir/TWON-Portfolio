{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTN: This script uses Google translate to detect job description language. Google translate will limit requests and take a very long time. Only run this script if redoing language detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from scrapped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from spacy.pipeline import Sentencizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a manually collected dictionary of incorrect/faulty keywords in scraped site data\n",
    "with open(f'{scraped_data}CBS/Data/keyword_trans_dict.txt') as f:\n",
    "    keyword_trans_dict = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 114 words to fix\n",
    "len(keyword_trans_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_broken_linkedin_files(glob_path):\n",
    "    fix_list = []\n",
    "    data_dict = {}\n",
    "    data_list = []\n",
    "\n",
    "    if glob_path.endswith('.json'):\n",
    "\n",
    "        with open(glob_path, encoding = 'utf-8') as csv_file_handler:\n",
    "            csv_reader = csv.DictReader(csv_file_handler)\n",
    "\n",
    "            for rows in csv_reader:\n",
    "                first_key = str(list(rows.keys())[0])\n",
    "                key = rows[first_key]\n",
    "                data_dict[key] = rows\n",
    "\n",
    "        for num in data_dict:\n",
    "            data_list.append(data_dict[num])\n",
    "\n",
    "        with open(glob_path, 'w', encoding = 'utf-8') as json_file_handler:\n",
    "            json_file_handler.write(json.dumps(data_list, indent = 4))\n",
    "    \n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_keywords(df_temp):\n",
    "\n",
    "    # This is a manually collected dictionary of incorrect/faulty keywords in scraped site data\n",
    "    with open(f'{scraped_data}CBS/Data/keyword_trans_dict.txt') as f:\n",
    "        keyword_trans_dict = json.load(f)\n",
    "\n",
    "    if len(df_temp) > 0 and isinstance(df_temp, pd.DataFrame):\n",
    "        for key, value in keyword_trans_dict.items():\n",
    "            df_temp.loc[\n",
    "                df_temp[df_temp['Search Keyword'].notnull()]['Search Keyword'].astype(str).apply(\n",
    "                lambda x: x.lower().strip()\n",
    "                ) == str(key).lower().strip(), 'Search Keyword'\n",
    "            ] = str(value).lower().strip()\n",
    "\n",
    "        unfixed = df_temp.loc[\n",
    "            df_temp[df_temp['Search Keyword'].notnull()]['Search Keyword'].astype(str).apply(lambda x: x.lower().strip()).isin([x.lower().strip() for x in list(keyword_trans_dict.keys())])\n",
    "        ]\n",
    "\n",
    "        if len(unfixed) != 0:\n",
    "            for key, value in keyword_trans_dict.items():\n",
    "                for idx, row in df_temp.iterrows():\n",
    "                    if row['Search Keyword'].astype(str).lower().strip() == str(key).lower().strip():\n",
    "                        df_temp.loc[idx, 'Search Keyword'] = str(value).lower().strip()\n",
    "    \n",
    "        unfixed = df_temp.loc[\n",
    "                df_temp[df_temp['Search Keyword'].notnull()]['Search Keyword'].astype(str).apply(lambda x: x.lower().strip()).isin([x.lower().strip() for x in list(keyword_trans_dict.keys())])\n",
    "            ]\n",
    "        if len(unfixed) != 0:\n",
    "            print('Some keywords were not fixed. Please check file unfixed_keywords.txt in data directory.')\n",
    "            with open(f'{data_dir}unfixed_keywords.txt', 'w') as f:\n",
    "                json.dump(unfixed, f)\n",
    "    \n",
    "    return df_temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_paths = []\n",
    "\n",
    "for site in site_list:\n",
    "    glob_paths.extend(glob.glob(f'{scraped_data}/{site}/Data/*.json')+glob.glob(f'{scraped_data}/{site}/Data/*.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 955 json and csv files\n",
    "len(glob_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use paths to open files, fix keywords, and drop unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix list catches all incorrect/faculty keyword search terms\n",
    "\n",
    "fix_list = []\n",
    "\n",
    "# Appended data catches all the fixed and cleaned dfs\n",
    "appended_data = []\n",
    "\n",
    "for glob_path in glob_paths:\n",
    "\n",
    "    if glob_path.endswith('.json'):\n",
    "        try:\n",
    "            df_temp = pd.read_json(glob_path).reset_index(drop=True)\n",
    "        except ValueError:\n",
    "            fix_list.append(glob_path)\n",
    "            if 'scraped_data/LinkedIn/Data/linkedin_jobs_df_' in glob_path:\n",
    "                data_json = fix_broken_linkedin_files(glob_path)\n",
    "                try:\n",
    "                    df_temp = pd.read_json(glob_path).reset_index(drop=True)\n",
    "                except ValueError:\n",
    "                    fix_list.append(glob_path)\n",
    "    elif glob_path.endswith('.csv'):\n",
    "        df_temp = pd.read_csv(glob_path).reset_index(drop=True)\n",
    "\n",
    "    if len(df_temp) > 0 and isinstance(df_temp, pd.DataFrame):\n",
    "        df_temp = fix_keywords(df_temp)\n",
    "        df_temp.reset_index(drop=True, inplace=True)\n",
    "        df_temp.drop(columns=cols, axis='columns', inplace=True, errors='ignore')\n",
    "        df_temp.drop(\n",
    "        df_temp.columns[\n",
    "                df_temp.columns.str.contains(\n",
    "                    'unnamed|index|level', regex=True, case=False, flags=re.I\n",
    "                )\n",
    "            ],\n",
    "            axis='columns',\n",
    "            inplace=True,\n",
    "            errors='ignore',\n",
    "        )\n",
    "    \n",
    "        if glob_path.endswith('.json'):\n",
    "            df_temp.to_json(glob_path, orient='records')\n",
    "        elif glob_path.endswith('.csv'):\n",
    "            df_temp.to_csv(glob_path, index=False)\n",
    "\n",
    "        appended_data.append(df_temp.reset_index(drop=True))\n",
    "\n",
    "# Concatonate list of dfs into one large df_jobs\n",
    "df_jobs = pd.concat(appended_data).reset_index(drop=True)\n",
    "\n",
    "# Save df_jobs to file\n",
    "if len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame):\n",
    "    df_jobs.to_pickle(f'{data_dir}df_jobs_raw.pkl')\n",
    "    \n",
    "    df_jobs.to_csv(f'{data_dir}df_jobs_raw.csv', index=False)\n",
    "else:\n",
    "    print(f'ERORR: LENGTH OF DF = {len(df_jobs)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we couldn't fix some keywords, we add them to list fix_list and write to file\n",
    "if len(fix_list) != 0:\n",
    "    print('Some keywords to fix!')\n",
    "    with open(f'{data_dir}fix_list.txt', 'w') as f:\n",
    "        json.dump(fix_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dfs, len = 527\n",
    "len(appended_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatonate list of dfs into one large df_jobs\n",
    "df_jobs = pd.concat(appended_data).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len = 204113\n",
    "len(df_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df_jobs to file\n",
    "if len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame):\n",
    "    df_jobs.to_pickle(f'{data_dir}df_jobs_raw.pkl')\n",
    "    \n",
    "    df_jobs.to_csv(f'{data_dir}df_jobs_raw.csv', index=False)\n",
    "else:\n",
    "    print(f'ERORR: LENGTH OF DF = {len(df_jobs)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop duplicated and missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM DF_JOBS_RAW\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from spacy.pipeline import Sentencizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_pickle(f'{data_dir}df_jobs_raw.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len = 204113\n",
    "len(df_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean columns\n",
    "df_jobs.columns = df_jobs.columns.to_series().apply(lambda x: str(x).strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA\n",
    "df_jobs.dropna(axis='index', how='all', inplace=True)\n",
    "df_jobs.dropna(axis='columns', how='all', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len = 204113\n",
    "len(df_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates on subset of 'Job Description'\n",
    "df_jobs.drop_duplicates(subset=['Job Description'], keep='first', ignore_index=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len = 62579\n",
    "len(df_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows with missing 'Job Description'\n",
    "df_jobs.drop(\n",
    "    df_jobs[\n",
    "        (df_jobs['Job Description'].isin(nan_list)) | \n",
    "        (df_jobs['Job Description'].isnull()) | \n",
    "        (df_jobs['Job Description'].isna())\n",
    "    ].index, \n",
    "    axis='index',\n",
    "    inplace=True,\n",
    "    errors='ignore'\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len = 62577\n",
    "len(df_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df_jobs to file\n",
    "if len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame):\n",
    "    df_jobs.to_pickle(f'{data_dir}df_jobs_raw_dropped.pkl')\n",
    "    \n",
    "    df_jobs.to_csv(f'{data_dir}df_jobs_raw_dropped.csv', index=False)\n",
    "else:\n",
    "    print(f'ERORR: LENGTH OF DF = {len(df_jobs)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect job description language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM DF_JOBS_RAW_DROPPED\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from spacy.pipeline import Sentencizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_pickle(f'{data_dir}df_jobs_raw_dropped.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62577"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 62577\n",
    "len(df_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62577 entries, 0 to 62576\n",
      "Data columns (total 19 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Search Keyword     62577 non-null  object \n",
      " 1   Platform           62577 non-null  object \n",
      " 2   Job ID             62577 non-null  object \n",
      " 3   Job Title          62577 non-null  object \n",
      " 4   Company Name       62574 non-null  object \n",
      " 5   Location           62577 non-null  object \n",
      " 6   Job Description    62577 non-null  object \n",
      " 7   Rating             3975 non-null   float64\n",
      " 8   Employment Type    61995 non-null  object \n",
      " 9   Company URL        59263 non-null  object \n",
      " 10  Job URL            62577 non-null  object \n",
      " 11  Job Age            62577 non-null  object \n",
      " 12  Job Age Number     62577 non-null  object \n",
      " 13  Collection Date    62577 non-null  object \n",
      " 14  Data Row           58599 non-null  float64\n",
      " 15  Tracking ID        58599 non-null  object \n",
      " 16  Industry           59184 non-null  object \n",
      " 17  Job Date           58602 non-null  object \n",
      " 18  Type of ownership  582 non-null    object \n",
      "dtypes: float64(2), object(17)\n",
      "memory usage: 9.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_jobs.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a manually collected dictionary of incorrect/faulty keywords in scraped site data\n",
    "with open(f'{scraped_data}CBS/Data/keyword_trans_dict.txt') as f:\n",
    "    keyword_trans_dict = json.load(f)\n",
    "\n",
    "if len(df_jobs['Search Keyword'].loc[df_jobs['Search Keyword'].isin(list(keyword_trans_dict.keys()))]) != 0:\n",
    "    df_jobs = fix_keywords(df_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "googletrans_readtime_error = googletrans.client.httpx._client.httpcore._exceptions.ReadTimeout\n",
    "\n",
    "if 'Language' not in df_jobs.columns:\n",
    "    df_jobs['Language'] = np.nan # create Language col and fill it with nan\n",
    "\n",
    "# try:\n",
    "#     time.sleep(60)\n",
    "#     df_jobs['Language'] = df_jobs['Job Description'].apply(lambda x: translator.detect(str(x).lower().strip()).lang)\n",
    "# except:\n",
    "#     time.sleep(3600)\n",
    "#     df_jobs['Language'] = df_jobs['Job Description'].apply(lambda x: translator.detect(str(x).lower().strip()).lang)\n",
    "\n",
    "for idx, row in df_jobs.iterrows():\n",
    "    # This part ensures we don't start lang detection from index 0 if some lang detection was already done\n",
    "    if len(str(row['Job Description'])) != 0:\n",
    "        if type(row['Language']) == float and np.isnan(row['Language']): #if lang is nan, detect language\n",
    "\n",
    "            try:\n",
    "                print(f'Row {idx}: Translation in progress.')\n",
    "#                 time.sleep(10)\n",
    "                df_jobs.loc[idx, 'Language'] = str(translator.detect(str(row['Job Description']).lower().strip()).lang)\n",
    "                print(f'Row {idx}: Translation done.')\n",
    "            except:# googletrans_readtime_error:\n",
    "\n",
    "                if len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame):\n",
    "                    df_jobs.to_pickle(f'{data_dir}df_jobs_raw_language_detected.pkl')\n",
    "\n",
    "                    df_jobs.to_csv(f'{data_dir}df_jobs_raw_language_detected.csv', index=False)\n",
    "                else:\n",
    "                    print(f'ERORR: LENGTH OF DF = {len(df_jobs)}')\n",
    "\n",
    "                print(f'Row {idx}: Sleeping for half an hour starting at {time.strftime(\"%I:%M:%S %p\", time.localtime())}.')\n",
    "                print('-'*30)\n",
    "                time.sleep(1800)\n",
    "                print(f'Row {idx}: Done sleeping.')\n",
    "                print('-'*30)\n",
    "                print(f'Row {idx}: Translation in progress.')\n",
    "                df_jobs.loc[idx, 'Language'] = str(translator.detect(str(row['Job Description']).lower().strip()).lang)\n",
    "                print(f'Row {idx}: Translation done.')\n",
    "\n",
    "        else: # elif lang is not nan, skip and go to next idx\n",
    "            continue\n",
    "\n",
    "if len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame):\n",
    "    df_jobs.to_pickle(f'{data_dir}df_jobs_raw_language_detected.pkl')\n",
    "\n",
    "    df_jobs.to_csv(f'{data_dir}df_jobs_raw_language_detected.csv', index=False)\n",
    "else:\n",
    "    print(f'ERORR: LENGTH OF DF = {len(df_jobs)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nl              44863\n",
       "en              17591\n",
       "de                 53\n",
       "fr                 36\n",
       "['nl', 'en']        9\n",
       "['en', 'nl']        8\n",
       "pl                  5\n",
       "id                  4\n",
       "da                  4\n",
       "tr                  1\n",
       "['nl', 'af']        1\n",
       "st                  1\n",
       "af                  1\n",
       "Name: Language, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nl = 44863, en = 17591, ['en', 'nl'] = 8\n",
    "df_jobs['Language'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame):\n",
    "    df_jobs.to_pickle(f'{data_dir}df_jobs_raw_language_detected.pkl')\n",
    "\n",
    "    df_jobs.to_csv(f'{data_dir}df_jobs_raw_language_detected.csv', index=False)\n",
    "else:\n",
    "    print(f'ERORR: LENGTH OF DF = {len(df_jobs)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "study1_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
