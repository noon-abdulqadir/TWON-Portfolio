{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d4c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "if code_dir_name not in str(Path.cwd()).split('/')[-1]:\n",
    "    for _ in range(5):\n",
    "\n",
    "        parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "        if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "            code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "            if code_dir is not None:\n",
    "                break\n",
    "else:\n",
    "    code_dir = str(Path.cwd())\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef3f604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/Automating_Equity1/lib/python3.10/site-packages/outdated/utils.py:14: OutdatedPackageWarning: The package pingouin is out of date. Your version is 0.5.3, the latest is 0.5.4.\n",
      "Set the environment variable OUTDATED_IGNORE=1 to disable these warnings.\n",
      "  return warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e9e0b5635048d4af1206c820a2e34e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "from setup_module.estimators_get_pipe import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3ccdfed",
   "metadata": {},
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b36fe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    }
   ],
   "source": [
    "# Variables\n",
    "method = 'Supervised'\n",
    "classifiers_type = 'all'\n",
    "if classifiers_type == 'nonlinear':\n",
    "    classifiers_pipe = classifiers_pipe_nonlinear\n",
    "elif classifiers_type == 'linear':\n",
    "    classifiers_pipe = classifiers_pipe_linear\n",
    "elif classifiers_type == 'ensemble':\n",
    "    classifiers_pipe = classifiers_pipe_ensemble\n",
    "elif classifiers_type == 'all':\n",
    "    classifiers_pipe = classifiers_pipe\n",
    "\n",
    "results_save_path = f'{models_save_path}{method} Results/'\n",
    "with open(f'{data_dir}{method}_results_save_path.txt', 'w') as f:\n",
    "    f.write(results_save_path)\n",
    "if not os.path.exists(results_save_path):\n",
    "    os.makedirs(results_save_path)\n",
    "done_xy_save_path = f'{results_save_path}Search+Xy/'\n",
    "with open(f'{data_dir}{method}_done_xy_save_path.txt', 'w') as f:\n",
    "    f.write(done_xy_save_path)\n",
    "if not os.path.exists(done_xy_save_path):\n",
    "    os.makedirs(done_xy_save_path)\n",
    "\n",
    "t = time.time()\n",
    "n_jobs = -1\n",
    "n_splits = 10\n",
    "n_repeats = 3\n",
    "random_state = 42\n",
    "refit = True\n",
    "class_weight = 'balanced'\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=n_splits, n_repeats=n_repeats, random_state=random_state\n",
    ")\n",
    "scoring = 'recall'\n",
    "scores = [\n",
    "    'recall', 'accuracy', 'f1', 'roc_auc',\n",
    "    'explained_variance', 'matthews_corrcoef'\n",
    "]\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score, zero_division=0),\n",
    "    'recall_score': make_scorer(recall_score, zero_division=0),\n",
    "    'accuracy_score': make_scorer(accuracy_score, zero_division=0),\n",
    "}\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "metrics_dict = {\n",
    "    f'{scoring.title()} Best Score': np.nan,\n",
    "    f'{scoring.title()} Best Threshold': np.nan,\n",
    "    'Train - Mean Cross Validation Score': np.nan,\n",
    "    f'Train - Mean Cross Validation - {scoring.title()}': np.nan,\n",
    "    f'Train - Mean Explained Variance - {scoring.title()}': np.nan,\n",
    "    'Test - Mean Cross Validation Score': np.nan,\n",
    "    f'Test - Mean Cross Validation - {scoring.title()}': np.nan,\n",
    "    f'Test - Mean Explained Variance - {scoring.title()}': np.nan,\n",
    "    'Explained Variance': np.nan,\n",
    "    'Accuracy': np.nan,\n",
    "    'Balanced Accuracy': np.nan,\n",
    "    'Precision': np.nan,\n",
    "    'Average Precision': np.nan,\n",
    "    'Recall': np.nan,\n",
    "    'F1-score': np.nan,\n",
    "    'Matthews Correlation Coefficient': np.nan,\n",
    "    'Brier Score': np.nan,\n",
    "    'Fowlkes–Mallows Index': np.nan,\n",
    "    'R2 Score': np.nan,\n",
    "    'ROC': np.nan,\n",
    "    'AUC': np.nan,\n",
    "    'Log Loss/Cross Entropy': np.nan,\n",
    "    'Cohen’s Kappa': np.nan,\n",
    "    'Geometric Mean': np.nan,\n",
    "    'Classification Report': np.nan,\n",
    "    'Imbalanced Classification Report': np.nan,\n",
    "    'Confusion Matrix': np.nan,\n",
    "    'Normalized Confusion Matrix': np.nan,\n",
    "}\n",
    "\n",
    "# Transformer variables\n",
    "max_length = 512\n",
    "returned_tensor = 'pt'\n",
    "cpu_counts = torch.multiprocessing.cpu_count()\n",
    "device = torch.device('mps') if torch.has_mps and torch.backends.mps.is_built() and torch.backends.mps.is_available(\n",
    ") else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device_name = str(device.type)\n",
    "print(f'Using {device_name.upper()}')\n",
    "# Set random seed\n",
    "random_state = 42\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "cores = multiprocessing.cpu_count()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f3c5be4",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f83df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_files(\n",
    "    results_save_path= results_save_path,\n",
    "    estimator_names_list=None,\n",
    "    vectorizer_names_list=None,\n",
    "    classifier_names_list=None,\n",
    "):\n",
    "    if estimator_names_list is None:\n",
    "        estimator_names_list = []\n",
    "\n",
    "    print(f'Searching for existing estimators in directory.')\n",
    "\n",
    "    for estimators_file in tqdm.tqdm(glob.glob(f'{results_save_path}*.*')):\n",
    "        if f'{method} Estimator - ' in estimators_file:\n",
    "\n",
    "            col=estimators_file.split(f'{method} Estimator - ')[-1].split(' - ')[0]\n",
    "            vectorizer_name=estimators_file.split(f'{col} - ')[-1].split(' + ')[0]\n",
    "            classifier_name=estimators_file.split(f'{vectorizer_name} + ')[-1].split(' (Save_protocol=')[0]\n",
    "\n",
    "            estimator_names_list.append(f'{col} - {vectorizer_name} + {classifier_name}')\n",
    "\n",
    "    return (\n",
    "        list(set(estimator_names_list))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6795da8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to place Xy and CV data in df and save\n",
    "def save_Xy(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    X_val, y_val,\n",
    "    col,\n",
    "    models_save_path=models_save_path, results_save_path=results_save_path,\n",
    "    method=method, done_xy_save_path=done_xy_save_path,\n",
    "    compression=None, protocol=None, path_suffix=None, data_dict=None\n",
    "):\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {col} - (Save_protocol={protocol}).pkl'\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "\n",
    "    # Check data\n",
    "    check_consistent_length(X_train, y_train)\n",
    "    check_consistent_length(X_test, y_test)\n",
    "    check_consistent_length(X_val, y_val)\n",
    "\n",
    "    # Make df_train_data\n",
    "    df_train_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "        },\n",
    "    )\n",
    "    # Make df_test_data\n",
    "    df_test_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "        },\n",
    "    )\n",
    "    # Make df_test_data\n",
    "    df_val_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_val': X_val,\n",
    "            'y_val': y_val,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Assign dfs to variables\n",
    "    data_dict['df_train_data'] = df_train_data\n",
    "    data_dict['df_test_data'] = df_test_data\n",
    "    data_dict['df_val_data'] = df_val_data\n",
    "\n",
    "    # Save files\n",
    "    print('='*20)\n",
    "    for file_name, file_ in data_dict.items():\n",
    "        save_path = f'{models_save_path}{file_name}{path_suffix}'\n",
    "        print(f'Saving Xy {file_name}')\n",
    "        file_.to_pickle(\n",
    "            save_path, protocol=protocol\n",
    "        )\n",
    "    print(f'Done saving Xy!\\n{list(data_dict.keys())}')\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b731ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    X_val, y_val,\n",
    "):\n",
    "    # Get train class weights\n",
    "    train_class_weights = compute_class_weight(class_weight = class_weight, classes = np.unique(y_train), y = y_train)\n",
    "    train_class_weights_ratio = train_class_weights[0]/train_class_weights[1]\n",
    "    train_class_weights_dict = dict(zip(np.unique(y_train), train_class_weights))\n",
    "\n",
    "    # Get train class weights\n",
    "    test_class_weights = compute_class_weight(class_weight = class_weight, classes = np.unique(y_test), y = y_test)\n",
    "    test_class_weights_ratio = test_class_weights[0]/test_class_weights[1]\n",
    "    test_class_weights_dict = dict(zip(np.unique(y_test), test_class_weights))\n",
    "\n",
    "    # Get val class weights\n",
    "    val_class_weights = compute_class_weight(class_weight = class_weight, classes = np.unique(y_val), y = y_val)\n",
    "    val_class_weights_ratio = val_class_weights[0]/val_class_weights[1]\n",
    "    val_class_weights_dict = dict(zip(np.unique(y_val), val_class_weights))\n",
    "\n",
    "    return (\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd883382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_Xy(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    X_val, y_val,\n",
    "    train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "    test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "    val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "):\n",
    "    # Check for consistent length\n",
    "    check_consistent_length(X_train, y_train)\n",
    "    check_consistent_length(X_test, y_test)\n",
    "    check_consistent_length(X_val, y_val)\n",
    "\n",
    "    print('Done splitting data into training and testing sets.')\n",
    "    print('='*20)\n",
    "    print(f'Training set shape: {y_train.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Training set example:\\n{X_train[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Testing set shape: {y_test.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing set example:\\n{X_test[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Validation set shape: {y_val.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation set example:\\n{X_val[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Training data class weights:\\nRatio = {train_class_weights_ratio:.2f} (0 = {train_class_weights[0]:.2f}, 1 = {train_class_weights[1]:.2f})')\n",
    "    print('-'*10)\n",
    "    print(f'Testing data class weights:\\nRatio = {test_class_weights_ratio:.2f} (0 = {test_class_weights[0]:.2f}, 1 = {test_class_weights[1]:.2f})')\n",
    "    print('-'*10)\n",
    "    print(f'Validation data class weights:\\nRatio = {val_class_weights_ratio:.2f} (0 = {val_class_weights[0]:.2f}, 1 = {val_class_weights[1]:.2f})')\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f3840ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, col, text_col=text_col, analysis_columns=analysis_columns):\n",
    "\n",
    "    train_ratio = 0.75\n",
    "    test_ratio = 0.10\n",
    "    validation_ratio = 0.15\n",
    "    test_split = test_size = 1 - train_ratio\n",
    "    validation_split = test_ratio / (test_ratio + validation_ratio)\n",
    "\n",
    "    # Split\n",
    "    print('='*20)\n",
    "    print('Splitting data into training and testing:')\n",
    "    print(f'Ratios: train_size = {train_ratio}, test size = {test_ratio}')\n",
    "\n",
    "    df = df.dropna(subset=analysis_columns, how='any')\n",
    "    df = df.loc[df[text_col].progress_apply(len) >= 5]\n",
    "    print(f'DF length: {len(df)}')\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        df, train_size=1-test_split, test_size=test_split, random_state=random_state\n",
    "    )\n",
    "    val, test = train_test_split(\n",
    "        test, test_size=validation_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train = np.array(list(train[text_col].astype('str').values))\n",
    "    y_train = column_or_1d(train[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    X_test = np.array(list(test[text_col].astype('str').values))\n",
    "    y_test = column_or_1d(test[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    X_val = np.array(list(val[text_col].astype('str').values))\n",
    "    y_val = column_or_1d(val[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    # Get class weights\n",
    "    (\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    ) = get_class_weights(\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "    )\n",
    "    # Print info\n",
    "    print_Xy(\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        train, X_train, y_train,\n",
    "        test, X_test, y_test,\n",
    "        val, X_val, y_val,\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f41e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Xy(\n",
    "    col,\n",
    "    models_save_path=models_save_path, results_save_path=results_save_path, method=method,\n",
    "    path_suffix=None, data_dict=None, protocol=None,\n",
    "):\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {col} - (Save_protocol={protocol}).pkl'\n",
    "\n",
    "    print('+'*30)\n",
    "    print(f'{\"=\"*10} Loading Xy from previous for {col} {\"=\"*10}')\n",
    "    print('+'*30)\n",
    "    # Read all dfs\n",
    "    for file_path in glob.glob(f'{models_save_path}*{path_suffix}'):\n",
    "        file_name = file_path.split(f'{models_save_path}')[-1].split(path_suffix)[0]\n",
    "        print(f'Loading {file_name}')\n",
    "        if path_suffix in file_path and 'df_' in file_name and 'cv_results' not in file_name:\n",
    "            data_dict[file_name] = pd.read_pickle(file_path)\n",
    "\n",
    "    # Train data\n",
    "    df_train_data = data_dict['df_train_data']\n",
    "    X_train = df_train_data['X_train'].values\n",
    "    y_train = df_train_data['y_train'].values\n",
    "    # Test data\n",
    "    df_test_data = data_dict['df_test_data']\n",
    "    X_test = df_test_data['X_test'].values\n",
    "    y_test = df_test_data['y_test'].values\n",
    "    # Val data\n",
    "    df_val_data = data_dict['df_val_data']\n",
    "    X_val = df_val_data['X_val'].values\n",
    "    y_val = df_val_data['y_val'].values\n",
    "\n",
    "    print(f'Done loading Xy from previous for {col}!')\n",
    "\n",
    "    # Get class weights\n",
    "    (\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    ) = get_class_weights(\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "    )\n",
    "    # Print info\n",
    "    print_Xy(\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )\n",
    "    print(f'Done loading Xy from previous for {col}!')\n",
    "\n",
    "    return (\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights_dict, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1f74f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize unusual classifiers after fitting\n",
    "def normalize_after_fitting(estimator, X_train, y_train, X_test, y_test, grid_search, searchcv, vectorizer_name, classifier_name):\n",
    "    # Classifiers to normalize = ['GaussianNB', 'DecisionTreeClassifier', 'RandomForestClassifier', 'ExtraTreesClassifier', 'AdaBoostClassifier', 'GradientBoostingClassifier', 'XGBClassifier', 'Perceptron', 'Sequential']\n",
    "\n",
    "    # Get feature importance if classifier provides them and use as X\n",
    "    if any(hasattr(estimator, feature_attr) for feature_attr in ['feature_importances_', 'coef_']):\n",
    "        feature_selector = SelectFromModel(estimator, prefit=True)\n",
    "        X_train = feature_selector.transform(X_train)\n",
    "        X_test = X_test[:, feature_selector.get_support()]\n",
    "        df_feature_importances = pd.DataFrame(\n",
    "            {\n",
    "                'features': X_test.values,\n",
    "                'feature_importances': estimator.feature_importances_\n",
    "            }\n",
    "        )\n",
    "        df_feature_importances = df_feature_importances.sort_values('feature_importances', ascending=False)\n",
    "        print(df_feature_importances.head(20))\n",
    "        print(f'Best estimator has feature_importances of shape:\\n{estimator}')\n",
    "    else:\n",
    "        df_feature_importances = None\n",
    "\n",
    "    # For perceptron: calibrate classifier to get prediction probabilities\n",
    "    if (not hasattr(searchcv, 'predict_proba') and not hasattr(searchcv, '_predict_proba_lr') and hasattr(searchcv, 'decision_function')) or classifier_name == 'Perceptron' or estimator.__class__.__name__ == 'Perceptron':\n",
    "        path_suffix = f' - {col} - {vectorizer_name} + {classifier_name} (Save_protocol={protocol}).pkl'\n",
    "        data_dict = {\n",
    "            'Estimator': estimator,\n",
    "            'Grid Search': grid_search,\n",
    "            'SearchCV': searchcv,\n",
    "        }\n",
    "        for file_name, file_ in data_dict.items():\n",
    "            with open(\n",
    "                f'{results_save_path}{method} {file_name}{path_suffix}', 'wb'\n",
    "            ) as f:\n",
    "                joblib.dump(file_, f, compress=False, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(f'Saved {method} {file_name}{path_suffix} to {results_save_path}')\n",
    "\n",
    "        searchcv = CalibratedClassifierCV(\n",
    "            searchcv, cv=cv, method='sigmoid'\n",
    "        ).fit(X_train, y_train)\n",
    "\n",
    "    # For Sequential classifier: compile for binary classification, optimize with adam and score on recall\n",
    "    if classifier_name == 'Sequential':\n",
    "        searchcv.compile(\n",
    "            loss='binary_crossentropy', optimizer='adamw', metrics=list(scoring)\n",
    "        ).fit(X_train, y_train)\n",
    "\n",
    "    return (\n",
    "        estimator, X_train, y_train, X_test, y_test, searchcv, df_feature_importances\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e46bb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to place Xy and CV data in df and save\n",
    "def save_Xy_search_cv_estimator(\n",
    "    grid_search, searchcv, cv_results,\n",
    "    X_train, y_train, y_train_pred, y_train_pred_prob,\n",
    "    X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "    X_val, y_val, y_val_pred, y_val_pred_prob,\n",
    "    df_feature_importances,\n",
    "    estimator,\n",
    "    col, vectorizer_name, classifier_name,\n",
    "    results_save_path=results_save_path,\n",
    "    method=method, done_xy_save_path=done_xy_save_path,\n",
    "    path_suffix=None, data_dict=None,\n",
    "    compression=None, protocol=None,\n",
    "):\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {col} - {vectorizer_name} + {classifier_name} (Save_protocol={protocol}).pkl'\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "\n",
    "    # Check predicted data\n",
    "    check_consistent_length(X_train, y_train, y_train_pred, y_train_pred_prob)\n",
    "    check_consistent_length(X_test, y_test, y_test_pred, y_test_pred_prob)\n",
    "    check_consistent_length(X_val, y_val, y_val_pred, y_val_pred_prob)\n",
    "\n",
    "    # Make data dict\n",
    "    if classifier_name != 'Perceptron' or estimator.__class__.__name__ != 'CalibratedClassifierCV':\n",
    "        data_dict['Estimator'] = estimator\n",
    "        data_dict['Grid Search'] = grid_search\n",
    "        data_dict['SearchCV'] = searchcv\n",
    "    # Make df_cv_results\n",
    "    data_dict['df_cv_results'] = pd.DataFrame(\n",
    "        cv_results\n",
    "    )\n",
    "    # Make df_train_data\n",
    "    data_dict['df_train_data'] = pd.DataFrame(\n",
    "        {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'y_train_pred': y_train_pred,\n",
    "            'y_train_pred_prob': y_train_pred_prob,\n",
    "        },\n",
    "    )\n",
    "    # Make df_test_data\n",
    "    data_dict['df_test_data'] = pd.DataFrame(\n",
    "        {\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'y_test_pred': y_test_pred,\n",
    "            'y_test_pred_prob': y_test_pred_prob,\n",
    "        },\n",
    "    )\n",
    "    # Make df_val_data\n",
    "    data_dict['df_val_data'] = pd.DataFrame(\n",
    "        {\n",
    "            'X_val': X_val,\n",
    "            'y_val': y_val,\n",
    "            'y_val_pred': y_val_pred,\n",
    "            'y_val_pred_prob': y_val_pred_prob,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Make df_feature_importances\n",
    "    if df_feature_importances is not None:\n",
    "        data_dict['df_feature_importances'] = df_feature_importances\n",
    "\n",
    "    # Save files\n",
    "    print('='*20)\n",
    "    saved_files_list = []\n",
    "    for file_name, file_ in data_dict.items():\n",
    "        save_path = done_xy_save_path if file_name != 'Estimator' else results_save_path\n",
    "        print(f'Saving {file_name}')\n",
    "        if 'df_' not in file_name:\n",
    "            with open(\n",
    "                f'{save_path}{method} {file_name}{path_suffix}', 'wb'\n",
    "            ) as f:\n",
    "                joblib.dump(file_, f, compress=compression, protocol=protocol)\n",
    "        else:\n",
    "            file_.to_pickle(\n",
    "                f'{save_path}{method} {file_name}{path_suffix}', protocol=protocol\n",
    "            )\n",
    "        saved_files_list.append(file_name)\n",
    "    assert set(data_dict.keys()) == set(saved_files_list), f'Not all files were saved! Missing: {set(data_dict.keys()) ^ set(saved_files_list)}'\n",
    "    print(f'Done saving Xy, CV data, and estimator!\\n{list(data_dict.keys())}')\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f51e73ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that all classifiers were used\n",
    "def assert_all_classifiers_used(\n",
    "    classifiers_pipe, estimators_list=None, used_classifiers=None, results_save_path=results_save_path, method=method\n",
    "):\n",
    "    if estimators_list is None:\n",
    "        estimators_list = []\n",
    "    if used_classifiers is None:\n",
    "        used_classifiers = []\n",
    "\n",
    "    for estimator_path in glob.glob(f'{results_save_path}{method} Estimator - *.*'):\n",
    "        if f'{method} Estimator - ' in estimator_path:\n",
    "            classifier_name = estimator_path.split(f'{results_save_path}{method} ')[1].split(' + ')[1].split(' (Save_protocol=')[0]\n",
    "        used_classifiers.append(classifier_name)\n",
    "\n",
    "    assert {\n",
    "        x\n",
    "        for x in classifiers_pipe\n",
    "        if x not in classifier_ignore_list\n",
    "    } == set(\n",
    "        used_classifiers\n",
    "    ), f'Not all classifiers were used!\\nAvaliable Classifiers:\\n{set(list(classifiers_pipe.keys()))}\\nUsed Classifiers:\\n{set(used_classifiers)}\\nLeftout Classifiers:\\n{set(list(classifiers_pipe.keys())) ^ set(used_classifiers)}'\n",
    "    print('All classifiers were used!')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "036fcf10",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52fe8e7a",
   "metadata": {},
   "source": [
    "### READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7fbe29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe loaded with shape: (5947, 75)\n"
     ]
    }
   ],
   "source": [
    "with open(f'{data_dir}df_manual_len.txt', 'r') as f:\n",
    "    df_manual_len = int(f.read())\n",
    "\n",
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_training.pkl')\n",
    "assert len(df_manual) == df_manual_len, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH {df_manual_len} BUT IS OF LENGTH {len(df_manual)}'\n",
    "print(f'Dataframe loaded with shape: {df_manual.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0212dc9",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Starting!\n",
      "########################################\n",
      "Searching for existing estimators in directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 304/304 [00:00<00:00, 1020054.73it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "============================== TRAINING DATASET OF LENGTH 5947 ON WARMTH ==============================\n",
      "--------------------\n",
      "Vectorizers to be used (3):\n",
      "['CountVectorizer', 'TfidfVectorizer', 'FeatureUnion']\n",
      "Total number of vectorizer parameters = 6\n",
      "Selectors to be used (1):\n",
      "['SelectKBest']\n",
      "Total number of selector parameters = 2\n",
      "Resamplers to be used (1):\n",
      "['SMOTETomek']\n",
      "Total number of resamplers parameters = 2\n",
      "classifiers to be used (10):\n",
      "['DummyClassifier', 'MultinomialNB', 'KNeighborsClassifier', 'LogisticRegression', 'DecisionTreeClassifier', 'RandomForestClassifier', 'XGBClassifier', 'SGDClassifier', 'VotingClassifier', 'StackingClassifier']\n",
      "Total number of classifiers parameters = 35\n",
      "Loading previous Xy.\n",
      "++++++++++++++++++++++++++++++\n",
      "========== Loading Xy from previous for Warmth ==========\n",
      "++++++++++++++++++++++++++++++\n",
      "Loading df_train_data\n",
      "Loading df_test_data\n",
      "Loading df_val_data\n",
      "Done loading Xy from previous for Warmth!\n",
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (4446,)\n",
      "----------\n",
      "Training set example:\n",
      "This internship is for you if:\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (593,)\n",
      "----------\n",
      "Testing set example:\n",
      "General switchboard number +44 (0)207 801 3380.\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (889,)\n",
      "----------\n",
      "Validation set example:\n",
      "Effective ability to prioritize tasks and deliver on deadlines, with high performance standards and a commitment to excellence.\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.37 (0 = 0.68, 1 = 1.86)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.39 (0 = 0.69, 1 = 1.79)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.40 (0 = 0.70, 1 = 1.76)\n",
      "====================\n",
      "Done loading Xy from previous for Warmth!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9fbcb850b742e4a4a8d125fcd0587a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Already trained Warmth - CountVectorizer + DummyClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - CountVectorizer + MultinomialNB\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - CountVectorizer + KNeighborsClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - CountVectorizer + LogisticRegression\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - CountVectorizer + DecisionTreeClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - CountVectorizer + RandomForestClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - CountVectorizer + XGBClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - CountVectorizer + SGDClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - CountVectorizer + VotingClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - CountVectorizer + StackingClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - TfidfVectorizer + DummyClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - TfidfVectorizer + MultinomialNB\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - TfidfVectorizer + KNeighborsClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - TfidfVectorizer + LogisticRegression\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - TfidfVectorizer + DecisionTreeClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - TfidfVectorizer + RandomForestClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - TfidfVectorizer + XGBClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - TfidfVectorizer + SGDClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - TfidfVectorizer + VotingClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - TfidfVectorizer + StackingClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - FeatureUnion + DummyClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - FeatureUnion + MultinomialNB\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - FeatureUnion + KNeighborsClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - FeatureUnion + LogisticRegression\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - FeatureUnion + DecisionTreeClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - FeatureUnion + RandomForestClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - FeatureUnion + XGBClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - FeatureUnion + SGDClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - FeatureUnion + VotingClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Warmth - FeatureUnion + StackingClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "============================== TRAINING DATASET OF LENGTH 5947 ON COMPETENCE ==============================\n",
      "--------------------\n",
      "Vectorizers to be used (3):\n",
      "['CountVectorizer', 'TfidfVectorizer', 'FeatureUnion']\n",
      "Total number of vectorizer parameters = 6\n",
      "Selectors to be used (1):\n",
      "['SelectKBest']\n",
      "Total number of selector parameters = 2\n",
      "Resamplers to be used (1):\n",
      "['SMOTETomek']\n",
      "Total number of resamplers parameters = 2\n",
      "classifiers to be used (10):\n",
      "['DummyClassifier', 'MultinomialNB', 'KNeighborsClassifier', 'LogisticRegression', 'DecisionTreeClassifier', 'RandomForestClassifier', 'XGBClassifier', 'SGDClassifier', 'VotingClassifier', 'StackingClassifier']\n",
      "Total number of classifiers parameters = 35\n",
      "Loading previous Xy.\n",
      "++++++++++++++++++++++++++++++\n",
      "========== Loading Xy from previous for Competence ==========\n",
      "++++++++++++++++++++++++++++++\n",
      "Loading df_test_data\n",
      "Loading df_train_data\n",
      "Loading df_val_data\n",
      "Done loading Xy from previous for Competence!\n",
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (4446,)\n",
      "----------\n",
      "Training set example:\n",
      "This internship is for you if:\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (593,)\n",
      "----------\n",
      "Testing set example:\n",
      "General switchboard number +44 (0)207 801 3380.\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (889,)\n",
      "----------\n",
      "Validation set example:\n",
      "Effective ability to prioritize tasks and deliver on deadlines, with high performance standards and a commitment to excellence.\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.88 (0 = 0.94, 1 = 1.07)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.86 (0 = 0.93, 1 = 1.08)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.85 (0 = 0.92, 1 = 1.09)\n",
      "====================\n",
      "Done loading Xy from previous for Competence!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d7e629cbd546e99b909088ccc1ff2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 29.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Already trained Competence - CountVectorizer + DummyClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - CountVectorizer + MultinomialNB\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - CountVectorizer + KNeighborsClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - CountVectorizer + LogisticRegression\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - CountVectorizer + DecisionTreeClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - CountVectorizer + RandomForestClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - CountVectorizer + XGBClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - CountVectorizer + SGDClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - CountVectorizer + VotingClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - CountVectorizer + StackingClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - TfidfVectorizer + DummyClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - TfidfVectorizer + MultinomialNB\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - TfidfVectorizer + KNeighborsClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - TfidfVectorizer + LogisticRegression\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - TfidfVectorizer + DecisionTreeClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - TfidfVectorizer + RandomForestClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - TfidfVectorizer + XGBClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - TfidfVectorizer + SGDClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - TfidfVectorizer + VotingClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - TfidfVectorizer + StackingClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - FeatureUnion + DummyClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - FeatureUnion + MultinomialNB\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - FeatureUnion + KNeighborsClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - FeatureUnion + LogisticRegression\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - FeatureUnion + DecisionTreeClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - FeatureUnion + RandomForestClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - FeatureUnion + XGBClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - FeatureUnion + SGDClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - FeatureUnion + VotingClassifier\n",
      "--------------------\n",
      "--------------------\n",
      "Already trained Competence - FeatureUnion + StackingClassifier\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/homebrew/Caskroom/mambaforge/base/envs/Automating_Equity1/lib/python3.10/site-packages/IPyt</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">hon/core/magics/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">execution.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1325</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">time</span>                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1322 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1323 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>st = clock2()                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1324 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1325 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>exec(code, glob, local_ns)                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1326 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>out=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1327 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># multi-line %%time case</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1328 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> expr_val <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">230</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">assert_all_classifiers_used</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">15</span>                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>classifier_name = estimator_path.split(<span style=\"color: #808000; text-decoration-color: #808000\">f'{</span>results_save_path<span style=\"color: #808000; text-decoration-color: #808000\">}{</span>method<span style=\"color: #808000; text-decoration-color: #808000\">} '</span>)[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>].s    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>used_classifiers.append(classifier_name)                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>15 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">assert</span> {                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">16 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> x <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> classifiers_pipe                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">18 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> x <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> classifier_ignore_list                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AssertionError: </span>Not all classifiers were used!\n",
       "Avaliable Classifiers:\n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'KNeighborsClassifier'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'DecisionTreeClassifier'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'DummyClassifier'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'StackingClassifier'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'XGBClassifier'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'VotingClassifier'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'SGDClassifier'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'LogisticRegression'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'RandomForestClassifier'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'MultinomialNB'</span><span style=\"font-weight: bold\">}</span>\n",
       "Used Classifiers:\n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'KNeighborsClassifier'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'BaggingClassifier'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'DecisionTreeClassifier'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'MLPClassifier'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'DummyClassifier'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'StackingClassifier'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'XGBClassifier'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'AdaBoostClassifier'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'VotingClassifier'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'PassiveAggressiveClassifier'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'SGDClassifier'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'LinearSVC'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'LogisticRegression'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'GradientBoostingClassifier'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Perceptron'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'RandomForestClassifier'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'MultinomialNB'</span><span style=\"font-weight: bold\">}</span>\n",
       "Leftout Classifiers:\n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'AdaBoostClassifier'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Perceptron'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'BaggingClassifier'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'MLPClassifier'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'PassiveAggressiveClassifier'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'LinearSVC'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'GradientBoostingClassifier'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/homebrew/Caskroom/mambaforge/base/envs/Automating_Equity1/lib/python3.10/site-packages/IPyt\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mhon/core/magics/\u001b[0m\u001b[1;33mexecution.py\u001b[0m:\u001b[94m1325\u001b[0m in \u001b[92mtime\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1322 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1323 \u001b[0m\u001b[2m│   │   │   \u001b[0mst = clock2()                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1324 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1325 \u001b[2m│   │   │   │   \u001b[0mexec(code, glob, local_ns)                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1326 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mout=\u001b[94mNone\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1327 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# multi-line %%time case\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1328 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m expr_val \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m230\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92massert_all_classifiers_used\u001b[0m:\u001b[94m15\u001b[0m                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m12 \u001b[0m\u001b[2m│   │   │   \u001b[0mclassifier_name = estimator_path.split(\u001b[33mf\u001b[0m\u001b[33m'\u001b[0m\u001b[33m{\u001b[0mresults_save_path\u001b[33m}\u001b[0m\u001b[33m{\u001b[0mmethod\u001b[33m}\u001b[0m\u001b[33m \u001b[0m\u001b[33m'\u001b[0m)[\u001b[94m1\u001b[0m].s    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m13 \u001b[0m\u001b[2m│   │   \u001b[0mused_classifiers.append(classifier_name)                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m14 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m15 \u001b[2m│   \u001b[0m\u001b[94massert\u001b[0m {                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2m│   │   \u001b[0mx                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m x \u001b[95min\u001b[0m classifiers_pipe                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m x \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m classifier_ignore_list                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAssertionError: \u001b[0mNot all classifiers were used!\n",
       "Avaliable Classifiers:\n",
       "\u001b[1m{\u001b[0m\u001b[32m'KNeighborsClassifier'\u001b[0m, \u001b[32m'DecisionTreeClassifier'\u001b[0m, \u001b[32m'DummyClassifier'\u001b[0m, \u001b[32m'StackingClassifier'\u001b[0m, \u001b[32m'XGBClassifier'\u001b[0m, \n",
       "\u001b[32m'VotingClassifier'\u001b[0m, \u001b[32m'SGDClassifier'\u001b[0m, \u001b[32m'LogisticRegression'\u001b[0m, \u001b[32m'RandomForestClassifier'\u001b[0m, \u001b[32m'MultinomialNB'\u001b[0m\u001b[1m}\u001b[0m\n",
       "Used Classifiers:\n",
       "\u001b[1m{\u001b[0m\u001b[32m'KNeighborsClassifier'\u001b[0m, \u001b[32m'BaggingClassifier'\u001b[0m, \u001b[32m'DecisionTreeClassifier'\u001b[0m, \u001b[32m'MLPClassifier'\u001b[0m, \u001b[32m'DummyClassifier'\u001b[0m, \n",
       "\u001b[32m'StackingClassifier'\u001b[0m, \u001b[32m'XGBClassifier'\u001b[0m, \u001b[32m'AdaBoostClassifier'\u001b[0m, \u001b[32m'VotingClassifier'\u001b[0m, \u001b[32m'PassiveAggressiveClassifier'\u001b[0m, \n",
       "\u001b[32m'SGDClassifier'\u001b[0m, \u001b[32m'LinearSVC'\u001b[0m, \u001b[32m'LogisticRegression'\u001b[0m, \u001b[32m'GradientBoostingClassifier'\u001b[0m, \u001b[32m'Perceptron'\u001b[0m, \n",
       "\u001b[32m'RandomForestClassifier'\u001b[0m, \u001b[32m'MultinomialNB'\u001b[0m\u001b[1m}\u001b[0m\n",
       "Leftout Classifiers:\n",
       "\u001b[1m{\u001b[0m\u001b[32m'AdaBoostClassifier'\u001b[0m, \u001b[32m'Perceptron'\u001b[0m, \u001b[32m'BaggingClassifier'\u001b[0m, \u001b[32m'MLPClassifier'\u001b[0m, \u001b[32m'PassiveAggressiveClassifier'\u001b[0m, \n",
       "\u001b[32m'LinearSVC'\u001b[0m, \u001b[32m'GradientBoostingClassifier'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "print('#'*40)\n",
    "print('Starting!')\n",
    "print('#'*40)\n",
    "\n",
    "# Define columns to be used\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "# Get existing estimators\n",
    "estimator_names_list = get_existing_files()\n",
    "done_estimators = glob.glob(f'{done_xy_save_path}*')\n",
    "done_files = [\n",
    "    'df_train_data', 'df_test_data', 'df_val_data', 'df_cv_results', 'Grid Search', 'SearchCV'\n",
    "]\n",
    "\n",
    "for col in tqdm.tqdm(analysis_columns):\n",
    "\n",
    "    print('-'*20)\n",
    "    print(f'{\"=\"*30} TRAINING DATASET OF LENGTH {len(df_manual)} ON {col.upper()} {\"=\"*30}')\n",
    "    print('-'*20)\n",
    "    print(\n",
    "        f'Vectorizers to be used ({len(list(vectorizers_pipe.values()))}):\\n{list(vectorizers_pipe.keys())}'\n",
    "    )\n",
    "    print(\n",
    "        f'Total number of vectorizer parameters = {sum([len(list(vectorizers_pipe.values())[i]) for i in range(len(vectorizers_pipe))])}'\n",
    "    )\n",
    "    print(\n",
    "        f'Selectors to be used ({len(list(selectors_pipe.values()))}):\\n{list(selectors_pipe.keys())}'\n",
    "    )\n",
    "    print(\n",
    "        f'Total number of selector parameters = {sum([len(list(selectors_pipe.values())[i][1]) for i in range(len(selectors_pipe))])}'\n",
    "    )\n",
    "    print(\n",
    "        f'Resamplers to be used ({len(list(resamplers_pipe.keys()))}):\\n{list(resamplers_pipe.keys())}'\n",
    "    )\n",
    "    print(\n",
    "        f'Total number of resamplers parameters = {sum([len(list(resamplers_pipe.values())[i][1]) for i in range(len(resamplers_pipe))])}'\n",
    "    )\n",
    "    print(\n",
    "        f'classifiers to be used ({len(list(classifiers_pipe.keys()))}):\\n{list(classifiers_pipe.keys())}'\n",
    "    )\n",
    "    print(\n",
    "        f'Total number of classifiers parameters = {sum([len(list(classifiers_pipe.values())[i][1]) for i in range(len(classifiers_pipe))])}'\n",
    "    )\n",
    "\n",
    "    assert len(df_manual[df_manual[str(col)].map(df_manual[str(col)].value_counts() > 1)]) != 0, f'Dataframe has no {col} values!'\n",
    "\n",
    "    if (len(glob.glob(f'{models_save_path}df_*_data - {col} - (Save_protocol=*).pkl')) == 3) or (len(glob.glob(f'{models_save_path}df_*_data - {col} - (Save_protocol=*).pkl')) == 6):\n",
    "        # Load previous Xy\n",
    "        print('Loading previous Xy.')\n",
    "        (\n",
    "            X_train, y_train,\n",
    "            X_test, y_test,\n",
    "            X_val, y_val,\n",
    "            train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "            test_class_weights_dict, test_class_weights_ratio, test_class_weights_dict,\n",
    "            val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "        ) = load_Xy(\n",
    "            col\n",
    "        )\n",
    "    else:\n",
    "        print('Splitting data.')\n",
    "        # Split data\n",
    "        (\n",
    "            train, X_train, y_train,\n",
    "            test, X_test, y_test,\n",
    "            val, X_val, y_val,\n",
    "            train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "            test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "            val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "        ) = split_data(\n",
    "            df_manual, col,\n",
    "        )\n",
    "        # Save Xy data\n",
    "        save_Xy(\n",
    "            X_train, y_train,\n",
    "            X_test, y_test,\n",
    "            X_val, y_val,\n",
    "            col,\n",
    "        )\n",
    "\n",
    "    for (\n",
    "        vectorizer_name, vectorizer_and_params\n",
    "    ), (\n",
    "        selector_name, selector_and_params\n",
    "    ), (\n",
    "        resampler_name, resampler_and_params\n",
    "    ), (\n",
    "        classifier_name, classifier_and_params\n",
    "    ) in tqdm_product(\n",
    "        vectorizers_pipe.items(), selectors_pipe.items(), resamplers_pipe.items(), classifiers_pipe.items()\n",
    "    ):\n",
    "        if (f'{col} - {vectorizer_name} + {classifier_name}' in estimator_names_list) and (\n",
    "            all(f'{done_xy_save_path}{method} {str(done_file)} - {col} - {vectorizer_name} + {classifier_name} (Save_protocol={pickle.HIGHEST_PROTOCOL}).pkl' in done_estimators for done_file in done_files)\n",
    "        ):\n",
    "            print('-'*20)\n",
    "            print(\n",
    "                f'Already trained {col} - {vectorizer_name} + {classifier_name}'\n",
    "            )\n",
    "            print('-'*20)\n",
    "            continue\n",
    "\n",
    "        print('-'*20)\n",
    "        print(f'Using estimator: {classifier_name}')\n",
    "        print('-'*20)\n",
    "        ## Normalize Xy for unusual classifiers before fitting\n",
    "        if classifier_name == 'GaussianNB':\n",
    "            X_train = X_train.todense()\n",
    "            X_test = X_test.todense()\n",
    "            X_val = X_val.todense()\n",
    "\n",
    "        # Identify names and params\n",
    "        vectorizer = vectorizer_and_params[0]\n",
    "        vectorizer_params = vectorizer_and_params[-1]\n",
    "\n",
    "        selector = selector_and_params[0]\n",
    "        selector_params = selector_and_params[-1]\n",
    "\n",
    "        resampler = resampler_and_params[0]\n",
    "        resampler_params = resampler_and_params[-1]\n",
    "\n",
    "        classifier = classifier_and_params[0]\n",
    "        classifier_params = classifier_and_params[-1]\n",
    "\n",
    "        # Pipeline\n",
    "        ## Steps\n",
    "        if col == 'Warmth':\n",
    "            steps = [\n",
    "                (vectorizer_name, vectorizer),\n",
    "                (selector_name, selector),\n",
    "                (resampler_name, resampler),\n",
    "                (classifier_name, classifier)\n",
    "            ]\n",
    "        else:\n",
    "            steps = [\n",
    "                (vectorizer_name, vectorizer),\n",
    "                (selector_name, selector),\n",
    "                (classifier_name, classifier)\n",
    "            ]\n",
    "\n",
    "        ## Params\n",
    "        param_grid = {\n",
    "            **vectorizer_params,\n",
    "            **selector_params,\n",
    "            **classifier_params,\n",
    "        }\n",
    "\n",
    "        ## Pipeline\n",
    "        pipe = imblearn.pipeline.Pipeline(steps=steps)\n",
    "\n",
    "        # Search\n",
    "        print('-'*20)\n",
    "        print(f'{\"=\"*30} Using GridSearchCV {\"=\"*30}')\n",
    "        print('-'*20)\n",
    "        print(f'GridSearchCV with:\\nPipe:\\n{pipe}\\nParams:\\n{param_grid}')\n",
    "        print('+'*30)\n",
    "\n",
    "        # Use if StratifiedKFold causes issues\n",
    "        # cv = PredefinedSplit(test_fold=[-1]*len(X_train) + [0]*len(X_val))\n",
    "        # Pass arguments to gridsearch\n",
    "        grid_search = HalvingGridSearchCV(\n",
    "            estimator=pipe,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            n_jobs=n_jobs,\n",
    "            return_train_score=True,\n",
    "            verbose=1,\n",
    "            error_score='raise',\n",
    "            refit=refit,\n",
    "            random_state=random_state,\n",
    "            scoring=scorers['recall_score'],\n",
    "        )\n",
    "\n",
    "        with joblib.parallel_backend(backend='loky', n_jobs=n_jobs):\n",
    "            # Fit SearchCV\n",
    "            print('Fitting GridSearchCV')\n",
    "            searchcv = grid_search.fit(np.concatenate((X_train, X_val), axis=0), np.concatenate((y_train, y_val), axis=0))\n",
    "\n",
    "        # Reidentify and name best estimator and params\n",
    "        estimator = searchcv.best_estimator_\n",
    "        cv_results = searchcv.cv_results_\n",
    "        vectorizer = estimator[0]\n",
    "        vectorizer_params = vectorizer.get_params()\n",
    "        vectorizer_name = vectorizer.__class__.__name__\n",
    "        selector = estimator[1]\n",
    "        selector_params = selector.get_params()\n",
    "        selector_name = selector.__class__.__name__\n",
    "        classifier = estimator[-1]\n",
    "        classifier_params = classifier.get_params()\n",
    "        classifier_name = classifier.__class__.__name__\n",
    "        if col == 'Warmth':\n",
    "            resampler = estimator[-2]\n",
    "            resampler_params = resampler.get_params()\n",
    "            resampler_name = resampler.__class__.__name__\n",
    "\n",
    "        # Normalize Xy for unusual classifiers after fitting\n",
    "        (\n",
    "            estimator, X_train, y_train, X_test, y_test, searchcv, df_feature_importances\n",
    "        ) = normalize_after_fitting(\n",
    "            estimator, X_train, y_train, X_test, y_test, grid_search, searchcv, vectorizer_name, classifier_name\n",
    "        )\n",
    "\n",
    "        # Set prediction probability attribute\n",
    "        if hasattr(searchcv, 'predict_proba'):\n",
    "            searchcv_predict_attr = searchcv.predict_proba\n",
    "        elif hasattr(searchcv, '_predict_proba_lr'):\n",
    "            searchcv_predict_attr = searchcv._predict_proba_lr\n",
    "\n",
    "        # Get predictions and probabilities\n",
    "        y_train_pred = estimator.predict(X_train)\n",
    "        y_train_pred_prob = searchcv_predict_attr(X_train)[:, 1]\n",
    "\n",
    "        y_test_pred = searchcv.predict(X_test)\n",
    "        y_test_pred_prob = searchcv_predict_attr(X_test)[:, 1]\n",
    "\n",
    "        y_val_pred = searchcv.predict(X_val)\n",
    "        y_val_pred_prob = searchcv_predict_attr(X_val)[:, 1]\n",
    "\n",
    "        # Save Xy and CV data\n",
    "        save_Xy_search_cv_estimator(\n",
    "            grid_search, searchcv, cv_results,\n",
    "            X_train, y_train, y_train_pred, y_train_pred_prob,\n",
    "            X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "            X_val, y_val, y_val_pred, y_val_pred_prob,\n",
    "            df_feature_importances, estimator,\n",
    "            col, vectorizer_name, classifier_name,\n",
    "        )\n",
    "\n",
    "# Assert that all classifiers were used\n",
    "assert_all_classifiers_used(classifiers_pipe=classifiers_pipe)\n",
    "print('#'*40)\n",
    "print('DONE!')\n",
    "print('#'*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17e0c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
