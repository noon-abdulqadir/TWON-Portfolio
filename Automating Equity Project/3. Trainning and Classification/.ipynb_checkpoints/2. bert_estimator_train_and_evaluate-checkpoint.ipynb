{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d4c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50c3b1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# models dir\n",
    "models_save_path = f'{data_dir}classification models/'\n",
    "\n",
    "# output tables dir\n",
    "table_save_path = f'{data_dir}output tables/'\n",
    "\n",
    "# plots dir\n",
    "plot_save_path = f'{data_dir}plots/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "757b442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import timeit\n",
    "import tqdm\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import joblib\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cd6aa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ML imports\n",
    "import sklearn\n",
    "from sklearn import feature_selection, metrics, set_config, svm, utils\n",
    "from sklearn.utils import (check_consistent_length, check_random_state, check_X_y, parallel_backend)\n",
    "from sklearn.utils.validation import (check_is_fitted, column_or_1d,\n",
    "                                      has_fit_parameter)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import FeatureUnion, make_pipeline, Pipeline\n",
    "from sklearn.feature_selection import (SelectFdr, SelectFpr,\n",
    "                                       SelectFromModel, SelectFwe,\n",
    "                                       SelectKBest, SelectPercentile, chi2,\n",
    "                                       f_classif, f_regression,\n",
    "                                       mutual_info_classif,\n",
    "                                       mutual_info_regression)\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import (LogisticRegression,\n",
    "                                  PassiveAggressiveClassifier, Perceptron,\n",
    "                                  SGDClassifier)\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (AdaBoostClassifier, BaggingClassifier,\n",
    "                              BaggingRegressor, ExtraTreesClassifier,\n",
    "                              GradientBoostingClassifier,\n",
    "                              RandomForestClassifier, StackingClassifier,\n",
    "                              StackingRegressor, VotingClassifier,\n",
    "                              VotingRegressor)\n",
    "from sklearn.model_selection import (GridSearchCV, KFold, LeaveOneOut,\n",
    "                                     RandomizedSearchCV,\n",
    "                                     RepeatedStratifiedKFold, ShuffleSplit,\n",
    "                                     StratifiedKFold,\n",
    "                                     StratifiedShuffleSplit,\n",
    "                                     cross_val_score, cross_val_predict, cross_validate,\n",
    "                                     learning_curve, train_test_split)\n",
    "from sklearn.metrics import (ConfusionMatrixDisplay,accuracy_score, balanced_accuracy_score,\n",
    "                             brier_score_loss, classification_report, cohen_kappa_score,\n",
    "                             confusion_matrix, f1_score, log_loss,\n",
    "                             make_scorer, matthews_corrcoef, fowlkes_mallows_score,\n",
    "                             precision_recall_curve, precision_score,\n",
    "                             recall_score, roc_auc_score)\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from scipy.special import softmax\n",
    "import imblearn\n",
    "from imblearn.pipeline import make_pipeline, Pipeline\n",
    "from imblearn.metrics import geometric_mean_score, make_index_balanced_accuracy\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC, RandomOverSampler\n",
    "from imblearn.under_sampling import (EditedNearestNeighbours, NearMiss,\n",
    "                                     RandomUnderSampler, TomekLinks)\n",
    "from xgboost import XGBClassifier\n",
    "import plot_metric\n",
    "from plot_metric.functions import BinaryClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline,\n",
    "    BertTokenizer, BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments,\n",
    "    DistilBertTokenizerFast, DistilBertForSequenceClassification, BertForPreTraining, BertConfig, BertModel\n",
    ")\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "max_length = 512\n",
    "device = torch.device('mps') if torch.has_mps else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertForSequenceClassification.from_pretrained(bert_model_name).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55afc383",
   "metadata": {},
   "source": [
    "### READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7735d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "plt.style.use('tableau-colorblind10')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "max_length = 512\n",
    "device_name = 'mps'\n",
    "method = 'BERT'\n",
    "n_jobs = -1\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=10, n_repeats=3, random_state=random_state)\n",
    "t = time.time()\n",
    "cores = multiprocessing.cpu_count()\n",
    "scoring = 'recall'\n",
    "scores = ['recall', 'accuracy', 'f1', 'roc_auc', 'explained_variance', 'matthews_corrcoef']\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score),\n",
    "}\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb448ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, encoded):\n",
    "        self.encodings = encodings\n",
    "        self.encoded = encoded\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]).detach().clone() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.encoded[idx]).detach().clone()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "395dffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, col, analysis_columns, text_col=None):\n",
    "\n",
    "    if text_col is None:\n",
    "        text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "    train_ratio = 0.75\n",
    "    test_ratio = 0.10\n",
    "    validation_ratio = 0.15\n",
    "    test_split = test_size = 1 - train_ratio\n",
    "    validation_split = test_ratio / (test_ratio + validation_ratio)\n",
    "\n",
    "    # Split\n",
    "    print('='*20)\n",
    "    print('Splitting data into training, testing, and validation sets:')\n",
    "    print(f'Ratios: train_size = {train_ratio}, test size = {test_ratio}, validation size = {validation_ratio}')\n",
    "\n",
    "    df.dropna(subset=analysis_columns, how='any', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        df, train_size = 1-test_split, test_size = test_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    val, test = train_test_split(\n",
    "        test, test_size=validation_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train = np.array(list(train[text_col].astype('str').values))\n",
    "    y_train = column_or_1d(train[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    X_test = np.array(list(test[text_col].astype('str').values))\n",
    "    y_test = column_or_1d(test[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    X_val = np.array(list(val[text_col].astype('str').values))\n",
    "    y_val = column_or_1d(val[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    class_weights = compute_class_weight(class_weight = 'balanced', classes = [0,1], y = y_train)\n",
    "    class_weights_ratio = class_weights[0]/class_weights[1]\n",
    "    class_weights_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "    print('Done splitting data into training, testing, and validation sets.')\n",
    "\n",
    "    return (\n",
    "        train, X_train, y_train,\n",
    "        test, X_test, y_test,\n",
    "        val, X_val, y_val,\n",
    "        class_weights,\n",
    "        class_weights_ratio,\n",
    "        class_weights_dict\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f3840ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(df, col, analysis_columns, text_col=None):\n",
    "\n",
    "    # Split\n",
    "    (\n",
    "        train, X_train, y_train,\n",
    "        test, X_test, y_test,\n",
    "        val, X_val, y_val,\n",
    "        class_weights,\n",
    "        class_weights_ratio,\n",
    "        class_weights_dict\n",
    "    ) = split_data(\n",
    "        df=df_manual, col=col, analysis_columns=analysis_columns, text_col=text_col\n",
    "    )\n",
    "\n",
    "    if text_col is None:\n",
    "        text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "    print('='*20)\n",
    "    print(\n",
    "        'Encoding training, testing, and validation sets with BertTokenizerFast.from_pretrained'\n",
    "    )\n",
    "\n",
    "    bert_label2id = {label: id_ for id_, label in enumerate(set(label for label in y_train))}\n",
    "    bert_id2label = {id_: label for label, id_ in bert_label2id.items()}\n",
    "\n",
    "    X_train_bert_encodings = bert_tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "    y_train_bert_encoded = [bert_label2id[y] for y in y_train]\n",
    "    bert_train_dataset = MyDataset(X_train_bert_encodings, y_train_bert_encoded)\n",
    "\n",
    "    X_test_bert_encodings = bert_tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "    y_test_bert_encoded = [bert_label2id[y] for y in y_test]\n",
    "    bert_test_dataset = MyDataset(X_test_bert_encodings, y_test_bert_encoded)\n",
    "\n",
    "    X_val_bert_encodings = bert_tokenizer(X_val.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "    y_val_bert_encoded = [bert_label2id[y] for y in y_val]\n",
    "    bert_val_dataset = MyDataset(X_val_bert_encodings, y_val_bert_encoded)\n",
    "\n",
    "    print('Done encoding training, testing, and validation sets.')\n",
    "    print('='*20)\n",
    "    print(f'Training set shape: {y_train.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Training set example:\\n{X_train[0]}')\n",
    "    print('-'*10)\n",
    "    print(f'Training set BERT encodings example:\\n{\" \".join(bert_train_dataset.encodings[0].tokens[:30])}')\n",
    "    print('-'*10)\n",
    "    print(f'Training labels after BERT encoding: {set(y_train_bert_encoded)}')\n",
    "    print('~'*10)\n",
    "    print(f'Testing set shape: {y_test.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing set example:\\n{X_test[0]}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing set BERT encodings example:\\n{\" \".join(bert_test_dataset.encodings[0].tokens[:30])}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing labels after BERT encoding: {set(y_test_bert_encoded)}')\n",
    "    print('~'*10)\n",
    "    print(f'Validation set shape: {y_val.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation set example:\\n{X_val[0]}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation set BERT encodings example:\\n{\" \".join(bert_val_dataset.encodings[0].tokens[:30])}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation labels after BERT encoding: {set(y_val_bert_encoded)}')\n",
    "    print('~'*10)\n",
    "    print(f'Class weights:\\nRatio = {class_weights_ratio:.2f} (0 = {class_weights[0]:.2f}, 1 = {class_weights[1]:.2f})')\n",
    "    print('='*20)\n",
    "\n",
    "    return (\n",
    "        train, X_train, X_train_bert_encodings, y_train, y_train_bert_encoded, bert_train_dataset,\n",
    "        test, X_test, X_test_bert_encodings, y_test, y_test_bert_encoded, bert_test_dataset,\n",
    "        val, X_val, X_val_bert_encodings, y_val, y_val_bert_encoded, bert_val_dataset,\n",
    "        bert_label2id, bert_id2label, class_weights, class_weights_ratio, class_weights_dict\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42fdf3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(cm_curve, cm_normalized_curve, roc_curve, pr_curve, calibration_curve, col, vectorizer_name, classifier_name):\n",
    "\n",
    "    # Plots\n",
    "    print('=' * 20)\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    print('Plotting:')\n",
    "\n",
    "    ## Confusion Matrix\n",
    "    print('-' * 20)\n",
    "    print('Confusion Matrix:')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(f'Confusion Matrix {str(col)} - {vectorizer_name} + {classifier_name}')\n",
    "    cm_curve.plot(ax=ax)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    ## Normalized Confusion Matrix\n",
    "    print('-' * 20)\n",
    "    print('Normalized Confusion Matrix:')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(f'Normalized Confusion Matrix {str(col)} - {vectorizer_name} + {classifier_name}')\n",
    "    cm_normalized_curve.plot(ax=ax)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    ## ROC Curve\n",
    "    print('-' * 20)\n",
    "    print('ROC Curve:')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(f'ROC Curve {str(col)} - {vectorizer_name} + {classifier_name}')\n",
    "    roc_curve.plot(ax=ax)\n",
    "    ax.plot([0, 1], [0, 1],'r--', lw=1)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    ## PR Curve\n",
    "    print('-' * 20)\n",
    "    print('Precision Recall Curve:')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(f'Precision-Recall Curve {str(col)} - {vectorizer_name} + {classifier_name}')\n",
    "    pr_curve.plot(ax=ax)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    ## Calibration Curve\n",
    "    print('-' * 20)\n",
    "    print('Calibration Curve:')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(f'Calibration Curve {str(col)} - {vectorizer_name} + {classifier_name}')\n",
    "    calibration_curve.plot(ax=ax)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    print('=' * 20)\n",
    "\n",
    "    # Save Plots\n",
    "    print('Saving plots...')\n",
    "    for image_save_format in ['eps', 'png']:\n",
    "        cm_curve.figure_.savefig(\n",
    "            f'{plot_save_path}{method} Confusion Matrix {str(col)} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "        cm_normalized_curve.figure_.savefig(\n",
    "            f'{plot_save_path}{method} Normalized Confusion Matrix {str(col)} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "        roc_curve.figure_.savefig(\n",
    "            f'{plot_save_path}{method} ROC Curve {str(col)} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "        pr_curve.figure_.savefig(\n",
    "            f'{plot_save_path}{method} Precision Recall Curve {str(col)} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "        calibration_curve.figure_.savefig(\n",
    "            f'{plot_save_path}{method} Calibration Curve {str(col)} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0cf7d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predicted_results):\n",
    "\n",
    "    # Get y_test_pred\n",
    "    y_test = predicted_results.label_ids\n",
    "    y_test_pred = predicted_results.predictions.argmax(-1)\n",
    "    y_test_pred = y_test_pred.flatten().tolist()\n",
    "    y_test_pred = [bert_label2id[l] for l in y_test_pred]\n",
    "\n",
    "    # Get y_test_pred_proba\n",
    "    try:\n",
    "        y_test_pred_proba = torch.nn.functional.softmax(predicted_results, dim=-1)\n",
    "        print('Using torch.nn.functional.softmax')\n",
    "    except Exception:\n",
    "        y_test_pred_proba = softmax(predicted_results, axis=1)\n",
    "        print('Using scipy.special.softmax')\n",
    "    finally:\n",
    "        y_test_pred_proba = y_test_pred_proba.flatten().tolist()\n",
    "\n",
    "    # Get y_test and y_test_pred\n",
    "    y_test = predicted_results.label_ids\n",
    "    y_test_pred = predicted_results.predictions.argmax(-1)\n",
    "\n",
    "    # Get metrics\n",
    "    # Using y_pred\n",
    "    mean_validation_score = cross_validate_score_noscoring.get('test_score').mean()\n",
    "    explained_variance = cross_validate_score.get('test_explained_variance').mean()\n",
    "    accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "    precision = metrics.precision_score(y_test, y_test_pred, pos_label=1, labels=[1, 0])\n",
    "    recall = metrics.recall_score(y_test, y_test_pred, pos_label=1, labels=[1, 0])\n",
    "    f1 = metrics.f1_score(y_test, y_test_pred)\n",
    "    mcc = metrics.matthews_corrcoef(y_test, y_test_pred)\n",
    "    fm = metrics.fowlkes_mallows_score(y_test, y_test_pred)\n",
    "    kappa = metrics.cohen_kappa_score(y_test, y_test_pred)\n",
    "    gmean_iba = imblearn.metrics.make_index_balanced_accuracy(alpha=0.1, squared=True)(geometric_mean_score)\n",
    "    gmean = gmean_iba(y_test, y_test_pred)\n",
    "    report = metrics.classification_report(y_test, y_test_pred)\n",
    "    cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "    cm_normalized = metrics.confusion_matrix(y_test, y_test_pred, normalize='true')\n",
    "\n",
    "    # Using y_pred_prob\n",
    "    loss = metrics.log_loss(y_test, y_test_pred_prob)\n",
    "    roc_auc = metrics.roc_auc_score(y_test, y_test_pred_prob)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, y_test_pred_prob, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    precision_pr, recall_pr, threshold_pr = metrics.precision_recall_curve(y_test, y_test_pred_prob, pos_label=1)\n",
    "\n",
    "    # Matrices from Display\n",
    "    cm_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, y_test_pred\n",
    "    )\n",
    "    cm_normalized_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, y_test_pred, normalize='true'\n",
    "    )\n",
    "    roc_curve = metrics.RocCurveDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=1, name=f'{str(col)} - {vectorizer_name} + {classifier_name}'\n",
    "    )\n",
    "    pr_curve = metrics.PrecisionRecallDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=1, name=f'{str(col)} - {vectorizer_name} + {classifier_name}'\n",
    "    )\n",
    "    calibration_curve = CalibrationDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=1, name=f'{str(col)} - {vectorizer_name} + {classifier_name}'\n",
    "    )\n",
    "\n",
    "    #Place metrics into dict\n",
    "    metrics_dict = {\n",
    "        'Mean Validation Score': mean_validation_score,\n",
    "        'Explained Variance': explained_variance,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-score': f1,\n",
    "        'Matthews Correlation Coefficient': mcc,\n",
    "        'Fowlkes–Mallows Index': fm,\n",
    "        'ROC': roc_auc,\n",
    "        'AUC': auc,\n",
    "        f'{scoring.title()} Best Threshold': threshold,\n",
    "        'Log Loss/Cross Entropy': loss,\n",
    "        'Cohen’s Kappa': kappa,\n",
    "        'Geometric Mean': gmean,\n",
    "        'Classification Report': report,\n",
    "        'Confusion Matrix': cm,\n",
    "        'Normalized Confusion Matrix': cm_normalized\n",
    "    }\n",
    "\n",
    "    # Visualisation with plot_metric\n",
    "    bc = plot_metric.functions.BinaryClassification(y_test, y_test_pred, labels=[0, 1])\n",
    "\n",
    "    # Figures\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.subplot2grid((2,6), (1,1), colspan=2)\n",
    "    bc.plot_confusion_matrix(colorbar=True)\n",
    "    plt.subplot2grid((2,6), (1,3), colspan=2)\n",
    "    bc.plot_confusion_matrix(normalize=True, colorbar=True)\n",
    "    plt.subplot2grid(shape=(2,6), loc=(0,0), colspan=2)\n",
    "    bc.plot_roc_curve()\n",
    "    plt.subplot2grid((2,6), (0,2), colspan=2)\n",
    "    bc.plot_precision_recall_curve()\n",
    "    plt.subplot2grid((2,6), (0,4), colspan=2)\n",
    "    bc.plot_class_distribution()\n",
    "    plt.show()\n",
    "    bc.print_report()\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c44994c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(metrics_dict, df_metrics, col, classifier_name, tokenizer_name):\n",
    "\n",
    "    # Print metrics\n",
    "    print('=' * 20)\n",
    "    print('~' * 20)\n",
    "    print(' Metrics:')\n",
    "    print('~' * 20)\n",
    "    print(f'Classification Report:\\n {metrics_dict[\"Classification Report\"]}')\n",
    "    print('-' * 20)\n",
    "    for metric_name, metric_value in metrics_dict.items():\n",
    "        print(f'{metric_name}: {metric_value}')\n",
    "        print('-' * 20)\n",
    "\n",
    "        # Fill Table DF\n",
    "        if isinstance(metric_value, float):\n",
    "            df_metrics.loc[\n",
    "                (classifier_name), (col, tokenizer_name, metric_name)\n",
    "            ] = float(metric_value)\n",
    "        else:\n",
    "            df_metrics.loc[\n",
    "                (classifier_name), (col, tokenizer_name, metric_name)\n",
    "            ] = str(metric_value)\n",
    "\n",
    "    print('=' * 20)\n",
    "\n",
    "    # Plot Metrics\n",
    "    plot_metrics(\n",
    "        cm_curve, cm_normalized_curve, roc_curve, pr_curve, calibration_curve, col, tokenizer_name, classifier_name\n",
    "    )\n",
    "\n",
    "    return df_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b48de3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "def saving_model_and_table(col, df_metrics, estimator, tokenizer_name, classifier_name):\n",
    "\n",
    "    # Save metrics df\n",
    "    print(f'Saving Model and Table for {tokenizer_name} + {classifier_name}.')\n",
    "    df_metrics.to_csv(f'{table_save_path}Classifiers Table.csv')\n",
    "    df_metrics.to_pickle(f'{table_save_path}Classifiers Table.pkl')\n",
    "    df_metrics.to_excel(f'{table_save_path}Classifiers Table.xlsx')\n",
    "    df_metrics.to_latex(f'{table_save_path}Classifiers Table.tex')\n",
    "    df_metrics.to_markdown(f'{table_save_path}Classifiers Table.md')\n",
    "\n",
    "    # Save estimator\n",
    "    estimator.save_model(f'{models_save_path}{method} Estimator {str(col)} - {tokenizer_name} + {classifier_name})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29efbbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "def get_fitted_estimators():\n",
    "    \n",
    "    estimators_list = []\n",
    "\n",
    "    for model_path in glob.glob(f'{models_save_path}*.pkl'):\n",
    "        with open(model_path, 'rb') as f:\n",
    "            estimators_list.append(joblib.load(f))\n",
    "\n",
    "    return estimators_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34e44624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_plots(estimators_list, X_test, y_test, col):\n",
    "\n",
    "    assert len(estimators_list) != 0\n",
    "\n",
    "    print('=' * 20)\n",
    "    ## ROC Curve\n",
    "    print('-' * 20)\n",
    "    print(f'ROC Curve: {str(col)}')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(f'ROC Curve: {str(col)}')\n",
    "    for estimator in estimators_list:\n",
    "        roc_curve = metrics.RocCurveDisplay.from_estimator(\n",
    "            estimator, X_test, y_test, pos_label=1, ax=ax, name=f'{estimator.steps[0][0]} + {estimator.steps[1][0]} + {estimator.steps[-1][0]}'\n",
    "        )\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    ## PR Curve\n",
    "    print('-' * 20)\n",
    "    print(f'Precision Recall Curve: {str(col)}')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(f'Precision Recall Curve: {str(col)}')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_xlabel('Recall')\n",
    "    for estimator in estimators_list:\n",
    "        pr_curve = metrics.PrecisionRecallDisplay.from_estimator(\n",
    "            estimator, X_test, y_test, pos_label=1, ax=ax, name=f'{estimator.steps[0][0]} + {estimator.steps[1][0]} + {estimator.steps[-1][0]}'\n",
    "        )\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    ## Calibration Curve\n",
    "    print('-' * 20)\n",
    "    print(f'Calibration Curve: {str(col)}')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(f'Calibration Curve: {str(col)}')\n",
    "    for estimator in estimators_list:\n",
    "        calibration_curve = CalibrationDisplay.from_estimator(\n",
    "            estimator, X_test, y_test, pos_label=1, ax=ax, name=f'{estimator.steps[0][0]} + {estimator.steps[1][0]} + {estimator.steps[-1][0]}'\n",
    "        )\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    print('=' * 20)\n",
    "\n",
    "    # Save Plots\n",
    "    print('Saving plots.')\n",
    "    for image_save_format in ['eps', 'png']:\n",
    "        roc_curve.figure_.savefig(\n",
    "            f'{plot_save_path}{method} All ROC Curve {str(col)}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "        pr_curve.figure_.savefig(\n",
    "            f'{plot_save_path}{method} All Precision Recall Curve {str(col)}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "        calibration_curve.figure_.savefig(\n",
    "            f'{plot_save_path}{method} All Calibration Curve {str(col)}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe8e7a",
   "metadata": {},
   "source": [
    "### READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7fbe29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_trainning.pkl').reset_index(drop=True)\n",
    "# REMOVE THIS!!!!!!\n",
    "df_manual = df_manual.groupby(analysis_columns).sample(n=200).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "689d84dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Starting!\n",
      "########################################\n",
      "--------------------\n",
      "============================== TRAINING WARMTH ==============================\n",
      "--------------------\n",
      "====================\n",
      "Splitting data into training, testing, and validation sets:\n",
      "Ratios: train_size = 0.75, test size = 0.1, validation size = 0.15\n",
      "Done splitting data into training, testing, and validation sets.\n",
      "====================\n",
      "Encoding training, testing, and validation sets with BertTokenizerFast.from_pretrained\n",
      "Done encoding training, testing, and validation sets.\n",
      "====================\n",
      "Training set shape: (600,)\n",
      "----------\n",
      "Training set example:\n",
      "Excellent analytical skills with the ability to synthesise complex information and identify key issues, concerns or trends;\n",
      "----------\n",
      "Training set BERT encodings example:\n",
      "[CLS] excellent analytical skills with the ability to synthesis ##e complex information and identify key issues , concerns or trends ; [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "----------\n",
      "Training labels after BERT encoding: {0, 1}\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (80,)\n",
      "----------\n",
      "Testing set example:\n",
      "Manage client relationship by delivering high-quality, timely and value-added services.\n",
      "----------\n",
      "Testing set BERT encodings example:\n",
      "[CLS] manage client relationship by delivering high - quality , timely and value - added services . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "----------\n",
      "Testing labels after BERT encoding: {0, 1}\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (120,)\n",
      "----------\n",
      "Validation set example:\n",
      "Understanding Robots Ui\n",
      "----------\n",
      "Validation set BERT encodings example:\n",
      "[CLS] understanding robots ui [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "----------\n",
      "Validation labels after BERT encoding: {0, 1}\n",
      "~~~~~~~~~~\n",
      "Class weights:\n",
      "Ratio = 0.99 (0 = 0.99, 1 = 1.01)\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERT Model using BertTokenizerFast + BertForSequenceClassification for Warmth\n",
      "Passing arguments to estimator.\n",
      "Starting training for Warmth\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8cbbc3cc2d4b3aa40b627d3c1d0127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/46/q15p556n1dd63z6gkwyh896c0000gn/T/ipykernel_12278/3018810507.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]).detach().clone() for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('#'*40)\n",
    "print('Starting!')\n",
    "print('#'*40)\n",
    "\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "# Load Table DF\n",
    "df_metrics = pd.read_pickle(f'{table_save_path}Classifiers Table.pkl')\n",
    "\n",
    "for col in analysis_columns:\n",
    "\n",
    "    print('-'*20)\n",
    "    print(f'{\"=\"*30} TRAINING {col.upper()} {\"=\"*30}')\n",
    "    print('-'*20)\n",
    "\n",
    "    assert len(df_manual[df_manual[str(col)].map(df_manual[str(col)].value_counts() > 1)]) != 0\n",
    "\n",
    "    # Split\n",
    "    (\n",
    "        train, X_train, X_train_bert_encodings, y_train, y_train_bert_encoded, bert_train_dataset,\n",
    "        test, X_test, X_test_bert_encodings, y_test, y_test_bert_encoded, bert_test_dataset,\n",
    "        val, X_val, X_val_bert_encodings, y_val, y_val_bert_encoded, bert_val_dataset,\n",
    "        bert_label2id, bert_id2label, class_weights, class_weights_ratio, class_weights_dict\n",
    "    ) = encode_data(\n",
    "        df=df_manual, col=col, analysis_columns=analysis_columns, text_col=text_col\n",
    "    )\n",
    "\n",
    "    # Initialize Model\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    random_state = 42\n",
    "    max_length = 512\n",
    "    device = torch.device('mps') if torch.has_mps else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    bert_model_name = 'bert-base-uncased'\n",
    "    \n",
    "    # Load pre-trained BERT model\n",
    "    bert_model = BertForSequenceClassification.from_pretrained(\n",
    "        bert_model_name, num_labels=len(bert_id2label)\n",
    "    ).to(device)\n",
    "\n",
    "    # Name tokenizer and classifier name\n",
    "    tokenizer_name = bert_tokenizer.__class__.__name__\n",
    "    classifier_name = bert_model.__class__.__name__\n",
    "\n",
    "    print(f'Initializing BERT Model using {tokenizer_name} + {classifier_name} for {col}')\n",
    "\n",
    "    # Set BERT fine-tuning parameters\n",
    "    bert_training_args = TrainingArguments(\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=20,\n",
    "        learning_rate=5e-5,\n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.01,\n",
    "        output_dir='./results',\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        evaluation_strategy='steps',\n",
    "        optim='adamw_torch'\n",
    "    )\n",
    "\n",
    "    # Pass data to trainer \n",
    "    print('Passing arguments to estimator.')\n",
    "    estimator = Trainer(\n",
    "        model=bert_model,\n",
    "        args=bert_training_args,\n",
    "        train_dataset=bert_train_dataset,\n",
    "        eval_dataset=bert_test_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train trainer\n",
    "    print(f'Starting training for {col}')\n",
    "    estimator.train()\n",
    "    print('Done training!')\n",
    "\n",
    "    # Get predictions\n",
    "    print(f'Evaluating estimator for {col}')\n",
    "    estimator.evaluate()\n",
    "    # metrics_dict\n",
    "    predicted_results = estimator.predict(bert_val_dataset)\n",
    "    print(f'Predictions shape for {col}: {predicted_results.predictions.shape}')\n",
    "    \n",
    "    # Get y_test_pred\n",
    "    y_test = predicted_results.label_ids\n",
    "    y_test_pred = predicted_results.predictions.argmax(-1)\n",
    "    y_test_pred = y_test_pred.flatten().tolist()\n",
    "    y_test_pred = [bert_label2id[l] for l in y_test_pred]\n",
    "    print(f'Length of y_test_pred: {len(y_test_pred)}')\n",
    "\n",
    "    # Get y_test_pred_proba\n",
    "    y_test_pred_proba = torch.nn.functional.softmax(predicted_results, dim=-1)\n",
    "    y_test_pred_proba = y_test_pred_proba.flatten().tolist()\n",
    "    print(f'Length of y_test_pred_proba: {len(y_test_pred_proba)}')\n",
    "    \n",
    "    # Evluate estimator\n",
    "    df_metrics = evaluation(predicted_results, df_metrics, col, classifier_name, tokenizer_name)\n",
    "\n",
    "    # Save BERT Model\n",
    "    print(f'Saving estimator and metrics table for {col}')\n",
    "    saving_model_and_table(df_metrics, estimator, col, tokenizer_name, classifier_name)\n",
    "\n",
    "    # Compare Estimators\n",
    "    print('='*20)\n",
    "    print(f'Comparing estimators for {col}')\n",
    "    comparison_plots(get_fitted_estimators().append(estimator), X_test, y_test, col)\n",
    "    print('='*20)\n",
    "\n",
    "print('#'*40)\n",
    "print('DONE!')\n",
    "print('#'*40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c583a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "study1_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
