{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d4c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # isort:skip # fmt:skip # noqa # nopep8\n",
    "import sys # isort:skip # fmt:skip # noqa # nopep8\n",
    "from pathlib import Path # isort:skip # fmt:skip # noqa # nopep8\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "sys.path.append(code_dir)\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fef3f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup_module.imports import * # isort:skip # fmt:skip # noqa # nopep8\n",
    "from supervised_estimators_pipe import * # isort:skip # fmt:skip # noqa # nopep8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55afc383",
   "metadata": {},
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b36fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "random_state = 42\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "DetectorFactory.seed = random_state\n",
    "\n",
    "# Sklearn\n",
    "method = 'Supervised'\n",
    "final_models_save_path = f'{models_save_path}{method} Results/'\n",
    "t = time.time()\n",
    "n_jobs = -1\n",
    "n_splits = 10\n",
    "n_repeats = 3\n",
    "refit=True\n",
    "class_weight = 'balanced'\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=n_splits, n_repeats=n_repeats, random_state=random_state\n",
    ")\n",
    "cores = multiprocessing.cpu_count()\n",
    "scoring = 'recall'\n",
    "scores = ['recall', 'accuracy', 'f1', 'roc_auc',\n",
    "          'explained_variance', 'matthews_corrcoef']\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score),\n",
    "}\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "metrics_dict = {\n",
    "    'Mean Cross Validation Train Score': np.nan,\n",
    "    'Mean Cross Validation Test Score': np.nan,\n",
    "    f'Mean Explained Train Variance - {scoring.title()}': np.nan,\n",
    "    f'Mean Explained Test Variance - {scoring.title()}': np.nan,\n",
    "    'Explained Variance': np.nan,\n",
    "    'Accuracy': np.nan,\n",
    "    'Balanced Accuracy': np.nan,\n",
    "    'Precision': np.nan,\n",
    "    'Recall': np.nan,\n",
    "    'F1-score': np.nan,\n",
    "    'Matthews Correlation Coefficient': np.nan,\n",
    "    'Fowlkes–Mallows Index': np.nan,\n",
    "    'ROC': np.nan,\n",
    "    'AUC': np.nan,\n",
    "    f'{scoring.title()} Best Threshold': np.nan,\n",
    "    f'{scoring.title()} Best Score': np.nan,\n",
    "    'Log Loss/Cross Entropy': np.nan,\n",
    "    'Cohen’s Kappa': np.nan,\n",
    "    'Geometric Mean': np.nan,\n",
    "    'Classification Report': np.nan,\n",
    "    'Confusion Matrix': np.nan,\n",
    "    'Normalized Confusion Matrix': np.nan\n",
    "}\n",
    "\n",
    "# Plotting variables\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "tqdm.tqdm.pandas(desc='progress-bar')\n",
    "tqdm_auto.tqdm.pandas(desc='progress-bar')\n",
    "tqdm.notebook.tqdm().pandas(desc='progress-bar')\n",
    "tqdm_auto.notebook_tqdm().pandas(desc='progress-bar')\n",
    "# pbar = progressbar.ProgressBar(maxval=10)\n",
    "mpl.use('MacOSX')\n",
    "mpl.style.use(f'{code_dir}/setup_module/apa.mplstyle-main/apa.mplstyle')\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "font = {'family': 'arial', 'weight': 'normal', 'size': 10}\n",
    "mpl.rc('font', **font)\n",
    "plt.style.use('tableau-colorblind10')\n",
    "plt.set_cmap('Blues')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df119b8",
   "metadata": {},
   "source": [
    "## Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec7307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CountVectorizer\n",
    "count_ = CountVectorizer()\n",
    "count_params = {\n",
    "#     'TfidfVectorizer__stop_words': ['english'],\n",
    "    'CountVectorizer__analyzer': ['word'],\n",
    "    'CountVectorizer__ngram_range': [(1, 3)],\n",
    "    'CountVectorizer__lowercase': [True, False],\n",
    "    'CountVectorizer__max_df': [0.90, 0.85, 0.80, 0.75, 0.70],\n",
    "    'CountVectorizer__min_df': [0.10, 0.15, 0.20, 0.25, 0.30],\n",
    "}\n",
    "count = [count_, count_params]\n",
    "\n",
    "### TfidfVectorizer\n",
    "tfidf_ = TfidfVectorizer()\n",
    "tfidf_params = {\n",
    "#     'TfidfVectorizer__stop_words': ['english'],\n",
    "    'TfidfVectorizer__analyzer': ['word'],\n",
    "    'TfidfVectorizer__ngram_range': [(1, 3)],\n",
    "    'TfidfVectorizer__lowercase': [True, False],\n",
    "    'TfidfVectorizer___use_idf': [True, False],\n",
    "    'TfidfVectorizer__max_df': [0.90, 0.85, 0.80, 0.75, 0.70],\n",
    "    'TfidfVectorizer__min_df': [0.10, 0.15, 0.20, 0.25, 0.30],\n",
    "}\n",
    "tfidf = [tfidf_, tfidf_params]\n",
    "\n",
    "## Vectorizers List\n",
    "vectorizers_list = [\n",
    "    count, \n",
    "    tfidf\n",
    "]\n",
    "\n",
    "### BOW FeatureUnion\n",
    "transformer_list = []\n",
    "bow_params = {}\n",
    "for vectorizer_and_params in vectorizers_list:\n",
    "    transformer_list.append(\n",
    "        (vectorizer_and_params[0].__class__.__name__, vectorizer_and_params[0])\n",
    "    )\n",
    "    for k, v in vectorizer_and_params[1].items():\n",
    "        bow_params[f'FeatureUnion__{k}'] = v\n",
    "\n",
    "bow_ = FeatureUnion(\n",
    "    transformer_list=[transformer_list]\n",
    ")\n",
    "bow = [bow_, bow_params]\n",
    "\n",
    "## Vectorizers List append bow\n",
    "vectorizers_list.append(bow)\n",
    "\n",
    "## Vectorizers Dict\n",
    "vectorizers_pipe = {\n",
    "    vectorizer_and_params[0].__class__.__name__: vectorizer_and_params\n",
    "    for vectorizer_and_params in vectorizers_list\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaff38f",
   "metadata": {},
   "source": [
    "## Selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SelectKBest\n",
    "selectkbest_ = SelectKBest()\n",
    "selectkbest_params = {\n",
    "    'SelectKBest__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "    'SelectKBest__k': ['all'],\n",
    "}\n",
    "selectkbest = [selectkbest_, selectkbest_params]\n",
    "\n",
    "### SelectPercentile\n",
    "selectperc_ = SelectPercentile()\n",
    "selectperc_params = {\n",
    "    'SelectPercentile__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "    'SelectPercentile__percentile': [30, 40, 50, 60, 70, 80],\n",
    "}\n",
    "selectperc = [selectperc_, selectperc_params]\n",
    "\n",
    "### SelectFpr\n",
    "selectfpr_ = SelectFpr()\n",
    "selectfpr_params = {\n",
    "    'SelectFpr__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "}\n",
    "selectfpr = [selectfpr_, selectfpr_params]\n",
    "\n",
    "### SelectFdr\n",
    "selectfdr_ = SelectFdr()\n",
    "selectfdr_params = {\n",
    "    'SelectFdr__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "}\n",
    "selectfdr = [selectfdr_, selectfdr_params]\n",
    "\n",
    "### SelectFwe\n",
    "selectfwe_ = SelectFwe()\n",
    "selectfwe_params = {\n",
    "    'SelectFwe__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "}\n",
    "selectfwe = [selectfwe_, selectfwe_params]\n",
    "\n",
    "## Selectors List\n",
    "selectors_list = [\n",
    "    selectkbest,\n",
    "    # selectperc, selectfpr, selectfdr, selectfwe\n",
    "]\n",
    "## Selectors Dict\n",
    "selectors_pipe = {\n",
    "    selector_and_params[0].__class__.__name__: selector_and_params\n",
    "    for selector_and_params in selectors_list\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4da5a77",
   "metadata": {},
   "source": [
    "## Resamplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ed119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resamplers\n",
    "### SMOTETomek Resampler\n",
    "smotetomek_ = SMOTETomek()\n",
    "smotetomek_params = {\n",
    "    'SMOTETomek__random_state': [random_state],\n",
    "    'SMOTETomek__tomek': [TomekLinks(sampling_strategy='majority')],\n",
    "}\n",
    "smotetomek = [smotetomek_, smotetomek_params]\n",
    "\n",
    "## Resampler List\n",
    "resamplers_list = [\n",
    "    smotetomek,\n",
    "]\n",
    "\n",
    "## Resampler Dict\n",
    "resamplers_pipe = {\n",
    "    resampler_and_params[0].__class__.__name__: resampler_and_params\n",
    "    for resampler_and_params in resamplers_list\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ab6d49",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2222fd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers\n",
    "### Dummy Classifier\n",
    "dummy_ = DummyClassifier()\n",
    "dummy_params = {\n",
    "    'DummyClassifier__strategy': [\n",
    "        'stratified',\n",
    "        'most_frequent',\n",
    "        'prior',\n",
    "        'uniform',\n",
    "    ],\n",
    "    'DummyClassifier__random_state': [random_state],\n",
    "}\n",
    "\n",
    "dummy = [dummy_, dummy_params]\n",
    "\n",
    "### Multinomial Naive Bayes\n",
    "nb_ = MultinomialNB()\n",
    "nb_params = {\n",
    "    'MultinomialNB__fit_prior': [True, False],\n",
    "    # 'MultinomialNB__alpha': [0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "nb = [nb_, nb_params]\n",
    "\n",
    "### Bernoulli Naive Bayes\n",
    "bnb_ = BernoulliNB()\n",
    "bnb_params = {\n",
    "    'BernoulliNB__fit_prior': [True],\n",
    "    'BernoulliNB__alpha': [0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "bnb = [bnb_, bnb_params]\n",
    "\n",
    "### Gaussian Naive Bayes\n",
    "gnb_ = GaussianNB()\n",
    "gnb_params = {\n",
    "    'GaussianNB__var_smoothing': [1e-9],\n",
    "}\n",
    "\n",
    "gnb = [gnb_, gnb_params]\n",
    "\n",
    "### KNeighbors Classifier\n",
    "knn_ = KNeighborsClassifier()\n",
    "knn_params = {\n",
    "    'KNeighborsClassifier__weights': ['uniform', 'distance'],\n",
    "    'KNeighborsClassifier__n_neighbors': [2, 5, 15],\n",
    "    'KNeighborsClassifier__algorithm': ['auto'],\n",
    "    # 'KNeighborsClassifier__leaf_size': [30, 50, 100, 200, 300, 500],\n",
    "    'KNeighborsClassifier__p': [1, 2, 3, 4, 5],\n",
    "    # 'KNeighborsClassifier__metric': [\n",
    "    #     'minkowski',\n",
    "    #     'euclidean',\n",
    "    #     'cosine',\n",
    "    #     'correlation',\n",
    "    # ],\n",
    "    # 'KNeighborsClassifier__metric_params': [None, {'p': 2}, {'p': 3}],\n",
    "}\n",
    "\n",
    "knn = [knn_, knn_params]\n",
    "\n",
    "### Logistic Regression\n",
    "lr_ = LogisticRegression()\n",
    "lr_params = {\n",
    "    'LogisticRegression__penalty': ['elasticnet'],\n",
    "    'LogisticRegression__class_weight': [class_weight],\n",
    "    'LogisticRegression__random_state': [random_state],\n",
    "    'LogisticRegression__algorithm': ['auto'],\n",
    "    'LogisticRegression__fit_intercept': [True, False],\n",
    "    'LogisticRegression__multi_class': ['auto'],\n",
    "    'LogisticRegression__solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "    'LogisticRegression__C': [0.01, 1, 100],\n",
    "    # 'LogisticRegression__max_iter': [100, 200, 300, 500, 1000],\n",
    "}\n",
    "\n",
    "lr = [lr_, lr_params]\n",
    "\n",
    "### Passive Aggressive\n",
    "pa_ = PassiveAggressiveClassifier()\n",
    "pa_params = {\n",
    "    'PassiveAggressiveClassifier__loss': ['hinge', 'squared_hinge'],\n",
    "    'PassiveAggressiveClassifier__random_state': [random_state],\n",
    "    'PassiveAggressiveClassifier__fit_intercept': [True, False],\n",
    "    'PassiveAggressiveClassifier__class_weight': [class_weight],\n",
    "    'PassiveAggressiveClassifier__shuffle': [True, False],\n",
    "    'PassiveAggressiveClassifier__C': [0.01, 1, 100],\n",
    "    # 'PassiveAggressiveClassifier__max_iter': [100, 200, 300, 500, 1000],\n",
    "}\n",
    "\n",
    "pa = [pa_, pa_params]\n",
    "\n",
    "### Perceptron\n",
    "ptron_ = Perceptron()\n",
    "ptron_params = {\n",
    "    'Perceptron__penalty': ['elasticnet'],\n",
    "    'Perceptron__random_state': [random_state],\n",
    "    'Perceptron__fit_intercept': [True, False],\n",
    "    'Perceptron__class_weight': [class_weight],\n",
    "    'Perceptron__shuffle': [True, False],\n",
    "    'Perceptron__C': [0.01, 1, 100],\n",
    "    # 'Perceptron__max_iter': [100, 200, 300, 500, 1000],\n",
    "}\n",
    "\n",
    "ptron = [ptron_, ptron_params]\n",
    "\n",
    "### Stochastic Gradient Descent Aggressive\n",
    "sgd_ = SGDClassifier()\n",
    "sgd_params = {\n",
    "    'SGDClassifier__loss': ['hinge', 'squared_hinge'],\n",
    "    'SGDClassifier__random_state': [random_state],\n",
    "    'SGDClassifier__fit_intercept': [True, False],\n",
    "    'SGDClassifier__class_weight': [class_weight],\n",
    "    # 'SGDClassifier__max_iter': [100, 200, 300, 500, 1000],\n",
    "}\n",
    "\n",
    "sgd = [sgd_, sgd_params]\n",
    "\n",
    "### SVM\n",
    "svm_ = LinearSVC()\n",
    "svm_params = {\n",
    "    'LinearSVC__penalty': ['elasticnet'],\n",
    "    'LinearSVC__loss': ['hinge', 'squared_hinge'],\n",
    "    'LinearSVC__random_state': [random_state],\n",
    "    'LinearSVC__fit_intercept': [True, False],\n",
    "    'LinearSVC__class_weight': [class_weight],\n",
    "    'LinearSVC__C': [0.01, 1, 100],\n",
    "    'LinearSVC__gamma': ['scale', 'auto'],\n",
    "    # 'LinearSVC__multi_class': ['ovr', 'crammer_singer'],\n",
    "    # 'LinearSVC__max_iter': [100, 200, 300, 500, 1000],\n",
    "}\n",
    "\n",
    "svm = [svm_, svm_params]\n",
    "\n",
    "### Decision Tree\n",
    "dt_ = DecisionTreeClassifier()\n",
    "dt_params = {\n",
    "    'DecisionTreeClassifier__max_depth': [2, 5, 10],\n",
    "    'DecisionTreeClassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'DecisionTreeClassifier__random_state': [random_state],\n",
    "    'DecisionTreeClassifier__splitter': ['best', 'random'],\n",
    "    'DecisionTreeClassifier__max_features': ['auto'],\n",
    "    'DecisionTreeClassifier__class_weight': [class_weight],\n",
    "}\n",
    "\n",
    "dt = [dt_, dt_params]\n",
    "\n",
    "### Random Forest\n",
    "rf_ = RandomForestClassifier()\n",
    "rf_params = {\n",
    "    'RandomForestClassifier__max_depth': [2, 5, 10],\n",
    "    'RandomForestClassifier__n_estimators': [10, 20],\n",
    "    'RandomForestClassifier__max_features': ['auto'],\n",
    "    'RandomForestClassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'RandomForestClassifier__random_state': [random_state],\n",
    "    'RandomForestClassifier__class_weight': [class_weight],\n",
    "    'RandomForestClassifier__oob_score': [True],\n",
    "}\n",
    "\n",
    "rf = [rf_, rf_params]\n",
    "\n",
    "### Extra Trees\n",
    "et_ = ExtraTreesClassifier()\n",
    "et_params = {\n",
    "    'RandomForestClassifier__max_depth': [2, 5, 10],\n",
    "    'RandomForestClassifier__n_estimators': [10, 20],\n",
    "    'ExtraTreesClassifier__max_feature': ['auto'],\n",
    "    'ExtraTreesClassifier__random_state': [random_state],\n",
    "    'ExtraTreesClassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'ExtraTreesClassifier__class_weight': [class_weight],\n",
    "}\n",
    "\n",
    "et = [et_, et_params]\n",
    "\n",
    "### Gradient Boosting\n",
    "gbc_ = GradientBoostingClassifier()\n",
    "gbc_params = {\n",
    "    'GradientBoostingClassifier__random_state': [random_state],\n",
    "    'GradientBoostingClassifier__loss': ['log_loss', 'deviance', 'exponential'],\n",
    "    'GradientBoostingClassifier__max_features': ['auto'],\n",
    "}\n",
    "\n",
    "gbc = [gbc_, gbc_params]\n",
    "\n",
    "### AdaBoost\n",
    "ada_ = AdaBoostClassifier()\n",
    "ada_params = {\n",
    "    'AdaBoostClassifier__criterion': ['gini', 'entropy'],\n",
    "    'AdaBoostClassifier__random_state': [random_state],\n",
    "    'AdaBoostClassifier__n_estimators': [50, 100, 150],\n",
    "    'AdaBoostClassifier__base_estimator': [\n",
    "        SVC(kernel='linear'),\n",
    "        LogisticRegression(),\n",
    "        MultinomialNB(),\n",
    "        DecisionTreeClassifier(),\n",
    "    ],\n",
    "}\n",
    "\n",
    "ada = [ada_, ada_params]\n",
    "\n",
    "### XGBoost\n",
    "xgb_ = XGBClassifier()\n",
    "xgb_params = {\n",
    "    'XGBClassifier__seed': [random_state],\n",
    "    'XGBClassifier__eval_metric': ['logloss'],\n",
    "    'XGBClassifier__objective': ['binary:logistic'],\n",
    "}\n",
    "\n",
    "xgb = [xgb_, xgb_params]\n",
    "\n",
    "### MLP Classifier\n",
    "mlpc_ = MLPClassifier()\n",
    "mlpc_params = {\n",
    "    'MLPClassifier__hidden_layer_sizes': [(100,), (50,), (25,), (10,), (5,), (1,)],\n",
    "    'MLPClassifier__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'MLPClassifier__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'MLPClassifier__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'MLPClassifier__random_state': [random_state],\n",
    "}\n",
    "\n",
    "mlpc = [mlpc_, mlpc_params]\n",
    "\n",
    "### MLP Regressor\n",
    "mlpr_ = MLPRegressor()\n",
    "mlpr_params = {\n",
    "    'MLPRegressor__hidden_layer_sizes': [(100,), (50,), (25,), (10,), (5,), (1,)],\n",
    "    'MLPRegressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'MLPRegressor__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'MLPRegressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'MLPRegressor__random_state': [random_state],\n",
    "}\n",
    "\n",
    "mlpr = [mlpr_, mlpr_params]\n",
    "\n",
    "## Classifiers List\n",
    "classifers_list = [\n",
    "    dummy, nb, knn, lr, pa, ptron, svm, dt, rf, ada, xgb, mlpc, \n",
    "    # bnb, gnb, sgd, et, gbc, mlpr\n",
    "]\n",
    "\n",
    "## Classifiers Dict\n",
    "classifiers_pipe = {\n",
    "    classifier_and_params[0].__class__.__name__: classifier_and_params\n",
    "    for classifier_and_params in classifers_list\n",
    "}\n",
    "\n",
    "## Voting and Stacking Classifiers\n",
    "# Estimators for Voting and Stacking Classifiers\n",
    "voting_stacking_estimators = [\n",
    "    (classifier_and_params[0].__class__.__name__, classifier_and_params[0])\n",
    "    for classifier_and_params in classifers_list\n",
    "]\n",
    "\n",
    "### Voting Classifier\n",
    "voting_ = VotingClassifier(estimators = voting_stacking_estimators)\n",
    "voting_params = {\n",
    "    'VotingClassifier__voting': ['soft', 'hard'],\n",
    "    'VotingClassifier__weights': [None],\n",
    "}\n",
    "\n",
    "voting = [voting_, voting_params]\n",
    "\n",
    "### Stacking Classifier\n",
    "stacking_ = StackingClassifier(estimators = voting_stacking_estimators)\n",
    "stacking_params = {\n",
    "    'StackingClassifier__stack_method': ['auto', 'predict_proba', 'decision_function', 'predict'],\n",
    "    'StackingClassifier__passthrough': [True, False],\n",
    "}\n",
    "\n",
    "stacking = [stacking_, stacking_params]\n",
    "\n",
    "# Add stacking and voting classifiers to classifiers list and pipe dict\n",
    "classifers_list.append(voting)\n",
    "classifiers_pipe[voting[0].__class__.__name__] = voting\n",
    "classifers_list.append(stacking)\n",
    "classifiers_pipe[stacking[0].__class__.__name__] = stacking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3c5be4",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a26e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_and_close_plots():\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675cf660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_plots():\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f179e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df_metrics(vectorizers_pipe, classifiers_pipe, metrics_list, analysis_columns=None):\n",
    "\n",
    "    if analysis_columns is None:\n",
    "        analysis_columns = analysis_columns\n",
    "\n",
    "    index = pd.MultiIndex.from_product(\n",
    "        [list(map(lambda classifier: classifier, classifiers_pipe.keys()))],\n",
    "        names=['Classifiers'],\n",
    "    )\n",
    "    columns = pd.MultiIndex.from_product(\n",
    "        [\n",
    "            analysis_columns,\n",
    "            list(map(lambda vectorizer: vectorizer, vectorizers_pipe.keys())),\n",
    "            metrics_list,\n",
    "        ],\n",
    "        names=['Variable', 'Vectorizer', 'Measures'],\n",
    "    )\n",
    "    return pd.DataFrame(index=index, columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3840ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, col, text_col=None, analysis_columns=None):\n",
    "\n",
    "    if text_col is None:\n",
    "        text_col = 'Job Description spacy_sentencized'\n",
    "    if analysis_columns is None:\n",
    "        analysis_columns = ['Warmth', 'Competence']\n",
    "\n",
    "    train_ratio = 0.75\n",
    "    test_ratio = 0.10\n",
    "    validation_ratio = 0.15\n",
    "    test_split = test_size = 1 - train_ratio\n",
    "    validation_split = test_ratio / (test_ratio + validation_ratio)\n",
    "\n",
    "    # Split\n",
    "    print('='*20)\n",
    "    print('Splitting data into training, testing, and validation sets:')\n",
    "    print(f'Ratios: train_size = {train_ratio}, test size = {test_ratio}, validation size = {validation_ratio}')\n",
    "\n",
    "    df = df.dropna(subset=analysis_columns, how='any')\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        df, train_size = 1-test_split, test_size = test_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    val, test = train_test_split(\n",
    "        test, test_size=validation_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train = np.array(list(train[text_col].astype('str').values))\n",
    "    y_train = column_or_1d(train[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    X_test = np.array(list(test[text_col].astype('str').values))\n",
    "    y_test = column_or_1d(test[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    X_val = np.array(list(val[text_col].astype('str').values))\n",
    "    y_val = column_or_1d(val[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    class_weights = compute_class_weight(class_weight = class_weight, classes = [0,1], y = y_train)\n",
    "    class_weights_ratio = class_weights[0]/class_weights[1]\n",
    "    class_weights_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "    print('Done splitting data into training, testing, and validation sets.')\n",
    "    print('='*20)\n",
    "    print(f'Training set shape: {y_train.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Training set example:\\n{X_train[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Testing set shape: {y_test.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing set example:\\n{X_test[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Validation set shape: {y_val.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation set example:\\n{X_val[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Class weights:\\nRatio = {class_weights_ratio:.2f} (0 = {class_weights[0]:.2f}, 1 = {class_weights[1]:.2f})')\n",
    "    print('='*20)\n",
    "\n",
    "    return (\n",
    "        train, X_train, y_train,\n",
    "        test, X_test, y_test,\n",
    "        val, X_val, y_val,\n",
    "        class_weights,\n",
    "        class_weights_ratio,\n",
    "        class_weights_dict\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cc9dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_using_estimator(\n",
    "    estimator, X_test, y_test, col, vectorizer_name, classifier_name,\n",
    "    return_train_score=None, cv=None\n",
    "):\n",
    "    if return_train_score is None:\n",
    "        return_train_score = True\n",
    "    if cv is None:\n",
    "        cv = cv\n",
    "\n",
    "    # Using estimator\n",
    "    # Cross Validation\n",
    "    print('-'*20)\n",
    "    print('Cross Validating without scoring.')\n",
    "    cv_score_noscoring = cross_validate(\n",
    "        estimator,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv=cv,\n",
    "        scoring=None,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "\n",
    "    # Cross Validation with scoring\n",
    "    print('-'*20)\n",
    "    print(f'Cross Validating with {scores} scoring.')\n",
    "    cv_score_recall = cross_validate(\n",
    "        estimator,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv=cv,\n",
    "        scoring=scores,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "\n",
    "    # Get mean and std of cross validation scores\n",
    "    print('-'*20)\n",
    "    print('Getting mean and std of cross validation scores.')\n",
    "    cv_train_scores = cv_score_noscoring['train_score'].mean()\n",
    "    cv_test_scores = cv_score_noscoring['test_score'].mean()\n",
    "    cv_train_recall = cv_score_recall['train_recall'].mean()\n",
    "    cv_test_recall = cv_score_recall['test_recall'].mean()\n",
    "    cv_train_explained_variance_recall = cv_score_recall['train_explained_variance'].mean()\n",
    "    cv_test_explained_variance_recall = cv_score_recall['test_explained_variance'].mean()\n",
    "\n",
    "    # Save cross validation scores to dataframe\n",
    "    print('-'*20)\n",
    "    print('Saving cross validation scores to dataframe.')\n",
    "    df_cv_score_noscoring = pd.DataFrame(cv_score_noscoring)\n",
    "    df_cv_score_noscoring.to_pickle(f'{df_save_dir}df_cv_score_noscoring - {col}_{vectorizer_name}_{classifier_name}.pkl')\n",
    "    df_cv_score_recall = pd.DataFrame(cv_score_recall)\n",
    "    df_cv_score_recall.to_pickle(f'{df_save_dir}df_cv_score_recall - {col}_{vectorizer_name}_{classifier_name}.pkl')\n",
    "\n",
    "    return (\n",
    "        df_cv_score_recall,\n",
    "        cv_train_scores, cv_test_scores,\n",
    "        cv_train_recall, cv_test_recall,\n",
    "        cv_train_explained_variance_recall, cv_test_explained_variance_recall\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e84026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_with_estimator(\n",
    "    estimator, X_test, y_test, col, vectorizer_name, classifier_name,\n",
    "    param_name=None, param_range=None, axis=None, cv=None, n_jobs=None, random_state=None, alpha=None, verbose=None\n",
    "):\n",
    "    if axis is None:\n",
    "        axis = 1\n",
    "    if cv is None:\n",
    "        cv = cv\n",
    "    if n_jobs is None:\n",
    "        n_jobs = n_jobs\n",
    "    if random_state is None:\n",
    "        random_state = random_state\n",
    "    if alpha is None:\n",
    "        alpha = 0.1\n",
    "    if verbose is None:\n",
    "        verbose=1\n",
    "\n",
    "    # Make param names and values\n",
    "    if param_range is None:\n",
    "        param_range = np.arange(1, 100, 10)\n",
    "    if param_name is None:\n",
    "        param_names = [\n",
    "            param_name\n",
    "            for param_dict in estimator.steps\n",
    "            for param_name in \n",
    "            [\n",
    "                name\n",
    "                for name, value in param_dict[1].get_params().items()\n",
    "                if name != 'random_state'\n",
    "                and isinstance(value, (list, int, float))\n",
    "                and not isinstance(value, bool)\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "    # Using estimator\n",
    "    # Learning Curves\n",
    "    print('Plotting Learning Curve.')\n",
    "    print('-'*20)\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator=estimator,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        random_state=random_state,\n",
    "        shuffle=True,\n",
    "        scoring=scoring,\n",
    "        verbose=verbose,\n",
    "        # train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=axis)\n",
    "    train_scores_std = np.std(train_scores, axis=axis)\n",
    "    test_scores_mean = np.mean(test_scores, axis=axis)\n",
    "    test_scores_std = np.std(test_scores, axis=axis)\n",
    "\n",
    "    close_plots()\n",
    "    plt.figure()\n",
    "    plt.suptitle(\n",
    "        f'{str(col)} - Learning Curves for {scoring.title()} - {vectorizer_name} + {classifier_name}'\n",
    "        )\n",
    "    plt.xlabel('Training examples')\n",
    "    plt.ylabel('Score')\n",
    "    plt.grid()\n",
    "    plt.fill_between(\n",
    "        train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=alpha, color='r'\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=alpha, color='g'\n",
    "    )\n",
    "    plt.plot(\n",
    "        train_sizes, train_scores_mean, 'o-', color='r', label='Training score'\n",
    "    )\n",
    "    plt.plot(\n",
    "        train_sizes, test_scores_mean, 'o-', color='g', label='Cross-validation score'\n",
    "    )\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    # Save figure\n",
    "    for image_save_format in ['eps', 'png']:\n",
    "        plt.savefig(f'{plot_save_path}{method} {str(col)} - Learning Curve - {vectorizer_name} + {classifier_name}.{image_save_format}', format=image_save_format)\n",
    "    show_and_close_plots()\n",
    "\n",
    "    # # Validation Curve\n",
    "    # for param_name in param_names:\n",
    "    #     if param_name:\n",
    "    #         param_title = ' '.join(param_name.split('_')).title()\n",
    "    #         print(f'Plotting Validation Curve for {param_title}.')\n",
    "    #         print('-'*20)\n",
    "    #         train_scores, test_scores = validation_curve(\n",
    "    #             estimator=estimator,\n",
    "    #             X=X_train,\n",
    "    #             y=y_train,\n",
    "    #             param_name=param_name,\n",
    "    #             param_range=param_range,\n",
    "    #             cv=cv,\n",
    "    #             n_jobs=n_jobs,\n",
    "    #             scoring=scoring,\n",
    "    #             verbose=verbose,\n",
    "    #         )\n",
    "    #         train_scores_mean = np.mean(train_scores, axis=axis)\n",
    "    #         train_scores_std = np.std(train_scores, axis=axis)\n",
    "    #         test_scores_mean = np.mean(test_scores, axis=axis)\n",
    "    #         test_scores_std = np.std(test_scores, axis=axis)\n",
    "\n",
    "    #         # Ploting\n",
    "    #         plt.figure()\n",
    "    #         plt.suptitle(\n",
    "    #             f'{str(col)} - Validation Curve for {scoring.title()}on {param_title} - {vectorizer_name} + {classifier_name}'\n",
    "    #         )\n",
    "    #         plt.xlabel(param_name)\n",
    "    #         plt.ylabel('Score')\n",
    "    #         plt.grid()\n",
    "    #         plt.fill_between(\n",
    "    #             param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=alpha, color='r'\n",
    "    #         )\n",
    "    #         plt.fill_between(\n",
    "    #             param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=alpha, color='g'\n",
    "    #         )\n",
    "    #         plt.plot(\n",
    "    #             param_range, train_scores_mean, 'o-', color='r', label='Training score'\n",
    "    #         )\n",
    "    #         plt.plot(\n",
    "    #             param_range, test_scores_mean, 'o-', color='g', label='Cross-validation score'\n",
    "    #         )\n",
    "    #         plt.legend(loc='best')\n",
    "    #         plt.show()\n",
    "\n",
    "    #         # Save figure\n",
    "    #         for image_save_format in ['eps', 'png']:\n",
    "    #             plt.savefig(\n",
    "    #                 f'{plot_save_path}{method} {str(col)} - Validation Curve for {param_title} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "    #                 format=image_save_format\n",
    "    #             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7a0330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_y_pred(\n",
    "    y_test, y_test_pred, col, vectorizer_name, classifier_name,\n",
    "    pos_label=None, labels=None\n",
    "):\n",
    "\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    if labels is None:\n",
    "        labels = [1, 0]\n",
    "\n",
    "    # Using y_pred\n",
    "    explained_variance = metrics.explained_variance_score(y_test, y_test_pred)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "    balanced_accuracy = metrics.balanced_accuracy_score(y_test, y_test_pred)\n",
    "    precision = metrics.precision_score(y_test, y_test_pred, pos_label=1, labels=[1, 0])\n",
    "    recall = metrics.recall_score(y_test, y_test_pred, pos_label=1, labels=[1, 0])\n",
    "    f1 = metrics.f1_score(y_test, y_test_pred)\n",
    "    mcc = metrics.matthews_corrcoef(y_test, y_test_pred)\n",
    "    fm = metrics.fowlkes_mallows_score(y_test, y_test_pred)\n",
    "    kappa = metrics.cohen_kappa_score(y_test, y_test_pred)\n",
    "    gmean_iba = imblearn.metrics.make_index_balanced_accuracy(alpha=0.1, squared=True)(geometric_mean_score)\n",
    "    gmean = gmean_iba(y_test, y_test_pred)\n",
    "    report = metrics.classification_report(y_test, y_test_pred)\n",
    "    cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "    cm_normalized = metrics.confusion_matrix(y_test, y_test_pred, normalize='true')\n",
    "\n",
    "    return (\n",
    "        explained_variance, accuracy, balanced_accuracy, precision,\n",
    "        recall, f1, mcc, fm, kappa, gmean, report, cm, cm_normalized\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c21c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_label = 1\n",
    "# close_plots()\n",
    "# cm_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "#     y_test, y_test_pred\n",
    "# )\n",
    "# cm_normalized_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "#     y_test, y_test_pred, normalize='true'\n",
    "# )\n",
    "# roc_curve = metrics.RocCurveDisplay.from_predictions(\n",
    "#     y_test, y_test_pred, pos_label=pos_label\n",
    "# )\n",
    "# pr_curve = metrics.PrecisionRecallDisplay.from_predictions(\n",
    "#     y_test, y_test_pred, pos_label=pos_label\n",
    "# )\n",
    "# calibration_curve = CalibrationDisplay.from_predictions(\n",
    "#     y_test, y_test_pred, pos_label=pos_label\n",
    "# )\n",
    "# show_and_close_plots()\n",
    "\n",
    "# # Plots\n",
    "# plots_dict = {\n",
    "#     'Confusion Matrix': cm_curve,\n",
    "#     'Normalized Confusion Matrix': cm_normalized_curve,\n",
    "#     'ROC Curve': roc_curve,\n",
    "#     'Precision-Recall Curve': pr_curve,\n",
    "#     'Calibration Curve': calibration_curve,\n",
    "# }\n",
    "\n",
    "# print('=' * 20)\n",
    "# # close_plots()\n",
    "# print('Plotting metrics with y_pred_prob:')\n",
    "# print('='*20)\n",
    "# for plot_name, plot_ in plots_dict.items():\n",
    "#     close_plots()\n",
    "#     print(f'Plotting {plot_name}:')\n",
    "#     plt.figure()\n",
    "#     plt.suptitle(\n",
    "#         f'{str(col)} - {plot_name} - {vectorizer_name} + {classifier_name}'\n",
    "#         )\n",
    "#     if plot_name == 'ROC Curve':\n",
    "#         plt.plot([0, 1], [0, 1], 'r--', lw=1)\n",
    "#     try:\n",
    "#         plot_.plot(color='C0')\n",
    "#     except (TypeError, AttributeError):\n",
    "#         plot_.plot()\n",
    "#         try:\n",
    "#             plt.gca().get_lines()[0].set_color('blue')\n",
    "#         except IndexError:\n",
    "#             plot_.plot(cmap=plt.cm.Blues)\n",
    "#     plt.legend(loc='best')\n",
    "#     print('=' * 20)\n",
    "\n",
    "#     # Save Plots\n",
    "#     print(f'Saving {plot_name}...')\n",
    "#     for image_save_format in ['eps', 'png']:\n",
    "#         plt.savefig(\n",
    "#             f'{plot_save_path}{method} {str(col)} - {plot_name} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "#             format=image_save_format, dpi=3000, bbox_inches='tight'\n",
    "#         )\n",
    "#     show_and_close_plots()\n",
    "#     print(f'Saved {plot_name}!')\n",
    "#     print('=' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89a7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_with_y_pred(\n",
    "    y_test, y_test_pred, col, vectorizer_name, classifier_name,\n",
    "    pos_label=None\n",
    "):\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    # Using y_pred_prob\n",
    "    # Displays\n",
    "    close_plots()\n",
    "    cm_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, y_test_pred\n",
    "    )\n",
    "    cm_normalized_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, y_test_pred, normalize='true'\n",
    "    )\n",
    "    roc_curve = metrics.RocCurveDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=pos_label\n",
    "    )\n",
    "    pr_curve = metrics.PrecisionRecallDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=pos_label\n",
    "    )\n",
    "    calibration_curve = CalibrationDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=pos_label\n",
    "    )\n",
    "    show_and_close_plots()\n",
    "\n",
    "    # Plots\n",
    "    plots_dict = {\n",
    "        'Confusion Matrix': cm_curve,\n",
    "        'Normalized Confusion Matrix': cm_normalized_curve,\n",
    "        'ROC Curve': roc_curve,\n",
    "        'Precision-Recall Curve': pr_curve,\n",
    "        'Calibration Curve': calibration_curve,\n",
    "    }\n",
    "\n",
    "    print('=' * 20)\n",
    "    # close_plots()\n",
    "    print('Plotting metrics with y_pred_prob:')\n",
    "    print('='*20)\n",
    "\n",
    "    for plot_name, plot_ in plots_dict.items():\n",
    "        close_plots()\n",
    "        print(f'Plotting {plot_name}:')\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(\n",
    "            f'{str(col)} - {plot_name} - {vectorizer_name} + {classifier_name}'\n",
    "            )\n",
    "        if plot_name == 'ROC Curve':\n",
    "            ax.plot([0, 1], [0, 1], 'r--', lw=1)\n",
    "        plot_.plot(ax=ax)\n",
    "        print('=' * 20)\n",
    "\n",
    "        # Save Plots\n",
    "        print(f'Saving {plot_name}...')\n",
    "        for image_save_format in ['eps', 'png']:\n",
    "            plt.savefig(\n",
    "                f'{plot_save_path}{method} {str(col)} - {plot_name} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "                format=image_save_format, dpi=3000, bbox_inches='tight'\n",
    "            )\n",
    "        show_and_close_plots()\n",
    "        print(f'Saved {plot_name}!')\n",
    "        print('=' * 20)\n",
    "\n",
    "    # Visualisation with plot_metric\n",
    "    bc = plot_metric.functions.BinaryClassification(\n",
    "        y_test, y_test_pred, labels=[0, 1])\n",
    "\n",
    "    # Figures\n",
    "    close_plots()\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    plt.subplot2grid((2, 6), (1, 1), colspan=2)\n",
    "    bc.plot_confusion_matrix(colorbar=True)\n",
    "    plt.subplot2grid((2, 6), (1, 3), colspan=2)\n",
    "    bc.plot_confusion_matrix(normalize=True, colorbar=True)\n",
    "    plt.subplot2grid(shape=(2, 6), loc=(0, 0), colspan=2)\n",
    "    bc.plot_roc_curve()\n",
    "    plt.subplot2grid((2, 6), (0, 2), colspan=2)\n",
    "    bc.plot_precision_recall_curve()\n",
    "    plt.subplot2grid((2, 6), (0, 4), colspan=2)\n",
    "    bc.plot_class_distribution()\n",
    "    bc.print_report()\n",
    "    for image_save_format in ['eps', 'png']:\n",
    "        plt.savefig(\n",
    "            f'{plot_save_path}{method} {str(col)} - plot_metric Curves - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "    show_and_close_plots()\n",
    "\n",
    "    # Heatmap\n",
    "    print('Plotting Heatmap:')\n",
    "    close_plots()\n",
    "    classifications_dict = defaultdict(int)\n",
    "    for _y_test, _y_test_pred in zip(y_test, y_test_pred):\n",
    "        if _y_test != _y_test_pred:\n",
    "            classifications_dict[(_y_test, _y_test_pred)] += 1\n",
    "\n",
    "    dicts_to_plot = [\n",
    "        {\n",
    "            f'True {str(col)} value': _y_test,\n",
    "            f'Predicted {str(col)} value': _y_test_pred,\n",
    "            'Number of Classifications': _count,\n",
    "        }\n",
    "        for (_y_test, _y_test_pred), _count in classifications_dict.items()\n",
    "    ]\n",
    "    df_to_plot = pd.DataFrame(dicts_to_plot)\n",
    "    df_wide = df_to_plot.pivot_table(\n",
    "        index=f'True {str(col)} value', \n",
    "        columns=f'Predicted {str(col)} value', \n",
    "        values='Number of Classifications'\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(9,7))\n",
    "    sns.set(style='ticks', font_scale=1.2)\n",
    "    sns.heatmap(df_wide, linewidths=1, cmap=plt.cm.Blues, annot=True)    \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'{str(col)} Heatmap - {vectorizer_name} + {classifier_name}')\n",
    "    print('Saving Heatmap...')\n",
    "    for image_save_format in ['eps', 'png']:\n",
    "        plt.savefig(\n",
    "            f'{plot_save_path}{method} {str(col)} - Heatmap - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "    print('Saved Heatmap!')\n",
    "    show_and_close_plots()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de7dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_y_pred_prob(\n",
    "    y_test, y_test_pred_prob, col, vectorizer_name, classifier_name,\n",
    "    pos_label=None\n",
    "):\n",
    "\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "\n",
    "    # Using y_pred_prob\n",
    "    average_precision = metrics.average_precision_score(y_test, y_test_pred_prob)\n",
    "    roc_auc = metrics.roc_auc_score(y_test, y_test_pred_prob)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, y_test_pred_prob, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    loss = metrics.log_loss(y_test, y_test_pred_prob)\n",
    "    precision_pr, recall_pr, threshold_pr = metrics.precision_recall_curve(y_test, y_test_pred_prob, pos_label=1)\n",
    "\n",
    "    return (\n",
    "        average_precision, roc_auc, auc,\n",
    "        fpr, tpr, threshold,loss,\n",
    "        precision_pr, recall_pr, threshold_pr\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162b0699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "    col, vectorizer_name, classifier_name,\n",
    "    with_estimator=None, with_y_pred=None, with_y_pred_prob=None,\n",
    "    cv=None, pos_label=None, verbose=None, n_jobs=None,\n",
    "):\n",
    "\n",
    "    if cv is None:\n",
    "        cv = cv\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    if verbose is None:\n",
    "        verbose = 1\n",
    "    if n_jobs is None:\n",
    "        n_jobs = n_jobs\n",
    "\n",
    "    if with_estimator is None:\n",
    "        with_estimator = True\n",
    "    if with_y_pred is None:\n",
    "        with_y_pred = True\n",
    "    if with_y_pred_prob is None:\n",
    "        with_y_pred_prob = True\n",
    "\n",
    "    # Get metrics\n",
    "    print('='*20)\n",
    "    # Using estimator\n",
    "    if with_estimator:\n",
    "        print('Computing metrics using estimator.')\n",
    "        (\n",
    "            df_cv_score_recall,\n",
    "            cv_train_scores, cv_test_scores,\n",
    "            cv_train_recall, cv_test_recall,\n",
    "            cv_train_explained_variance_recall, cv_test_explained_variance_recall\n",
    "        ) = compute_metrics_using_estimator(\n",
    "             estimator, X_test, y_test, col, vectorizer_name, classifier_name,\n",
    "        )\n",
    "    # Using y_test_pred\n",
    "    if with_y_pred:\n",
    "        print('-'*20)\n",
    "        print('Computing metrics using y_test_pred.')\n",
    "        (\n",
    "            explained_variance, accuracy, balanced_accuracy, precision,\n",
    "            recall, f1, mcc, fm, kappa, gmean, report, cm, cm_normalized\n",
    "        ) = compute_metrics_with_y_pred(\n",
    "            y_test, y_test_pred, col, vectorizer_name, classifier_name\n",
    "        )\n",
    "    # Using y_test_pred_prob\n",
    "    if with_y_pred_prob:\n",
    "        print('-'*20)\n",
    "        print('Computing metrics using y_test_pred_prob.')\n",
    "        (\n",
    "            average_precision, roc_auc, auc,\n",
    "            fpr, tpr, threshold,loss,\n",
    "            precision_pr, recall_pr, threshold_pr\n",
    "        ) = compute_metrics_with_y_pred_prob(\n",
    "            y_test, y_test_pred_prob, col, vectorizer_name, classifier_name,\n",
    "        )\n",
    "\n",
    "    #Place metrics into dict\n",
    "    print('-'*20)\n",
    "    print('Appending metrics to dict.')\n",
    "    metrics_dict = {\n",
    "        'Mean Cross Validation Train Score': float(cv_train_scores),\n",
    "        'Mean Cross Validation Test Score': float(cv_test_scores),\n",
    "        f'Mean Cross Validation Train - {scoring.title()}': float(cv_train_recall),\n",
    "        f'Mean Cross Validation Test - {scoring.title()}': float(cv_test_recall),\n",
    "        f'Mean Explained Train Variance - {scoring.title()}': float(cv_train_explained_variance_recall),\n",
    "        f'Mean Explained Test Variance - {scoring.title()}': float(cv_test_explained_variance_recall),\n",
    "        'Explained Variance': float(explained_variance),\n",
    "        'Accuracy': float(accuracy),\n",
    "        'Balanced Accuracy': float(balanced_accuracy),\n",
    "        'Precision': float(precision),\n",
    "        'Average Precision': float(average_precision),\n",
    "        'Recall': float(recall),\n",
    "        'F1-score': float(f1),\n",
    "        'Matthews Correlation Coefficient': float(mcc),\n",
    "        'Fowlkes–Mallows Index': float(fm),\n",
    "        'ROC': float(roc_auc),\n",
    "        'AUC': float(auc),\n",
    "        f'{scoring.title()} Best Threshold': threshold,\n",
    "        f'{scoring.title()} Best Score': float(best_score),\n",
    "        'Log Loss/Cross Entropy': float(loss),\n",
    "        'Cohen’s Kappa': float(kappa),\n",
    "        'Geometric Mean': float(gmean),\n",
    "        'Classification Report': report,\n",
    "        'Confusion Matrix': cm,\n",
    "        'Normalized Confusion Matrix': cm_normalized\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        metrics_dict, df_cv_score_recall,\n",
    "        cv_train_scores, cv_test_scores,\n",
    "        cv_train_recall, cv_test_recall,\n",
    "        cv_train_explained_variance_recall, cv_test_explained_variance_recall,\n",
    "        explained_variance, accuracy, balanced_accuracy, precision, recall,\n",
    "        f1, mcc, fm, kappa, gmean, report, cm, cm_normalized,\n",
    "        average_precision, roc_auc, auc, fpr, tpr, threshold, \n",
    "        loss, precision_pr, recall_pr, threshold_pr,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fdf3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(\n",
    "    estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "    col, vectorizer_name, classifier_name, \n",
    "    with_estimator=None, with_y_pred=None, with_y_pred_prob=None\n",
    "):\n",
    "    if with_estimator is None:\n",
    "        with_estimator = True\n",
    "    if with_y_pred is None:\n",
    "        with_y_pred = True\n",
    "    if with_y_pred_prob is None:\n",
    "        with_y_pred_prob = True\n",
    "\n",
    "    # Plotting\n",
    "    print('~'*20)\n",
    "    print('Plotting metrics.')\n",
    "    print('~'*20)\n",
    "    # Using estimator\n",
    "    if with_estimator:\n",
    "        plot_metrics_with_estimator(\n",
    "             estimator, X_test, y_test, col, vectorizer_name, classifier_name,\n",
    "        )\n",
    "    # Using y_test_pred\n",
    "    if with_y_pred:\n",
    "        plot_metrics_with_y_pred(\n",
    "            y_test, y_test_pred, col, vectorizer_name, classifier_name,\n",
    "        )\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8278ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(\n",
    "    estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "    best_score, df_metrics,\n",
    "    col, vectorizer_name, classifier_name, scoring=None,\n",
    "):\n",
    "    if scoring is None:\n",
    "        scoring = 'recall'\n",
    "\n",
    "    # Get metrics dict\n",
    "    (\n",
    "        metrics_dict, df_cv_score_recall,\n",
    "        cv_train_scores, cv_test_scores,\n",
    "        cv_train_recall, cv_test_recall,\n",
    "        cv_train_explained_variance_recall, cv_test_explained_variance_recall,\n",
    "        explained_variance, accuracy, balanced_accuracy, precision, recall,\n",
    "        f1, mcc, fm, kappa, gmean, report, cm, cm_normalized,\n",
    "        average_precision, roc_auc, auc, fpr, tpr, threshold, \n",
    "        loss, precision_pr, recall_pr, threshold_pr,\n",
    "    ) = compute_metrics(\n",
    "        estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "        col, vectorizer_name, classifier_name\n",
    "    )\n",
    "\n",
    "    # Print metrics\n",
    "    print('=' * 20)\n",
    "    print('~' * 20)\n",
    "    print(f' Testing Metrics for {str(col)} - {vectorizer_name} + {classifier_name}')\n",
    "    print('~' * 20)\n",
    "    print(f'Classification Report:\\n {metrics_dict[\"Classification Report\"]}')\n",
    "    print('-' * 20)\n",
    "    for metric_name, metric_value in metrics_dict.items():\n",
    "        with contextlib.suppress(TypeError, ValueError):\n",
    "            metric_value = float(metric_value)\n",
    "        if isinstance(metric_value, (int, float)):\n",
    "            print(f'{metric_name}: {round(metric_value, 2)}')\n",
    "        else:\n",
    "            print(f'{metric_name}:\\n{metric_value}')\n",
    "        print('-' * 20)\n",
    "\n",
    "        # Fill Table DF\n",
    "        if isinstance(metric_value, float):\n",
    "            df_metrics.loc[\n",
    "                (classifier_name), (col, vectorizer_name, metric_name)\n",
    "            ] = metric_value\n",
    "        else:\n",
    "            df_metrics.loc[\n",
    "                (classifier_name), (col, vectorizer_name, metric_name)\n",
    "            ] = str(metric_value)\n",
    "\n",
    "    print('=' * 20)\n",
    "\n",
    "    # Plot Metrics\n",
    "    plot_metrics(\n",
    "        estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "        col, vectorizer_name, classifier_name,\n",
    "    )\n",
    "\n",
    "    return df_metrics, metrics_dict, df_cv_score_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e46bb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to place Xy and CV data in df and save\n",
    "def save_Xy_search_cv_estimator(\n",
    "    grid_search, searchcv,\n",
    "    X_train, y_train, y_train_pred,\n",
    "    X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "    X_val, y_val,\n",
    "    estimator,\n",
    "    col, vectorizer_name, classifier_name,\n",
    "    compression=None, save_path=None\n",
    "):\n",
    "\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "    if save_path is None:\n",
    "        save_path = f'{final_models_save_path}SearchCV/'\n",
    "\n",
    "    # Save search\n",
    "    ## Save grid_search\n",
    "    with open(\n",
    "        f'{save_path}{method} Grid Search {str(col)} - {vectorizer_name} + {classifier_name}.pkl', 'wb'\n",
    "    ) as f:\n",
    "        joblib.dump(grid_search, f, compress=compression)\n",
    "\n",
    "    ## Save searchcv\n",
    "    with open(\n",
    "        f'{save_path}{method} SearchCV {str(col)} - {vectorizer_name} + {classifier_name}.pkl', 'wb'\n",
    "    ) as f:\n",
    "        joblib.dump(searchcv, f, compress=compression)\n",
    "\n",
    "    ## Save searchcv data\n",
    "    df_cv_results = pd.DataFrame(searchcv.cv_results_)\n",
    "    df_cv_results.to_pickle(\n",
    "        f'{save_path}{method} df_searchcv_results - {col}_{vectorizer_name}_{classifier_name}.pkl'\n",
    "    )\n",
    "\n",
    "    # Save Xy data\n",
    "    ## Save train data\n",
    "    df_train_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'y_train_pred': y_train_pred,\n",
    "        },\n",
    "    )\n",
    "    df_train_data.to_pickle(\n",
    "        f'{save_path}{method} df_train_data - {col}_{vectorizer_name}_{classifier_name}.pkl'\n",
    "    )\n",
    "\n",
    "    ## Save test data\n",
    "    df_test_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'y_test_pred': y_test_pred,\n",
    "            'y_test_pred_prob': y_test_pred_prob,\n",
    "        },\n",
    "    )\n",
    "    df_test_data.to_pickle(\n",
    "        f'{save_path}{method} df_test_data - {col}_{vectorizer_name}_{classifier_name}.pkl'\n",
    "    )\n",
    "\n",
    "    ## Save val data\n",
    "    df_val_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_val': X_val,\n",
    "            'y_val': y_val,\n",
    "        },\n",
    "    )\n",
    "    df_val_data.to_pickle(\n",
    "        f'{save_path}{method} df_val_data - {col}_{vectorizer_name}_{classifier_name}.pkl'\n",
    "    )\n",
    "\n",
    "    # Save estimator\n",
    "    with open(\n",
    "        f'{save_path}{method} Estimator {str(col)} - {vectorizer_name} + {classifier_name}.pkl', 'wb'\n",
    "    ) as f:\n",
    "        joblib.dump(estimator, f, compress=compression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4495381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "def saving_model_and_table(df_metrics, estimator, col, vectorizer_name, classifier_name):\n",
    "\n",
    "    # Save metrics df\n",
    "    print(f'Saving Model and Table for {vectorizer_name} + {classifier_name}.')\n",
    "    df_metrics.to_csv(f'{table_save_path}Classifiers Table.csv')\n",
    "    df_metrics.to_pickle(f'{table_save_path}Classifiers Table.pkl')\n",
    "    df_metrics.to_excel(f'{table_save_path}Classifiers Table.xlsx')\n",
    "    df_metrics.to_latex(f'{table_save_path}Classifiers Table.tex')\n",
    "    df_metrics.to_markdown(f'{table_save_path}Classifiers Table.md')\n",
    "\n",
    "    # Save estimator\n",
    "    with open(f'{final_models_save_path}{method} Estimator {str(col)} - {vectorizer_name} + {classifier_name}.pkl', 'wb') as f:\n",
    "        joblib.dump(estimator, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc524e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "def get_fitted_estimators():\n",
    "    \n",
    "    estimators_list = []\n",
    "\n",
    "    for model_path in glob.glob(f'{final_models_save_path}*.pkl'):\n",
    "        with open(model_path, 'rb') as f:\n",
    "            estimators_list.append(joblib.load(f))\n",
    "\n",
    "    return estimators_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44148bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_plots(estimators_list, X_test, y_test, col, curves_dict=None, cmap=plt.cm.Blues):\n",
    "\n",
    "    curves_dict = {\n",
    "        'ROC Curve': metrics.RocCurveDisplay,\n",
    "        'Precision Recall Curve': metrics.PrecisionRecallDisplay,\n",
    "        'Calibration Curve': metrics.CalibrationDisplay,\n",
    "    }\n",
    "\n",
    "    assert len(estimators_list) != 0\n",
    "\n",
    "    for curve_name, curve_package in curves_dict.items():\n",
    "        print('-' * 20)\n",
    "        print(f'{str(col)} - {str(curve_name)}')\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(f'{str(col)} - {str(curve_name)}')\n",
    "        for estimator in estimators_list:\n",
    "            curve = curve_package.from_estimator(\n",
    "                estimator, X_test, y_test, pos_label=1, ax=ax, cmap=cmap,\n",
    "                name=f'{estimator.steps[0][0]} + {estimator.steps[1][0]} + {estimator.steps[-1][0]}'\n",
    "            )\n",
    "        show_and_close_plots()\n",
    "\n",
    "        # Save Plots\n",
    "        print('Saving plots.')\n",
    "        for image_save_format in ['eps', 'png']:\n",
    "            curve.figure_.savefig(\n",
    "                f'{plot_save_path}{method} {str(col)} - All {str(curve_name)}s.{image_save_format}',\n",
    "                format=image_save_format,\n",
    "                dpi=3000, bbox_inches='tight'\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036fcf10",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe8e7a",
   "metadata": {},
   "source": [
    "### READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fbe29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_trainning.pkl').reset_index(drop=True)\n",
    "# TODO REMOVE THIS!!!!!!\n",
    "df_manual = df_manual.groupby(analysis_columns).sample(n=50).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0212dc9",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print('#'*40)\n",
    "print('Starting!')\n",
    "print('#'*40)\n",
    "\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "with joblib.parallel_backend(backend='multiprocessing', n_jobs=n_jobs):\n",
    "    # Load Table DF\n",
    "    # df_metrics = make_df_metrics(vectorizers_pipe, classifiers_pipe, list(metrics_dict.keys()), analysis_columns)\n",
    "\n",
    "    for col in tqdm.tqdm(analysis_columns):\n",
    "\n",
    "        print('-'*20)\n",
    "        print(f'{\"=\"*30} TRAINING {col.upper()} {\"=\"*30}')\n",
    "        print('-'*20)\n",
    "        print(f'Vectorizers to be used ({len(list(vectorizers_pipe.values()))}):\\n{list(vectorizers_pipe.keys())}')\n",
    "        print(f'Total number of vectorizer parameters = {sum([len(list(vectorizers_pipe.values())[i][1]) for i in range(len(vectorizers_pipe))])}')\n",
    "        print(f'Selectors to be used ({len(list(selectors_pipe.values()))}):\\n{list(selectors_pipe.keys())}')\n",
    "        print(f'Total number of selector parameters = {sum([len(list(selectors_pipe.values())[i][1]) for i in range(len(selectors_pipe))])}')\n",
    "        print(f'Resamplers to be used ({len(list(resamplers_pipe.keys()))}):\\n{list(resamplers_pipe.keys())}')\n",
    "        print(f'Total number of resamplers parameters = {sum([len(list(resamplers_pipe.values())[i][1]) for i in range(len(resamplers_pipe))])}')\n",
    "        print(f'Classifers to be used ({len(list(classifiers_pipe.keys()))}):\\n{list(classifiers_pipe.keys())}')\n",
    "        print(f'Total number of classifers parameters = {sum([len(list(classifiers_pipe.values())[i][1]) for i in range(len(classifiers_pipe))])}')\n",
    "        \n",
    "\n",
    "        assert len(df_manual[df_manual[str(col)].map(df_manual[str(col)].value_counts() > 1)]) != 0\n",
    "\n",
    "        # Split\n",
    "        (\n",
    "            train, X_train, y_train,\n",
    "            test, X_test, y_test,\n",
    "            val, X_val, y_val,\n",
    "            class_weights,\n",
    "            class_weights_ratio,\n",
    "            class_weights_dict\n",
    "        ) = split_data(\n",
    "            df_manual, col, text_col, analysis_columns,\n",
    "        )\n",
    "\n",
    "        for (\n",
    "            vectorizer_name, vectorizer_and_params\n",
    "        ), (\n",
    "            selector_name, selector_and_params\n",
    "        ), (\n",
    "            resampler_name, resampler_and_params\n",
    "        ), (\n",
    "            classifier_name, classifier_and_params\n",
    "        ) in tqdm_product(\n",
    "            vectorizers_pipe.items(), selectors_pipe.items(), resamplers_pipe.items(), classifiers_pipe.items()\n",
    "        ):\n",
    "\n",
    "            vectorizer = vectorizer_and_params[0]\n",
    "            vectorizer_params = vectorizer_and_params[1]\n",
    "\n",
    "            selector = selector_and_params[0]\n",
    "            selector_params = selector_and_params[1]\n",
    "\n",
    "            resampler = resampler_and_params[0]\n",
    "            resampler_params = resampler_and_params[1]\n",
    "\n",
    "            classifier = classifier_and_params[0]\n",
    "            classifier_params = classifier_and_params[1]\n",
    "\n",
    "            # Pipeline\n",
    "            ## Steps\n",
    "            if col == 'Warmth':\n",
    "                steps = [\n",
    "                    (vectorizer_name, vectorizer),\n",
    "                    (selector_name, selector),\n",
    "                    (resampler_name, resampler),\n",
    "                    (classifier_name, classifier)\n",
    "                ]\n",
    "            else:\n",
    "                steps = [\n",
    "                    (vectorizer_name, vectorizer),\n",
    "                    (selector_name, selector),\n",
    "                    (classifier_name, classifier)\n",
    "                ]\n",
    "\n",
    "            ## Params\n",
    "            param_grid = {\n",
    "                **vectorizer_params,\n",
    "                **selector_params,\n",
    "                **classifier_params,\n",
    "            }\n",
    "\n",
    "            ## Pipeline\n",
    "            pipe = imblearn.pipeline.Pipeline(steps=steps)\n",
    "\n",
    "            # Search\n",
    "            print('-'*20)\n",
    "            print(f'{\"=\"*30} Using GridSearchCV {\"=\"*30}')\n",
    "            print('-'*20)\n",
    "            print(f'GridSearchCV with:\\nPipe:\\n{pipe}\\nParams:\\n{param_grid}')\n",
    "            print('+'*30)\n",
    "\n",
    "        # with joblib.parallel_backend(backend='multiprocessing', n_jobs=n_jobs):\n",
    "            grid_search = HalvingGridSearchCV(\n",
    "                estimator=pipe,\n",
    "                param_grid=param_grid,\n",
    "                cv=cv,\n",
    "                n_jobs=n_jobs,\n",
    "                return_train_score=True,\n",
    "                verbose=1,\n",
    "                random_state=random_state,\n",
    "                refit=refit,\n",
    "                scoring=scoring,\n",
    "            )\n",
    "            # grid_search = GridSearchCV(\n",
    "            #     estimator=pipe,\n",
    "            #     param_grid=param_grid,\n",
    "            #     cv=cv,\n",
    "            #     n_jobs=n_jobs,\n",
    "            #     return_train_score=True,\n",
    "            #     verbose=1,\n",
    "            #     # scoring=scores,\n",
    "            #     # refit=scoring,\n",
    "            # )\n",
    "\n",
    "            # Fit SearchCV\n",
    "            searchcv = grid_search.fit(X_train, y_train)\n",
    "\n",
    "            # # Save SearchCV\n",
    "            # with open(f'{final_models_save_path}{method} SearchCV {str(col)} - {vectorizer_name} + {classifier_name}.pkl', 'wb') as f:\n",
    "            #     joblib.dump(searchcv, f)\n",
    "\n",
    "            # HACK\n",
    "            # Identify and name steps in estimator\n",
    "            # estimator = searchcv.best_estimator_\n",
    "            # vectorizer = searchcv.estimator[0]\n",
    "            # vectorizer_name = vectorizer.__class__.__name__\n",
    "            # selector = searchcv.estimator[1]\n",
    "            # selector_name = selector.__class__.__name__\n",
    "            # classifier = searchcv.estimator[-1]\n",
    "            # classifier_name = classifier.__class__.__name__\n",
    "            # if col == 'Warmth':\n",
    "            #     resampler = searchcv.estimator[-2]\n",
    "            #     resampler_name = resampler.__class__.__name__\n",
    "            # Identify and name steps in estimator\n",
    "            # estimator = searchcv.best_estimator_\n",
    "            vectorizer = searchcv.best_estimator_[0]\n",
    "            vectorizer_name = vectorizer.__class__.__name__\n",
    "            selector = searchcv.best_estimator_[1]\n",
    "            selector_name = selector.__class__.__name__\n",
    "            classifier = searchcv.best_estimator_[-1]\n",
    "            classifier_name = classifier.__class__.__name__\n",
    "            if col == 'Warmth':\n",
    "                resampler = searchcv.best_estimator_[-2]\n",
    "                resampler_name = resampler.__class__.__name__\n",
    "\n",
    "            # Best Parameters on CV\n",
    "            # best_index = searchcv.best_index_\n",
    "            # n_splits = searchcv.n_splits_\n",
    "            # estimator = searchcv.best_estimator_\n",
    "            # best_params = searchcv.best_params_\n",
    "            # best_train_score = searchcv.best_score_\n",
    "            # best_test_score = searchcv.score(X_test, y_test)\n",
    "            # train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "            # Make predictions\n",
    "            # estimator = searchcv.best_estimator_\n",
    "            y_train_pred = (estimator:=searchcv.best_estimator_).predict(X_train)\n",
    "            if hasattr(searchcv, 'predict_proba'):\n",
    "                searchcv_predict_attr = searchcv.predict_proba\n",
    "            elif hasattr(searchcv, '_predict_proba_lr'):\n",
    "                searchcv_predict_attr = searchcv._predict_proba_lr\n",
    "            y_test_pred = searchcv.predict(X_test)\n",
    "            y_test_pred_prob = searchcv_predict_attr(X_test)[:, 1]\n",
    "\n",
    "            print('='*20)\n",
    "            print(\n",
    "                f'GridSearch - Best mean train score: M = {float(best_mean_train_score:=searchcv.cv_results_[\"mean_train_score\"][best_index:=searchcv.best_index_]):.2f}, SD = {int(best_std_train_score:=searchcv.cv_results_[\"std_train_score\"][best_index]):.2f}\\n'\n",
    "            )\n",
    "            print(\n",
    "                f'GridSearch - Best mean test score: M = {float(best_mean_test_score:=searchcv.cv_results_[\"mean_test_score\"][best_index]):.2f}, SD = {int(best_std_test_score:=searchcv.cv_results_[\"std_test_score\"][best_index]):.2f}\\n'\n",
    "            )\n",
    "            print(\n",
    "                f'Number of splits: {int(n_splits:=searchcv.n_splits_)}\\n'\n",
    "            )\n",
    "            print(\n",
    "                f'Best estimator and parameters:\\n{estimator}\\n')\n",
    "            print(\n",
    "                f'Best parameters:\\n{(best_params:=searchcv.best_params_)}\\n'\n",
    "            )\n",
    "            print(\n",
    "                f'Training Classification Report:\\n{(train_report:=classification_report(y_train, y_train_pred))}\\n'\n",
    "            )\n",
    "            print('-'*20)\n",
    "            print(\n",
    "                f'Best train score: {float(best_train_score:=searchcv.best_score_):.2f}\\n'\n",
    "            )\n",
    "            print(\n",
    "                f'Best test score: {float(best_test_score:=searchcv.score(X_test, y_test)):.2f}\\n'\n",
    "            )\n",
    "            print('-'*20)\n",
    "            print('Training Confusion Matrix:\\n')\n",
    "            close_plots()\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.set_title(f'{str(col)} - Training Confusion Matrix - {vectorizer_name} + {classifier_name}')\n",
    "            train_cm = metrics.ConfusionMatrixDisplay.from_estimator(\n",
    "                estimator, X_train, y_train, ax=ax, cmap=plt.cm.Blues\n",
    "            )\n",
    "            show_and_close_plots()\n",
    "            print('='*20)\n",
    "\n",
    "            # Place Xy and CV data in df and save\n",
    "            save_Xy_search_cv_estimator(\n",
    "                grid_search, searchcv,\n",
    "                X_train, y_train, y_train_pred,\n",
    "                X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "                X_val, y_val,\n",
    "                estimator,\n",
    "                col, vectorizer_name, classifier_name,\n",
    "            )\n",
    "\n",
    "            # HACK REMOVE THIS!!!!!!\n",
    "            sys.exit(0)\n",
    "            # Evaluate Model\n",
    "            df_metrics, metrics_dict, df_cv_score_recall = evaluation(\n",
    "                estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "                best_score, df_metrics,\n",
    "                col, vectorizer_name, classifier_name, \n",
    "            )\n",
    "\n",
    "            # Fit best model on validation set\n",
    "            print(f'Fitting {estimator}.')\n",
    "            estimator.set_params(**estimator.get_params())\n",
    "            estimator = estimator.fit(X_val, y_val)\n",
    "\n",
    "            # Save Vectorizer, Selector, and Classifier\n",
    "            saving_model_and_table(df_metrics, estimator, col, vectorizer_name, classifier_name)\n",
    "\n",
    "        # Compare Estimators\n",
    "        print('='*20)\n",
    "        print(f'Comparing Estimators for {col}')\n",
    "        comparison_plots(get_fitted_estimators(), X_test, y_test, col)\n",
    "        print('='*20)\n",
    "\n",
    "print('#'*40)\n",
    "print('DONE!')\n",
    "print('#'*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b689f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "study1_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
