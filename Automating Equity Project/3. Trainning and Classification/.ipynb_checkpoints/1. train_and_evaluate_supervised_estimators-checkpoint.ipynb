{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c3b1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# models dir\n",
    "models_save_path = f'{data_dir}classification models/'\n",
    "\n",
    "# output tables dir\n",
    "table_save_path = f'{data_dir}output tables/'\n",
    "\n",
    "# plots dir\n",
    "plot_save_path = f'{data_dir}plots/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757b442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import timeit\n",
    "import tqdm\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import joblib\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd6aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML imports\n",
    "import sklearn\n",
    "from sklearn import feature_selection, metrics, set_config, svm, utils\n",
    "from sklearn.utils import (check_consistent_length, check_random_state, check_X_y, parallel_backend)\n",
    "from sklearn.utils.validation import (check_is_fitted, column_or_1d,\n",
    "                                      has_fit_parameter)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import FeatureUnion, make_pipeline, Pipeline\n",
    "from sklearn.feature_selection import (SelectFdr, SelectFpr,\n",
    "                                       SelectFromModel, SelectFwe,\n",
    "                                       SelectKBest, SelectPercentile, chi2,\n",
    "                                       f_classif, f_regression,\n",
    "                                       mutual_info_classif,\n",
    "                                       mutual_info_regression)\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import (LogisticRegression,\n",
    "                                  PassiveAggressiveClassifier, Perceptron,\n",
    "                                  SGDClassifier)\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (AdaBoostClassifier, BaggingClassifier,\n",
    "                              BaggingRegressor, ExtraTreesClassifier,\n",
    "                              GradientBoostingClassifier,\n",
    "                              RandomForestClassifier, StackingClassifier,\n",
    "                              StackingRegressor, VotingClassifier,\n",
    "                              VotingRegressor)\n",
    "from sklearn.model_selection import (GridSearchCV, KFold, LeaveOneOut,\n",
    "                                     RandomizedSearchCV,\n",
    "                                     RepeatedStratifiedKFold, ShuffleSplit,\n",
    "                                     StratifiedKFold,\n",
    "                                     StratifiedShuffleSplit,\n",
    "                                     cross_val_score, cross_val_predict, cross_validate,\n",
    "                                     learning_curve, train_test_split)\n",
    "from sklearn.metrics import (ConfusionMatrixDisplay,accuracy_score, balanced_accuracy_score,\n",
    "                             brier_score_loss, classification_report, cohen_kappa_score,\n",
    "                             confusion_matrix, f1_score, log_loss,\n",
    "                             make_scorer, matthews_corrcoef, fowlkes_mallows_score,\n",
    "                             precision_recall_curve, precision_score,\n",
    "                             recall_score, roc_auc_score)\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "import imblearn\n",
    "from imblearn.pipeline import make_pipeline, Pipeline\n",
    "from imblearn.metrics import geometric_mean_score, make_index_balanced_accuracy\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC, RandomOverSampler\n",
    "from imblearn.under_sampling import (EditedNearestNeighbours, NearMiss,\n",
    "                                     RandomUnderSampler, TomekLinks)\n",
    "from xgboost import XGBClassifier\n",
    "import plot_metric\n",
    "from plot_metric.functions import BinaryClassification\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55afc383",
   "metadata": {},
   "source": [
    "### READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7735d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "plt.style.use('tableau-colorblind10')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "max_length = 512\n",
    "device_name = 'mps'\n",
    "n_jobs = -1\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=10, n_repeats=3, random_state=random_state)\n",
    "t = time.time()\n",
    "cores = multiprocessing.cpu_count()\n",
    "scoring = 'recall'\n",
    "scores = ['recall', 'accuracy', 'f1', 'roc_auc', 'explained_variance', 'matthews_corrcoef']\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score),\n",
    "}\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df119b8",
   "metadata": {},
   "source": [
    "## Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec7307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CountVectorizer\n",
    "count_ = CountVectorizer()\n",
    "count_params = {\n",
    "#     'TfidfVectorizer__stop_words': ['english'],\n",
    "    'CountVectorizer__analyzer': ['word'],\n",
    "    'CountVectorizer__ngram_range': [(1, 3)],\n",
    "    'CountVectorizer__lowercase': [True, False],\n",
    "    'CountVectorizer__max_df': [0.90, 0.85, 0.80, 0.75, 0.70],\n",
    "    'CountVectorizer__min_df': [0.10, 0.15, 0.20, 0.25, 0.30],\n",
    "}\n",
    "count = [count_, count_params]\n",
    "\n",
    "### TfidfVectorizer\n",
    "tfidf_ = TfidfVectorizer()\n",
    "tfidf_params = {\n",
    "#     'TfidfVectorizer__stop_words': ['english'],\n",
    "    'TfidfVectorizer__analyzer': ['word'],\n",
    "    'TfidfVectorizer__ngram_range': [(1, 3)],\n",
    "    'TfidfVectorizer__lowercase': [True, False],\n",
    "    'TfidfVectorizer___use_idf': [True, False],\n",
    "    'TfidfVectorizer__max_df': [0.90, 0.85, 0.80, 0.75, 0.70],\n",
    "    'TfidfVectorizer__min_df': [0.10, 0.15, 0.20, 0.25, 0.30],\n",
    "}\n",
    "tfidf = [tfidf_, tfidf_params]\n",
    "\n",
    "## Vectorizers List\n",
    "vectorizers_list = [\n",
    "    count, \n",
    "#     tfidf\n",
    "]\n",
    "\n",
    "### BOW FeatureUnion\n",
    "transformer_list = []\n",
    "bow_params = {}\n",
    "for vectorizer_and_params in vectorizers_list:\n",
    "    transformer_list.append(\n",
    "        tuple([vectorizer_and_params[0].__class__.__name__, vectorizer_and_params[0]])\n",
    "    )\n",
    "    for k, v in vectorizer_and_params[1].items():\n",
    "        bow_params.update({'FeatureUnion__' + k: v})\n",
    "\n",
    "bow_ = FeatureUnion(\n",
    "    transformer_list=[transformer_list]\n",
    ")\n",
    "bow = [bow_, bow_params]\n",
    "\n",
    "# ## Vectorizers List append bow\n",
    "# vectorizers_list.append(bow)\n",
    "\n",
    "## Vectorizers Dict\n",
    "vectorizers_pipe = {\n",
    "    vectorizer_and_params[0].__class__.__name__: vectorizer_and_params\n",
    "    for vectorizer_and_params in vectorizers_list\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaff38f",
   "metadata": {},
   "source": [
    "## Selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SelectKBest\n",
    "selectkbest_ = SelectKBest()\n",
    "selectkbest_params = {\n",
    "    'SelectKBest__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "    'SelectKBest__k': ['all'],\n",
    "}\n",
    "selectkbest = [selectkbest_, selectkbest_params]\n",
    "\n",
    "### SelectPercentile\n",
    "selectperc_ = SelectPercentile()\n",
    "selectperc_params = {\n",
    "    'SelectPercentile__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "    'SelectPercentile__percentile': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "}\n",
    "selectperc = [selectperc_, selectperc_params]\n",
    "\n",
    "### SelectFpr\n",
    "selectfpr_ = SelectFpr()\n",
    "selectfpr_params = {\n",
    "    'SelectFpr__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "}\n",
    "selectfpr = [selectfpr_, selectfpr_params]\n",
    "\n",
    "### SelectFdr\n",
    "selectfdr_ = SelectFdr()\n",
    "selectfdr_params = {\n",
    "    'SelectFdr__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "}\n",
    "selectfdr = [selectfdr_, selectfdr_params]\n",
    "\n",
    "### SelectFwe\n",
    "selectfwe_ = SelectFwe()\n",
    "selectfwe_params = {\n",
    "    'SelectFwe__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "}\n",
    "selectfwe = [selectfwe_, selectfwe_params]\n",
    "\n",
    "## Selectors List\n",
    "selectors_list = [\n",
    "    selectkbest,\n",
    "    # selectperc, selectfpr, selectfdr, selectfwe\n",
    "]\n",
    "## Selectors Dict\n",
    "selectors_pipe = {\n",
    "    selector_and_params[0].__class__.__name__: selector_and_params\n",
    "    for selector_and_params in selectors_list\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4da5a77",
   "metadata": {},
   "source": [
    "## Resamplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ed119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resamplers\n",
    "### SMOTETomek Resampler\n",
    "smotetomek_ = SMOTETomek()\n",
    "smotetomek_params = {\n",
    "    'SMOTETomek__random_state': [random_state],\n",
    "    'SMOTETomek__tomek': [TomekLinks(sampling_strategy='majority')],\n",
    "}\n",
    "smotetomek = [smotetomek_, smotetomek_params]\n",
    "\n",
    "## Resampler List\n",
    "resamplers_list = [\n",
    "    smotetomek,\n",
    "]\n",
    "\n",
    "## Resampler Dict\n",
    "resamplers_pipe = {\n",
    "    resampler_and_params[0].__class__.__name__: resampler_and_params\n",
    "    for resampler_and_params in resamplers_list\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ab6d49",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2222fd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers\n",
    "### Dummy Classifier\n",
    "dummy_ = DummyClassifier()\n",
    "dummy_params = {\n",
    "    'DummyClassifier__strategy': [\n",
    "        'stratified',\n",
    "        'most_frequent',\n",
    "        'prior',\n",
    "        'uniform',\n",
    "    ],\n",
    "    'DummyClassifier__random_state': [random_state],\n",
    "}\n",
    "\n",
    "dummy = [dummy_, dummy_params]\n",
    "\n",
    "### Multinomial Naive Bayes\n",
    "nb_ = MultinomialNB()\n",
    "nb_params = {\n",
    "    'MultinomialNB__fit_prior': [True],\n",
    "    'MultinomialNB__alpha': [0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "nb = [nb_, nb_params]\n",
    "\n",
    "### Bernoulli Naive Bayes\n",
    "bnb_ = BernoulliNB()\n",
    "bnb_params = {\n",
    "    'BernoulliNB__fit_prior': [True],\n",
    "    'BernoulliNB__alpha': [0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "bnb = [bnb_, bnb_params]\n",
    "\n",
    "### Gaussian Naive Bayes\n",
    "gnb_ = GaussianNB()\n",
    "gnb_params = {\n",
    "    'GaussianNB__var_smoothing': [1e-9],\n",
    "}\n",
    "\n",
    "gnb = [gnb_, gnb_params]\n",
    "\n",
    "### KNeighbors Classifier\n",
    "knn_ = KNeighborsClassifier()\n",
    "knn_params = {\n",
    "    'KNeighborsClassifier__weights': ['uniform'],\n",
    "    'KNeighborsClassifier__n_neighbors': [2, 5, 15],\n",
    "    'KNeighborsClassifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'KNeighborsClassifier__leaf_size': [30, 50, 100, 200, 300, 500],\n",
    "    'KNeighborsClassifier__p': [1, 2, 3, 4, 5],\n",
    "    'KNeighborsClassifier__metric': [\n",
    "        'minkowski',\n",
    "        'euclidean',\n",
    "        'cosine',\n",
    "        'correlation',\n",
    "    ],\n",
    "    'KNeighborsClassifier__metric_params': [None, {'p': 2}, {'p': 3}],\n",
    "}\n",
    "\n",
    "knn = [knn_, knn_params]\n",
    "\n",
    "### Logistic Regression\n",
    "lr_ = LogisticRegression()\n",
    "lr_params = {\n",
    "    'LogisticRegression__penalty': ['l2'],\n",
    "    'LogisticRegression__random_state': [random_state],\n",
    "    'LogisticRegression__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'LogisticRegression__max_iter': [100, 200, 300, 500, 1000],\n",
    "    'LogisticRegression__multi_class': ['ovr', 'multinomial'],\n",
    "    'LogisticRegression__solver': ['newton-cg', 'liblinear'],\n",
    "    'LogisticRegression__C': [0.01, 1, 100],\n",
    "}\n",
    "\n",
    "lr = [lr_, lr_params]\n",
    "\n",
    "### Passive Aggressive\n",
    "pa_ = PassiveAggressiveClassifier()\n",
    "pa_params = {\n",
    "    'PassiveAggressiveClassifier__loss': ['hinge', 'squared_hinge'],\n",
    "    'PassiveAggressiveClassifier__random_state': [random_state],\n",
    "    'PassiveAggressiveClassifier__fit_intercept': [True, False],\n",
    "    'PassiveAggressiveClassifier__class_weight': ['balanced'],\n",
    "    'PassiveAggressiveClassifier__max_iter': [100, 200, 300, 500, 1000],\n",
    "}\n",
    "\n",
    "pa = [pa_, pa_params]\n",
    "\n",
    "### Stochastic Gradient Descent Aggressive\n",
    "sgd_ = SGDClassifier()\n",
    "sgd_params = {\n",
    "    'SGDClassifier__loss': ['hinge', 'squared_hinge'],\n",
    "    'SGDClassifier__random_state': [random_state],\n",
    "    'SGDClassifier__fit_intercept': [True, False],\n",
    "    'SGDClassifier__class_weight': ['balanced'],\n",
    "    'SGDClassifier__max_iter': [100, 200, 300, 500, 1000],\n",
    "}\n",
    "\n",
    "sgd = [sgd_, sgd_params]\n",
    "\n",
    "### SVM\n",
    "svm_ = LinearSVC()\n",
    "svm_params = {\n",
    "    'LinearSVC__penalty': ['l2'],\n",
    "    'LinearSVC__loss': ['hinge', 'squared_hinge'],\n",
    "    'LinearSVC__random_state': [random_state],\n",
    "    'LinearSVC__max_iter': [100, 200, 300, 500, 1000],\n",
    "    'LinearSVC__fit_intercept': [True, False],\n",
    "    'LinearSVC__class_weight': ['balanced'],\n",
    "    'LinearSVC__multi_class': ['ovr', 'crammer_singer'],\n",
    "}\n",
    "\n",
    "svm = [svm_, svm_params]\n",
    "\n",
    "### Decision Tree\n",
    "dt_ = DecisionTreeClassifier()\n",
    "dt_params = {\n",
    "    'DecisionTreeClassifier__max_depth': [5, 10],\n",
    "    'DecisionTreeClassifier__criterion': ['gini', 'entropy'],\n",
    "    'DecisionTreeClassifier__random_state': [random_state],\n",
    "    'DecisionTreeClassifier__splitter': ['best', 'random'],\n",
    "    'DecisionTreeClassifier__max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "dt = [dt_, dt_params]\n",
    "\n",
    "### Random Forest\n",
    "rf_ = RandomForestClassifier()\n",
    "rf_params = {\n",
    "    'RandomForestClassifier__n_estimators': [10, 20],\n",
    "    'RandomForestClassifier__max_depth': [5, 10],\n",
    "    'RandomForestClassifier__max_feature': [*np.arange(0.1, 1.1, 0.1)],\n",
    "    'RandomForestClassifier__random_state': [random_state],\n",
    "    'RandomForestClassifier__class_weight': ['balanced'],\n",
    "    'RandomForestClassifier__oob_score': [True],\n",
    "}\n",
    "\n",
    "rf = [rf_, rf_params]\n",
    "\n",
    "### Extra Trees\n",
    "et_ = ExtraTreesClassifier()\n",
    "et_params = {\n",
    "    'ExtraTreesClassifier__n_estimators': [10, 20],\n",
    "    'ExtraTreesClassifier__max_depth': [5, 10],\n",
    "    'ExtraTreesClassifier__max_feature': [*np.arange(0.1, 1.1, 0.1)],\n",
    "    'ExtraTreesClassifier__random_state': [random_state],\n",
    "    'ExtraTreesClassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'ExtraTreesClassifier__class_weight': ['balanced'],\n",
    "}\n",
    "\n",
    "et = [et_, et_params]\n",
    "\n",
    "### Gradient Boosting\n",
    "gbc_ = GradientBoostingClassifier()\n",
    "gbc_params = {\n",
    "    'GradientBoostingClassifier__max_depth': [5, 10],\n",
    "    'GradientBoostingClassifier__criterion': ['gini', 'entropy'],\n",
    "    'GradientBoostingClassifier__random_state': [random_state],\n",
    "    'GradientBoostingClassifier__n_estimators': [10, 20],\n",
    "    'GradientBoostingClassifier__loss': ['deviance', 'exponential'],\n",
    "    'GradientBoostingClassifier__subsample': [*np.arange(0.1, 1.1, 0.1)],\n",
    "    'GradientBoostingClassifier__max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "gbc = [gbc_, gbc_params]\n",
    "\n",
    "### AdaBoost\n",
    "ada_ = AdaBoostClassifier()\n",
    "ada_params = {\n",
    "    'AdaBoostClassifier__max_depth': [5, 10],\n",
    "    'AdaBoostClassifier__criterion': ['gini', 'entropy'],\n",
    "    'AdaBoostClassifier__random_state': [random_state],\n",
    "    'AdaBoostClassifier__n_estimators': [50, 100, 150],\n",
    "    'AdaBoostClassifier__base_estimator': [\n",
    "        SVC(probability=True, kernel='linear'),\n",
    "        LogisticRegression(),\n",
    "        MultinomialNB(),\n",
    "    ],\n",
    "}\n",
    "\n",
    "ada = [ada_, ada_params]\n",
    "\n",
    "### XGBoost\n",
    "xgb_ = XGBClassifier()\n",
    "xgb_params = {\n",
    "    'XGBClassifier__max_depth': [5, 10],\n",
    "    'XGBClassifier__learning_rate': [0.05],\n",
    "    'XGBClassifier__n_estimators': [1000],\n",
    "    'XGBClassifier__seed': [42],\n",
    "    'XGBClassifier__nthread': [1, 2, 3, 4],\n",
    "    'XGBClassifier__objective': ['binary:logitraw', 'binary:logistic', 'binary:hinge'],\n",
    "    'XGBClassifier__eval_metric': ['auc', 'rmse', 'rmsle', 'logloss'],\n",
    "    'XGBClassifier__sample_type': ['weighted', 'uniform'],\n",
    "}\n",
    "\n",
    "xgb = [xgb_, xgb_params]\n",
    "\n",
    "### MLP Classifier\n",
    "mlpc_ = MLPClassifier()\n",
    "mlpc_params = {\n",
    "    'MLPClassifier__hidden_layer_sizes': [(100,), (50,), (25,), (10,), (5,), (1,)],\n",
    "    'MLPClassifier__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'MLPClassifier__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'MLPClassifier__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'MLPClassifier__random_state': [random_state],\n",
    "}\n",
    "\n",
    "mlpc = [mlpc_, mlpc_params]\n",
    "\n",
    "### MLP Regressor\n",
    "mlpr_ = MLPRegressor()\n",
    "mlpr_params = {\n",
    "    'MLPRegressor__hidden_layer_sizes': [(100,), (50,), (25,), (10,), (5,), (1,)],\n",
    "    'MLPRegressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'MLPRegressor__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'MLPRegressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'MLPRegressor__random_state': [random_state],\n",
    "}\n",
    "\n",
    "mlpr = [mlpr_, mlpr_params]\n",
    "\n",
    "## Classifiers List\n",
    "classifers_list = [\n",
    "    dummy, \n",
    "    # nb, bnb, gnb, knn, lr, pa, sgd, svm, dt, rf, gbc, ada, xgb, mlpc, mlpr,\n",
    "]\n",
    "\n",
    "## Classifiers Dict\n",
    "classifiers_pipe = {\n",
    "    classifier_and_params[0].__class__.__name__: classifier_and_params\n",
    "    for classifier_and_params in classifers_list\n",
    "}\n",
    "\n",
    "## Voting and Stacking Classifiers\n",
    "# Estimators for Voting and Stacking Classifiers\n",
    "voting_stacking_estimators = [\n",
    "    tuple([classifier_and_params[0].__class__.__name__, classifier_and_params[0]])\n",
    "    for classifier_and_params in classifers_list\n",
    "]\n",
    "\n",
    "### Voting Classifier\n",
    "voting_ = VotingClassifier(estimators = voting_stacking_estimators)\n",
    "voting_params = {\n",
    "    'VotingClassifier__voting': ['soft', 'hard'],\n",
    "    'VotingClassifier__weights': [None],\n",
    "}\n",
    "\n",
    "voting = [voting_, voting_params]\n",
    "\n",
    "### Stacking Classifier\n",
    "stacking_ = StackingClassifier(estimators = voting_stacking_estimators)\n",
    "stacking_params = {\n",
    "    'StackingClassifier__stack_method': ['auto', 'predict_proba', 'decision_function', 'predict'],\n",
    "    'StackingClassifier__passthrough': [True, False],\n",
    "}\n",
    "\n",
    "stacking = [stacking_, stacking_params]\n",
    "\n",
    "# # Add stacking and voting classifiers to classifiers pipe dict\n",
    "# classifiers_pipe[voting[0].__class__.__name__] = voting\n",
    "# classifiers_pipe[stacking[0].__class__.__name__] = stacking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3840ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, col, analysis_columns, text_col=None):\n",
    "\n",
    "    if text_col == None:\n",
    "        text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "    train_ratio = 0.75\n",
    "    test_ratio = 0.10\n",
    "    validation_ratio = 0.15\n",
    "    test_split = test_size = 1 - train_ratio\n",
    "    validation_split = test_ratio / (test_ratio + validation_ratio)\n",
    "\n",
    "    # Split\n",
    "    print('='*20)\n",
    "    print(f'Splitting data into training, testing, and validation sets:')\n",
    "    print(f'Ratios: train_size = {train_ratio}, test size = {test_ratio}, validation size = {validation_ratio}')\n",
    "\n",
    "    df.dropna(subset=analysis_columns, how='any', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        df, train_size = 1-test_split, test_size = test_split, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    val, test = train_test_split(\n",
    "        test, test_size=validation_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train = column_or_1d(train[text_col].astype('str').values.tolist(), warn=True)\n",
    "    y_train = column_or_1d(train[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    X_test = column_or_1d(test[text_col].astype('str').values.tolist(), warn=True)\n",
    "    y_test = column_or_1d(test[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    X_val = column_or_1d(val[text_col].astype('str').values.tolist(), warn=True)\n",
    "    y_val = column_or_1d(val[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    class_weights = compute_class_weight(class_weight = 'balanced', classes = [0,1], y = y_train)\n",
    "    class_weights_ratio = class_weights[0]/class_weights[1]\n",
    "    class_weights_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "    print(f'Done splitting data into training, testing, and validation sets.')\n",
    "    print('='*20)\n",
    "    print(f'Training set shape: {y_train.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Training set example:\\n{X_train[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Testing set shape: {y_test.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing set example:\\n{X_test[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Validation set shape: {y_val.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation set example:\\n{X_val[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Class weights:\\nRatio = {class_weights_ratio:.2f} (0 = {class_weights[0]:.2f}, 1 = {class_weights[1]:.2f})')\n",
    "    print('='*20)\n",
    "\n",
    "    return (\n",
    "        train, X_train, y_train,\n",
    "        test, X_test, y_test,\n",
    "        val, X_val, y_val,\n",
    "        class_weights_dict\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a135ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df_metrics(vectorizers_pipe, classifiers_pipe, analysis_columns, metrics_list):\n",
    "\n",
    "    index = pd.MultiIndex.from_product(\n",
    "        [list(map(lambda classifier: classifier, classifiers_pipe.keys()))],\n",
    "        names=['Classifiers'],\n",
    "    )\n",
    "    columns = pd.MultiIndex.from_product(\n",
    "        [\n",
    "            analysis_columns,\n",
    "            list(map(lambda vectorizer: vectorizer, vectorizers_pipe.keys())),\n",
    "            metrics_list,\n",
    "        ],\n",
    "        names=['Variable', 'Vectorizer', 'Measures'],\n",
    "    )\n",
    "    df_metrics = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "    return df_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fdf3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(cm_curve, cm_normalized_curve, roc_curve, pr_curve, calibration_curve, col, vectorizer_name, selector_name, classifier_name):\n",
    "    # Plots\n",
    "    print('=' * 20)\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    print('Plotting:')\n",
    "\n",
    "    ## Confusion Matrix\n",
    "    print('-' * 20)\n",
    "    print(f'Confusion Matrix:')\n",
    "    cm_curve.ax_.set_title(f'Confusion Matrix {str(col)} - {vectorizer_name} + {classifier_name}')\n",
    "    cm_curve.plot()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    ## Normalized Confusion Matrix\n",
    "    print('-' * 20)\n",
    "    print(f'Normalized Confusion Matrix:')\n",
    "    cm_normalized_curve.ax_.set_title(f'Normalized Confusion Matrix {str(col)} - {vectorizer_name} + {classifier_name}')\n",
    "    cm_normalized_curve.plot()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    \n",
    "    ## ROC Curve\n",
    "    print('-' * 20)\n",
    "    print(f'ROC Curve:')\n",
    "    roc_curve.ax_.set_title(f'ROC Curve {str(col)} - {vectorizer_name} + {classifier_name}')\n",
    "    roc_curve.plot()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    ## PR Curve\n",
    "    print('-' * 20)\n",
    "    print(f'Precision Recall Curve:')\n",
    "    pr_curve.ax_.set_title(f'Precision-Recall Curve {str(col)} - {vectorizer_name} + {classifier_name}')\n",
    "    pr_curve.plot()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    ## Calibration Curve\n",
    "    print('-' * 20)\n",
    "    print(f'Calibration Curve:')\n",
    "    calibration_curve.ax_.set_title(f'Calibration Curve {str(col)} - {vectorizer_name} + {classifier_name}')\n",
    "    calibration_curve.plot()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    print('=' * 20)\n",
    "\n",
    "    # Save Plots\n",
    "    print('Saving plots.')\n",
    "    for image_save_format in ['eps', 'png']:\n",
    "        cm_curve.figure_.savefig(\n",
    "            plot_save_path\n",
    "            + f'Confusion Matrix {str(col)} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "        \n",
    "        cm_normalized_curve.figure_.savefig(\n",
    "            plot_save_path\n",
    "            + f'Normalized Confusion Matrix {str(col)} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "        roc_curve.figure_.savefig(\n",
    "            plot_save_path\n",
    "            + f'ROC Curve {str(col)} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "        pr_curve.figure_.savefig(\n",
    "            plot_save_path\n",
    "            + f'Precision Recall Curve {str(col)} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "        calibration_curve.figure_.savefig(\n",
    "            plot_save_path\n",
    "            + f'Calibration Curve {str(col)} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44994c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(estimator, col, vectorizer_name, classifier_name, X_test, y_test, y_test_pred, y_test_pred_prob, best_score, scoring):\n",
    "\n",
    "    # Get metrics and plots\n",
    "    cross_validate_score_noscoring = cross_validate(\n",
    "        estimator,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv=cv,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "\n",
    "    cross_validate_score = cross_validate(\n",
    "        estimator,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv=cv,\n",
    "        return_train_score=True,\n",
    "        scoring=scores,\n",
    "    )\n",
    "\n",
    "    mean_validation_score = cross_validate_score_noscoring.get('test_score').mean()\n",
    "    explained_variance = cross_validate_score.get('test_explained_variance').mean()\n",
    "    accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "    precision = metrics.precision_score(y_test, y_test_pred, pos_label=1, labels=[1, 0])\n",
    "    recall = metrics.recall_score(y_test, y_test_pred, pos_label=1, labels=[1, 0])\n",
    "    f1 = metrics.f1_score(y_test, y_test_pred)\n",
    "    mcc = metrics.matthews_corrcoef(y_test, y_test_pred)\n",
    "    fm = metrics.fowlkes_mallows_score(y_test, y_test_pred)\n",
    "    loss = metrics.log_loss(y_test, y_test_pred_prob)\n",
    "    kappa = metrics.cohen_kappa_score(y_test, y_test_pred)\n",
    "    gmean_iba = imblearn.metrics.make_index_balanced_accuracy(alpha=0.1, squared=True)(geometric_mean_score)\n",
    "    gmean = gmean_iba(y_test, y_test_pred)\n",
    "    report = metrics.classification_report(y_test, y_test_pred)\n",
    "    cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "    cm_normalized = metrics.confusion_matrix(y_test, y_test_pred, normalize='true')\n",
    "    roc_auc = metrics.roc_auc_score(y_test, y_test_pred)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, y_test_pred_prob, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    precision_pr, recall_pr, threshold_pr = metrics.precision_recall_curve(y_test, y_test_pred_prob, pos_label=1)\n",
    "    cm_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, y_test_pred\n",
    "    )\n",
    "    cm_normalized_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, y_test_pred, normalize='true'\n",
    "    )\n",
    "    roc_curve = metrics.RocCurveDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=1, name=f'{str(col)} - {vectorizer_name} + {classifier_name}'\n",
    "    )\n",
    "    pr_curve = metrics.PrecisionRecallDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=1, name=f'{str(col)} - {vectorizer_name} + {classifier_name}'\n",
    "    )\n",
    "    calibration_curve = CalibrationDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=1, name=f'{str(col)} - {vectorizer_name} + {classifier_name}'\n",
    "    )\n",
    "\n",
    "    metrics_dict = {\n",
    "        'Mean Validation Score': mean_validation_score,\n",
    "        'Explained Variance': explained_variance,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-score': f1,\n",
    "        'Matthews Correlation Coefficient': mcc,\n",
    "        'Fowlkes–Mallows Index': fm,\n",
    "        'ROC': roc_auc,\n",
    "        'AUC': auc,\n",
    "        f'{scoring.title()} Best Threshold': threshold,\n",
    "        f'{scoring.title()}  Best Score': best_score,\n",
    "        'Log Loss/Cross Entropy': loss,\n",
    "        'Cohen’s Kappa': kappa,\n",
    "        'Geometric Mean': gmean,\n",
    "        'Classification Report': report,\n",
    "        'Confusion Matrix': cm,\n",
    "        'Normalized Confusion Matrix': cm_normalized\n",
    "    }\n",
    "\n",
    "    # Print metrics\n",
    "    print('=' * 20)\n",
    "    print('~' * 20)\n",
    "    print(f' Metrics for {str(col)} - {vectorizer_name} + {classifier_name}')\n",
    "    print('~' * 20)\n",
    "    print(f'Classification Report:\\n {report}')\n",
    "    print('-' * 20)\n",
    "    for metric_name, metric_value in metrics_dict.items():\n",
    "        print(f'{metric_name}: {metric_value}')\n",
    "        print('-' * 20)\n",
    "\n",
    "        # Make Table DF\n",
    "        df_metrics = make_df_metrics(vectorizers_pipe, classifiers_pipe, analysis_columns, list(metrics_dict.keys()))\n",
    "        if isinstance(metric_value, float):\n",
    "            df_metrics.loc[\n",
    "                (classifier_name), (col, vectorizer_name, metric_name)\n",
    "            ] = float(metric_value)\n",
    "        else:\n",
    "            df_metrics.loc[\n",
    "                (classifier_name), (col, vectorizer_name, metric_name)\n",
    "            ] = str(metric_value)\n",
    "\n",
    "    print('=' * 20)\n",
    "    \n",
    "    # Visualisation with plot_metric\n",
    "    bc = plot_metric.functions.BinaryClassification(y_test, y_test_pred, labels=[0, 1])\n",
    "\n",
    "    # Figures\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.subplot2grid((2,6), (1,1), colspan=2)\n",
    "    bc.plot_confusion_matrix(colorbar=True)\n",
    "    plt.subplot2grid((2,6), (1,3), colspan=2)\n",
    "    bc.plot_confusion_matrix(normalize=True, colorbar=True)\n",
    "    plt.subplot2grid(shape=(2,6), loc=(0,0), colspan=2)\n",
    "    bc.plot_roc_curve()\n",
    "    plt.subplot2grid((2,6), (0,2), colspan=2)\n",
    "    bc.plot_precision_recall_curve()\n",
    "    plt.subplot2grid((2,6), (0,4), colspan=2)\n",
    "    bc.plot_class_distribution()\n",
    "    plt.show()\n",
    "    bc.print_report()\n",
    "\n",
    "    # Plot Metrics\n",
    "    plot_metrics(\n",
    "        cm_curve, cm_normalized_curve, roc_curve, pr_curve, calibration_curve, col, vectorizer_name, selector_name, classifier_name\n",
    "    )\n",
    "\n",
    "    return df_metrics, metrics_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b48de3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "def saving_model_and_table(col, df_metrics, estimator, vectorizer_name, classifier_name):\n",
    "\n",
    "    # Save classifier\n",
    "    print(f'Saving Model and Table for {vectorizer_name} + {classifier_name}.')\n",
    "    df_metrics.to_csv(table_save_path + 'Classifiers Table.csv')\n",
    "    df_metrics.to_pickle(table_save_path + 'Classifiers Table.pkl')\n",
    "    df_metrics.to_excel(table_save_path + 'Classifiers Table.xlsx')\n",
    "    df_metrics.style.to_latex(table_save_path + 'Classifiers Table.tex')\n",
    "    df_metrics.to_markdown(table_save_path + 'Classifiers Table.md')\n",
    "\n",
    "    with open(f'{models_save_path}Estimator {str(col)} - {vectorizer_name} + {classifier_name}.pkl', 'wb') as f:\n",
    "        joblib.dump(estimator, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc524e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "def get_fitted_estimators():\n",
    "    \n",
    "    estimators_list = []\n",
    "\n",
    "    for model_path in glob.glob(models_save_path+'*.pkl'):\n",
    "        with open(model_path, 'rb') as f:\n",
    "            estimators_list.append(joblib.load(f))\n",
    "\n",
    "    return estimators_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e44624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_plots(estimators_list, X_test, y_test, col):\n",
    "\n",
    "    assert len(estimators_list) != 0\n",
    "\n",
    "    print('=' * 20)\n",
    "    ## ROC Curve\n",
    "    print('-' * 20)\n",
    "    print(f'ROC Curve: {str(col)}')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(f'ROC Curve: {str(col)}')\n",
    "    for i, estimator in enumerate(estimators_list):\n",
    "        roc_curve = metrics.RocCurveDisplay.from_estimator(\n",
    "            estimator, X_test, y_test, pos_label=1, ax=ax, name=f'{estimator.steps[0][0]} + {estimator.steps[1][0]} + {estimator.steps[-1][0]}'\n",
    "        )\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    ## PR Curve\n",
    "    print('-' * 20)\n",
    "    print(f'Precision Recall Curve: {str(col)}')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(f'Precision Recall Curve: {str(col)}')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_xlabel('Recall')\n",
    "    for estimator in estimators_list:\n",
    "        pr_curve = metrics.PrecisionRecallDisplay.from_estimator(\n",
    "            estimator, X_test, y_test, pos_label=1, ax=ax, name=f'{estimator.steps[0][0]} + {estimator.steps[1][0]} + {estimator.steps[-1][0]}'\n",
    "        )\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    ## Calibration Curve\n",
    "    print('-' * 20)\n",
    "    print(f'Calibration Curve: {str(col)}')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(f'Calibration Curve: {str(col)}')\n",
    "    for estimator in estimators_list:\n",
    "        calibration_curve = CalibrationDisplay.from_estimator(\n",
    "            estimator, X_test, y_test, pos_label=1, ax=ax, name=f'{estimator.steps[0][0]} + {estimator.steps[1][0]} + {estimator.steps[-1][0]}'\n",
    "        )\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    print('=' * 20)\n",
    "\n",
    "    # Save Plots\n",
    "    print('Saving plots.')\n",
    "    for image_save_format in ['eps', 'png']:\n",
    "        roc_curve.figure_.savefig(\n",
    "            plot_save_path\n",
    "            + f'All ROC Curve {str(col)}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "        pr_curve.figure_.savefig(\n",
    "            plot_save_path\n",
    "            + f'All Precision Recall Curve {str(col)}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "        calibration_curve.figure_.savefig(\n",
    "            plot_save_path\n",
    "            + f'All Calibration Curve {str(col)}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe8e7a",
   "metadata": {},
   "source": [
    "### READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fbe29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_trainning.pkl').reset_index(drop=True)\n",
    "# REMOVE THIS!!!!!!\n",
    "df_manual = df_manual.groupby(analysis_columns).sample(n=50).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0212dc9",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print('#'*40)\n",
    "print('Starting!')\n",
    "print('#'*40)\n",
    "\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "for col in analysis_columns:\n",
    "\n",
    "    print('-'*20)\n",
    "    print(f'{\"=\"*30} TRAINING {col.upper()} {\"=\"*30}')\n",
    "    print('-'*20)\n",
    "\n",
    "    assert len(df_manual[df_manual[str(col)].map(df_manual[str(col)].value_counts() > 1)]) != 0\n",
    "\n",
    "    # Split\n",
    "    (\n",
    "        train, X_train, y_train,\n",
    "        test, X_test, y_test,\n",
    "        val, X_val, y_val,\n",
    "        class_weights_dict\n",
    "    ) = split_data(\n",
    "        df=df_manual, col=col, analysis_columns=analysis_columns, text_col=text_col\n",
    "    )\n",
    "\n",
    "    for (\n",
    "        vectorizer_name, vectorizer_and_params\n",
    "    ), (\n",
    "        selector_name, selector_and_params\n",
    "    ), (\n",
    "        resampler_name, resampler_and_params\n",
    "    ), (\n",
    "        classifier_name, classifier_and_params\n",
    "    ) in itertools.product(\n",
    "        vectorizers_pipe.items(), selectors_pipe.items(), resamplers_pipe.items(), classifiers_pipe.items()\n",
    "    ):\n",
    "\n",
    "        vectorizer = vectorizer_and_params[0]\n",
    "        vectorizer_params = vectorizer_and_params[1]\n",
    "\n",
    "        selector = selector_and_params[0]\n",
    "        selector_params = selector_and_params[1]\n",
    "\n",
    "        resampler = resampler_and_params[0]\n",
    "        resampler_params = resampler_and_params[1]\n",
    "\n",
    "        classifier = classifier_and_params[0]\n",
    "        classifier_params = classifier_and_params[1]\n",
    "\n",
    "        # Pipeline\n",
    "        ## Steps\n",
    "        if col == 'Warmth':\n",
    "            steps = [\n",
    "                (vectorizer_name, vectorizer),\n",
    "                (selector_name, selector),\n",
    "                (resampler_name, resampler),\n",
    "                (classifier_name, classifier)\n",
    "            ]\n",
    "        else:\n",
    "            steps = [\n",
    "                (vectorizer_name, vectorizer),\n",
    "                (selector_name, selector),\n",
    "                (classifier_name, classifier)\n",
    "            ]\n",
    "\n",
    "        ## Params\n",
    "        param_grid = {\n",
    "            **vectorizer_params,\n",
    "            **selector_params,\n",
    "            **classifier_params,\n",
    "        }\n",
    "\n",
    "        ## Pipeline\n",
    "        pipe = imblearn.pipeline.Pipeline(steps=steps)\n",
    "\n",
    "        # Search\n",
    "        print('-'*20)\n",
    "        print(f'{\"=\"*30} Using GridSearchCV {\"=\"*30}')\n",
    "        print('-'*20)\n",
    "        print(f'GridSearchCV with:\\nPipe:\\n{pipe}\\nParams:\\n{param_grid}')\n",
    "        print('+'*30)\n",
    "        search = GridSearchCV(\n",
    "            estimator=pipe,\n",
    "            param_grid=param_grid,\n",
    "            n_jobs=n_jobs,\n",
    "            scoring=scores,\n",
    "            cv=cv,\n",
    "            refit=scores[0],\n",
    "            return_train_score=True,\n",
    "        )\n",
    "\n",
    "        # Fit SearchCV\n",
    "        with joblib.parallel_backend(backend='threading', n_jobs=n_jobs):\n",
    "            searchcv = search.fit(X_train, y_train)\n",
    "\n",
    "        # Best Parameters\n",
    "        best_index = searchcv.best_index_\n",
    "        cv_results = sorted(searchcv.cv_results_)\n",
    "        best_params = searchcv.best_params_\n",
    "        best_score = searchcv.best_score_\n",
    "        n_splits = searchcv.n_splits_\n",
    "        estimator = searchcv.best_estimator_\n",
    "        y_train_pred = estimator.predict(X_train)\n",
    "\n",
    "        # Identify and name steps in estimator\n",
    "        vectorizer = estimator[0]\n",
    "        vectorizer_name = vectorizer.__class__.__name__\n",
    "        selector = estimator[1]\n",
    "        selector_name = selector.__class__.__name__\n",
    "        classifier = estimator[-1]\n",
    "        classifier_name = classifier.__class__.__name__\n",
    "        if col == 'Warmth':\n",
    "            resampler = estimator[-2]\n",
    "            resampler_name = resampler.__class__.__name__\n",
    "\n",
    "        print('=' * 20)\n",
    "        print(f'Best index for {scores[0]}: {best_index}')\n",
    "        print(f'Best estimator for {scores[0]}: {estimator}')\n",
    "        print(f'Best params for {scores[0]}: {best_params}')\n",
    "        print(f'Best score for {scores[0]}: {best_score}')\n",
    "        print(f'Number of splits for {scores[0]}: {n_splits}')\n",
    "\n",
    "        print('-' * 20)\n",
    "        train_report = classification_report(y_train, y_train_pred)\n",
    "        print(f'Training Classification Report:\\n{train_report}')\n",
    "        print(f'Training Confusion Matrix:')\n",
    "        train_cm = metrics.ConfusionMatrixDisplay.from_estimator(\n",
    "            estimator, X_train, y_train\n",
    "        ).ax_.set_title(f'Training Confusion Matrix {str(col)} - {vectorizer_name} + {classifier_name}')\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "        print('=' * 20)\n",
    "\n",
    "        # Make the predictions\n",
    "        if hasattr(searchcv, 'predict_proba'):\n",
    "            searchcv_predict_attr = searchcv.predict_proba\n",
    "        elif hasattr(searchcv, '_predict_proba_lr'):\n",
    "            searchcv_predict_attr = searchcv._predict_proba_lr\n",
    "        score = searchcv.score(X_test, y_test)\n",
    "        y_test_pred = searchcv.predict(X_test)\n",
    "        y_test_pred_prob = searchcv_predict_attr(X_test)[:, 1]\n",
    "\n",
    "        # Fit Best Model\n",
    "        print(f'Fitting {estimator}.')\n",
    "        estimator.set_params(**estimator.get_params())\n",
    "        estimator = estimator.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate Model\n",
    "        df_metrics, metrics_dict = evaluation(\n",
    "            estimator, col, vectorizer_name, classifier_name, X_test, y_test, y_test_pred, y_test_pred_prob, best_score, scoring\n",
    "        )\n",
    "\n",
    "        # Save Vectorizer, Selector, and Classifier\n",
    "        saving_model_and_table(col, df_metrics, estimator, vectorizer_name, classifier_name)\n",
    "\n",
    "    # Compare Estimators\n",
    "    print('='*20)\n",
    "    print(f'Comparing Estimators for {col}')\n",
    "    comparison_plots(get_fitted_estimators(), X_test, y_test, col)\n",
    "    print('='*20)\n",
    "\n",
    "print('#'*40)\n",
    "print('DONE!')\n",
    "print('#'*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751d412d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "study1_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
