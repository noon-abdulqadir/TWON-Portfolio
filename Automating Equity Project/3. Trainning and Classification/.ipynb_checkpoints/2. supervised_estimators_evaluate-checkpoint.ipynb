{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # isort:skip # fmt:skip # noqa # nopep8\n",
    "import sys  # isort:skip # fmt:skip # noqa # nopep8\n",
    "from pathlib import Path  # isort:skip # fmt:skip # noqa # nopep8\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "sys.path.append(code_dir)\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef3f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup_module.imports import * # isort:skip # fmt:skip # noqa # nopep8\n",
    "from supervised_estimators_pipe import * # isort:skip # fmt:skip # noqa # nopep8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55afc383",
   "metadata": {},
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*20)\n",
    "            print('Training Confusion Matrix:\\n')\n",
    "            close_plots()\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.set_title(\n",
    "                f'{str(col)} - Training Confusion Matrix - {vectorizer_name} + {classifier_name}'\n",
    "            )\n",
    "            train_cm = metrics.ConfusionMatrixDisplay.from_estimator(\n",
    "                estimator, X_train, y_train, ax=ax, cmap=plt.cm.Blues\n",
    "            )\n",
    "            show_and_close_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b36fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "# Sklearn variables\n",
    "method = 'Supervised'\n",
    "final_models_save_path = f'{models_save_path}{method} Results/'\n",
    "t = time.time()\n",
    "n_jobs = -1\n",
    "n_splits = 10\n",
    "n_repeats = 3\n",
    "random_state = 42\n",
    "refit = True\n",
    "class_weight = 'balanced'\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=n_splits, n_repeats=n_repeats, random_state=random_state\n",
    ")\n",
    "scoring = 'recall'\n",
    "scores = [\n",
    "    'recall', 'accuracy', 'f1', 'roc_auc',\n",
    "    'explained_variance', 'matthews_corrcoef'\n",
    "]\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score),\n",
    "}\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "metrics_dict = {\n",
    "    'Mean Cross Validation Train Score': np.nan,\n",
    "    'Mean Cross Validation Test Score': np.nan,\n",
    "    f'Mean Explained Train Variance - {scoring.title()}': np.nan,\n",
    "    f'Mean Explained Test Variance - {scoring.title()}': np.nan,\n",
    "    'Explained Variance': np.nan,\n",
    "    'Accuracy': np.nan,\n",
    "    'Balanced Accuracy': np.nan,\n",
    "    'Precision': np.nan,\n",
    "    'Recall': np.nan,\n",
    "    'F1-score': np.nan,\n",
    "    'Matthews Correlation Coefficient': np.nan,\n",
    "    'Fowlkes–Mallows Index': np.nan,\n",
    "    'ROC': np.nan,\n",
    "    'AUC': np.nan,\n",
    "    f'{scoring.title()} Best Threshold': np.nan,\n",
    "    f'{scoring.title()} Best Score': np.nan,\n",
    "    'Log Loss/Cross Entropy': np.nan,\n",
    "    'Cohen’s Kappa': np.nan,\n",
    "    'Geometric Mean': np.nan,\n",
    "    'Classification Report': np.nan,\n",
    "    'Confusion Matrix': np.nan,\n",
    "    'Normalized Confusion Matrix': np.nan\n",
    "}\n",
    "\n",
    "# Transformer variables\n",
    "max_length = 512\n",
    "returned_tensor = 'pt'\n",
    "cpu_counts = torch.multiprocessing.cpu_count()\n",
    "device = torch.device('mps') if torch.has_mps and torch.backends.mps.is_built() and torch.backends.mps.is_available(\n",
    ") else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device_name = str(device.type)\n",
    "print(f'Using {device_name.upper()}')\n",
    "# Set random seed\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "DetectorFactory.seed = random_state\n",
    "cores = multiprocessing.cpu_count()\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\n",
    "    bert_model_name, strip_accents=True\n",
    ")\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    bert_model_name\n",
    ").to(device)\n",
    "\n",
    "# Plotting variables\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "tqdm.tqdm.pandas(desc='progress-bar')\n",
    "tqdm_auto.tqdm.pandas(desc='progress-bar')\n",
    "tqdm.notebook.tqdm().pandas(desc='progress-bar')\n",
    "tqdm_auto.notebook_tqdm().pandas(desc='progress-bar')\n",
    "# pbar = progressbar.ProgressBar(maxval=10)\n",
    "mpl.use('MacOSX')\n",
    "mpl.style.use(f'{code_dir}/setup_module/apa.mplstyle-main/apa.mplstyle')\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "font = {'family': 'arial', 'weight': 'normal', 'size': 10}\n",
    "mpl.rc('font', **font)\n",
    "plt.style.use('tableau-colorblind10')\n",
    "plt.set_cmap('Blues')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd4400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME\n",
    "# Fit best model on validation set using SelectFromModel\n",
    "# HACK\n",
    "print(f'Fitting {estimator}.')\n",
    "sys.exit(0)\n",
    "# estimator.set_params(**estimator.get_params())\n",
    "# # estimator.fit(X_val, y_val)\n",
    "# features_important = SelectFromModel(estimator).fit_transform(X_val, y_val)\n",
    "# estimator = estimator.fit(SelectFromModel(estimator).fit(X_val, y_val), y_val)\n",
    "\n",
    "# vectorizer = estimator[0]\n",
    "# selector = estimator[1]\n",
    "# classifier = estimator[-1]\n",
    "# if col == 'Warmth':\n",
    "#     resampler = estimator[-2]\n",
    "\n",
    "# X_val = vectorizer.fit_transform(X_val)\n",
    "# X_val = selector.fit_transform(X_val, y_val)\n",
    "# if col == 'Warmth':\n",
    "#     X_val, y_val = resampler.fit_resample(X_val, y_val)\n",
    "\n",
    "# model_selector = SelectFromModel(classifier)\n",
    "# features_important = model_selector.fit_transform(X_val, y_val)\n",
    "# classifier = classifier.fit(features_important, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3c5be4",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a26e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_and_close_plots():\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675cf660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_plots():\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f179e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df_metrics(vectorizers_pipe, classifiers_pipe, metrics_list, analysis_columns=None):\n",
    "\n",
    "    if analysis_columns is None:\n",
    "        analysis_columns = analysis_columns\n",
    "\n",
    "    index = pd.MultiIndex.from_product(\n",
    "        [list(map(lambda classifier: classifier, classifiers_pipe.keys()))],\n",
    "        names=['Classifiers'],\n",
    "    )\n",
    "    columns = pd.MultiIndex.from_product(\n",
    "        [\n",
    "            analysis_columns,\n",
    "            list(map(lambda vectorizer: vectorizer, vectorizers_pipe.keys())),\n",
    "            metrics_list,\n",
    "        ],\n",
    "        names=['Variable', 'Vectorizer', 'Measures'],\n",
    "    )\n",
    "    return pd.DataFrame(index=index, columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c641cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to place Xy and CV data in df and save\n",
    "def load_Xy_search_cv_estimator(\n",
    "    col, vectorizer_name, classifier_name,\n",
    "    compression=None, save_path=None\n",
    "):\n",
    "\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "    if save_path is None:\n",
    "        save_path = f'{final_models_save_path}SearchCV/'\n",
    "\n",
    "    # Save search\n",
    "    ## Save grid_search\n",
    "    with open(\n",
    "        f'{save_path}{method} Grid Search {str(col)} - {vectorizer_name} + {classifier_name}.pkl', 'rb'\n",
    "    ) as f:\n",
    "        grid_search = joblib.load(f)\n",
    "\n",
    "    ## Save searchcv\n",
    "    with open(\n",
    "        f'{save_path}{method} SearchCV {str(col)} - {vectorizer_name} + {classifier_name}.pkl', 'rb'\n",
    "    ) as f:\n",
    "        searchcv = joblib.load(f)\n",
    "\n",
    "    ## Save searchcv data\n",
    "    df_cv_results.read_pickle(\n",
    "        f'{save_path}{method} df_searchcv_results - {col}_{vectorizer_name}_{classifier_name}.pkl'\n",
    "    )\n",
    "\n",
    "    # Save Xy data\n",
    "    ## Save train data\n",
    "    df_train_data.read_pickle(\n",
    "        f'{save_path}{method} df_train_data - {col}_{vectorizer_name}_{classifier_name}.pkl'\n",
    "    )\n",
    "    X_train = df_train_data['X_train'].values\n",
    "    y_train = df_train_data['y_train'].values\n",
    "    y_train_pred = df_train_data['y_train_pred'].values\n",
    "\n",
    "    ## Save test data\n",
    "    df_test_data.read_pickle(\n",
    "        f'{save_path}{method} df_test_data - {col}_{vectorizer_name}_{classifier_name}.pkl'\n",
    "    )\n",
    "    X_test = df_test_data['X_test'].values\n",
    "    y_test = df_test_data['y_test'].values\n",
    "    y_test_pred = df_test_data['y_test_pred'].values\n",
    "    y_test_pred_prob = df_test_data['y_test_pred_prob'].values\n",
    "\n",
    "    ## Save val data\n",
    "    df_val_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_val': X_val,\n",
    "            'y_val': y_val,\n",
    "        },\n",
    "    )\n",
    "    df_val_data.read_pickle(\n",
    "        f'{save_path}{method} df_val_data - {col}_{vectorizer_name}_{classifier_name}.pkl'\n",
    "    )\n",
    "    X_val = df_val_data['X_val'].values\n",
    "    y_val = df_val_data['y_val'].values\n",
    "\n",
    "    # Save feature importance\n",
    "    if df_feature_importances is not None:\n",
    "        df_feature_importances.to_pickle(\n",
    "            f'{save_path}{method} df_feature_importances - {col}_{vectorizer_name}_{classifier_name}.pkl'\n",
    "        )\n",
    "\n",
    "    # Save estimator\n",
    "    with open(\n",
    "        f'{final_models_save_path}{method} Estimator {str(col)} - {vectorizer_name} + {classifier_name}.pkl', 'rb'\n",
    "    ) as f:\n",
    "        estimator = joblib.load(f)\n",
    "    \n",
    "    return (\n",
    "        grid_search, searchcv,\n",
    "        X_train, y_train, y_train_pred,\n",
    "        X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "        X_val, y_val,\n",
    "        df_feature_importances, estimator,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3840ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data(\n",
    "    X_train, y_train, y_train_pred,\n",
    "    X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "    X_val, y_val,\n",
    "    estimator,\n",
    "    text_col=None, analysis_columns=None):\n",
    "    \n",
    "    check_consistent_length(X_train, X_test, X_val)\n",
    "    check_consistent_length(y_train, y_test, y_val, y_train_pred, y_test_pred, y_test_pred_prob)\n",
    "    check_is_fitted(estimator)\n",
    "\n",
    "    train_class_weights = compute_class_weight(class_weight = class_weight, classes = [0,1], y = y_train)\n",
    "    train_class_weights_ratio = train_class_weights[0]/train_class_weights[1]\n",
    "    train_class_weights_dict = dict(zip(np.unique(y_train), train_class_weights))\n",
    "    \n",
    "    test_class_weights = compute_class_weight(class_weight = class_weight, classes = [0,1], y = y_test)\n",
    "    test_class_weights_ratio = test_class_weights[0]/test_class_weights[1]\n",
    "    test_class_weights_dict = dict(zip(np.unique(y_test), test_class_weights))\n",
    "\n",
    "    print('Done splitting data into training, testing, and validation sets.')\n",
    "    print('='*20)\n",
    "    print(f'Training set shape: {y_train.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Training set example:\\n{X_train[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Testing set shape: {y_test.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing set example:\\n{X_test[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Validation set shape: {y_val.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation set example:\\n{X_val[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Training data class weights:\\nRatio = {train_class_weights_ratio:.2f} (0 = {train_class_weights[0]:.2f}, 1 = {train_class_weights[1]:.2f})')\n",
    "    print('-'*10)\n",
    "    print(f'Testing data class weights:\\nRatio = {test_class_weights_ratio:.2f} (0 = {test_class_weights[0]:.2f}, 1 = {test_class_weights[1]:.2f})')\n",
    "    print('='*20)\n",
    "\n",
    "    return (\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "        train_class_weights,\n",
    "        train_class_weights_ratio,\n",
    "        train_class_weights_dict,\n",
    "        test_class_weights,\n",
    "        test_class_weights_ratio,\n",
    "        test_class_weights_dict\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cc9dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_using_estimator(\n",
    "    estimator, X_test, y_test, col, vectorizer_name, classifier_name,\n",
    "    return_train_score=None, cv=None\n",
    "):\n",
    "    if return_train_score is None:\n",
    "        return_train_score = True\n",
    "    if cv is None:\n",
    "        cv = cv\n",
    "\n",
    "    # Using estimator\n",
    "    # Cross Validation\n",
    "    print('-'*20)\n",
    "    print('Cross Validating without scoring.')\n",
    "    cv_score_noscoring = cross_validate(\n",
    "        estimator,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv=cv,\n",
    "        scoring=None,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "\n",
    "    # Cross Validation with scoring\n",
    "    print('-'*20)\n",
    "    print(f'Cross Validating with {scores} scoring.')\n",
    "    cv_score_recall = cross_validate(\n",
    "        estimator,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv=cv,\n",
    "        scoring=scores,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "\n",
    "    # Get mean and std of cross validation scores\n",
    "    print('-'*20)\n",
    "    print('Getting mean and std of cross validation scores.')\n",
    "    cv_train_scores = cv_score_noscoring['train_score'].mean()\n",
    "    cv_test_scores = cv_score_noscoring['test_score'].mean()\n",
    "    cv_train_recall = cv_score_recall['train_recall'].mean()\n",
    "    cv_test_recall = cv_score_recall['test_recall'].mean()\n",
    "    cv_train_explained_variance_recall = cv_score_recall['train_explained_variance'].mean()\n",
    "    cv_test_explained_variance_recall = cv_score_recall['test_explained_variance'].mean()\n",
    "\n",
    "    # Save cross validation scores to dataframe\n",
    "    print('-'*20)\n",
    "    print('Saving cross validation scores to dataframe.')\n",
    "    df_cv_score_noscoring = pd.DataFrame(cv_score_noscoring)\n",
    "    df_cv_score_noscoring.to_pickle(f'{df_save_dir}df_cv_score_noscoring - {col}_{vectorizer_name}_{classifier_name}.pkl')\n",
    "    df_cv_score_recall = pd.DataFrame(cv_score_recall)\n",
    "    df_cv_score_recall.to_pickle(f'{df_save_dir}df_cv_score_recall - {col}_{vectorizer_name}_{classifier_name}.pkl')\n",
    "\n",
    "    return (\n",
    "        df_cv_score_recall,\n",
    "        cv_train_scores, cv_test_scores,\n",
    "        cv_train_recall, cv_test_recall,\n",
    "        cv_train_explained_variance_recall, cv_test_explained_variance_recall\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e84026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_with_estimator(\n",
    "    estimator, X_test, y_test, col, vectorizer_name, classifier_name,\n",
    "    param_name=None, param_range=None, axis=None, cv=None, n_jobs=None, random_state=None, alpha=None, verbose=None\n",
    "):\n",
    "    if axis is None:\n",
    "        axis = 1\n",
    "    if cv is None:\n",
    "        cv = cv\n",
    "    if n_jobs is None:\n",
    "        n_jobs = n_jobs\n",
    "    if random_state is None:\n",
    "        random_state = random_state\n",
    "    if alpha is None:\n",
    "        alpha = 0.1\n",
    "    if verbose is None:\n",
    "        verbose=1\n",
    "\n",
    "    # Make param names and values\n",
    "    if param_range is None:\n",
    "        param_range = np.arange(1, 100, 10)\n",
    "    if param_name is None:\n",
    "        param_names = [\n",
    "            param_name\n",
    "            for param_dict in estimator.steps\n",
    "            for param_name in \n",
    "            [\n",
    "                name\n",
    "                for name, value in param_dict[1].get_params().items()\n",
    "                if name != 'random_state'\n",
    "                and isinstance(value, (list, int, float))\n",
    "                and not isinstance(value, bool)\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "    # Using estimator\n",
    "    # Learning Curves\n",
    "    print('Plotting Learning Curve.')\n",
    "    print('-'*20)\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator=estimator,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        random_state=random_state,\n",
    "        shuffle=True,\n",
    "        scoring=scoring,\n",
    "        verbose=verbose,\n",
    "        # train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=axis)\n",
    "    train_scores_std = np.std(train_scores, axis=axis)\n",
    "    test_scores_mean = np.mean(test_scores, axis=axis)\n",
    "    test_scores_std = np.std(test_scores, axis=axis)\n",
    "\n",
    "    close_plots()\n",
    "    plt.figure()\n",
    "    plt.suptitle(\n",
    "        f'{str(col)} - Learning Curves for {scoring.title()} - {vectorizer_name} + {classifier_name}'\n",
    "        )\n",
    "    plt.xlabel('Training examples')\n",
    "    plt.ylabel('Score')\n",
    "    plt.grid()\n",
    "    plt.fill_between(\n",
    "        train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=alpha, color='r'\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=alpha, color='g'\n",
    "    )\n",
    "    plt.plot(\n",
    "        train_sizes, train_scores_mean, 'o-', color='r', label='Training score'\n",
    "    )\n",
    "    plt.plot(\n",
    "        train_sizes, test_scores_mean, 'o-', color='g', label='Cross-validation score'\n",
    "    )\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    # Save figure\n",
    "    for image_save_format in ['eps', 'png']:\n",
    "        plt.savefig(f'{plot_save_path}{method} {str(col)} - Learning Curve - {vectorizer_name} + {classifier_name}.{image_save_format}', format=image_save_format)\n",
    "    show_and_close_plots()\n",
    "\n",
    "    # # Validation Curve\n",
    "    # for param_name in param_names:\n",
    "    #     if param_name:\n",
    "    #         param_title = ' '.join(param_name.split('_')).title()\n",
    "    #         print(f'Plotting Validation Curve for {param_title}.')\n",
    "    #         print('-'*20)\n",
    "    #         train_scores, test_scores = validation_curve(\n",
    "    #             estimator=estimator,\n",
    "    #             X=X_train,\n",
    "    #             y=y_train,\n",
    "    #             param_name=param_name,\n",
    "    #             param_range=param_range,\n",
    "    #             cv=cv,\n",
    "    #             n_jobs=n_jobs,\n",
    "    #             scoring=scoring,\n",
    "    #             verbose=verbose,\n",
    "    #         )\n",
    "    #         train_scores_mean = np.mean(train_scores, axis=axis)\n",
    "    #         train_scores_std = np.std(train_scores, axis=axis)\n",
    "    #         test_scores_mean = np.mean(test_scores, axis=axis)\n",
    "    #         test_scores_std = np.std(test_scores, axis=axis)\n",
    "\n",
    "    #         # Ploting\n",
    "    #         plt.figure()\n",
    "    #         plt.suptitle(\n",
    "    #             f'{str(col)} - Validation Curve for {scoring.title()}on {param_title} - {vectorizer_name} + {classifier_name}'\n",
    "    #         )\n",
    "    #         plt.xlabel(param_name)\n",
    "    #         plt.ylabel('Score')\n",
    "    #         plt.grid()\n",
    "    #         plt.fill_between(\n",
    "    #             param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=alpha, color='r'\n",
    "    #         )\n",
    "    #         plt.fill_between(\n",
    "    #             param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=alpha, color='g'\n",
    "    #         )\n",
    "    #         plt.plot(\n",
    "    #             param_range, train_scores_mean, 'o-', color='r', label='Training score'\n",
    "    #         )\n",
    "    #         plt.plot(\n",
    "    #             param_range, test_scores_mean, 'o-', color='g', label='Cross-validation score'\n",
    "    #         )\n",
    "    #         plt.legend(loc='best')\n",
    "    #         plt.show()\n",
    "\n",
    "    #         # Save figure\n",
    "    #         for image_save_format in ['eps', 'png']:\n",
    "    #             plt.savefig(\n",
    "    #                 f'{plot_save_path}{method} {str(col)} - Validation Curve for {param_title} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "    #                 format=image_save_format\n",
    "    #             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7a0330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_y_pred(\n",
    "    y_test, y_test_pred, col, vectorizer_name, classifier_name,\n",
    "    pos_label=None, labels=None\n",
    "):\n",
    "\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    if labels is None:\n",
    "        labels = [1, 0]\n",
    "\n",
    "    # Using y_pred\n",
    "    explained_variance = metrics.explained_variance_score(y_test, y_test_pred)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "    balanced_accuracy = metrics.balanced_accuracy_score(y_test, y_test_pred)\n",
    "    precision = metrics.precision_score(y_test, y_test_pred, pos_label=1, labels=[1, 0])\n",
    "    recall = metrics.recall_score(y_test, y_test_pred, pos_label=1, labels=[1, 0])\n",
    "    f1 = metrics.f1_score(y_test, y_test_pred)\n",
    "    mcc = metrics.matthews_corrcoef(y_test, y_test_pred)\n",
    "    fm = metrics.fowlkes_mallows_score(y_test, y_test_pred)\n",
    "    kappa = metrics.cohen_kappa_score(y_test, y_test_pred)\n",
    "    gmean_iba = imblearn.metrics.make_index_balanced_accuracy(alpha=0.1, squared=True)(geometric_mean_score)\n",
    "    gmean = gmean_iba(y_test, y_test_pred)\n",
    "    report = metrics.classification_report(y_test, y_test_pred)\n",
    "    cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "    cm_normalized = metrics.confusion_matrix(y_test, y_test_pred, normalize='true')\n",
    "\n",
    "    return (\n",
    "        explained_variance, accuracy, balanced_accuracy, precision,\n",
    "        recall, f1, mcc, fm, kappa, gmean, report, cm, cm_normalized\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c21c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_label = 1\n",
    "# close_plots()\n",
    "# cm_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "#     y_test, y_test_pred\n",
    "# )\n",
    "# cm_normalized_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "#     y_test, y_test_pred, normalize='true'\n",
    "# )\n",
    "# roc_curve = metrics.RocCurveDisplay.from_predictions(\n",
    "#     y_test, y_test_pred, pos_label=pos_label\n",
    "# )\n",
    "# pr_curve = metrics.PrecisionRecallDisplay.from_predictions(\n",
    "#     y_test, y_test_pred, pos_label=pos_label\n",
    "# )\n",
    "# calibration_curve = CalibrationDisplay.from_predictions(\n",
    "#     y_test, y_test_pred, pos_label=pos_label\n",
    "# )\n",
    "# show_and_close_plots()\n",
    "\n",
    "# # Plots\n",
    "# plots_dict = {\n",
    "#     'Confusion Matrix': cm_curve,\n",
    "#     'Normalized Confusion Matrix': cm_normalized_curve,\n",
    "#     'ROC Curve': roc_curve,\n",
    "#     'Precision-Recall Curve': pr_curve,\n",
    "#     'Calibration Curve': calibration_curve,\n",
    "# }\n",
    "\n",
    "# print('=' * 20)\n",
    "# # close_plots()\n",
    "# print('Plotting metrics with y_pred_prob:')\n",
    "# print('='*20)\n",
    "# for plot_name, plot_ in plots_dict.items():\n",
    "#     close_plots()\n",
    "#     print(f'Plotting {plot_name}:')\n",
    "#     plt.figure()\n",
    "#     plt.suptitle(\n",
    "#         f'{str(col)} - {plot_name} - {vectorizer_name} + {classifier_name}'\n",
    "#         )\n",
    "#     if plot_name == 'ROC Curve':\n",
    "#         plt.plot([0, 1], [0, 1], 'r--', lw=1)\n",
    "#     try:\n",
    "#         plot_.plot(color='C0')\n",
    "#     except (TypeError, AttributeError):\n",
    "#         plot_.plot()\n",
    "#         try:\n",
    "#             plt.gca().get_lines()[0].set_color('blue')\n",
    "#         except IndexError:\n",
    "#             plot_.plot(cmap=plt.cm.Blues)\n",
    "#     plt.legend(loc='best')\n",
    "#     print('=' * 20)\n",
    "\n",
    "#     # Save Plots\n",
    "#     print(f'Saving {plot_name}...')\n",
    "#     for image_save_format in ['eps', 'png']:\n",
    "#         plt.savefig(\n",
    "#             f'{plot_save_path}{method} {str(col)} - {plot_name} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "#             format=image_save_format, dpi=3000, bbox_inches='tight'\n",
    "#         )\n",
    "#     show_and_close_plots()\n",
    "#     print(f'Saved {plot_name}!')\n",
    "#     print('=' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89a7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_with_y_pred(\n",
    "    y_test, y_test_pred, col, vectorizer_name, classifier_name,\n",
    "    pos_label=None\n",
    "):\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    # Using y_pred_prob\n",
    "    # Displays\n",
    "    close_plots()\n",
    "    cm_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, y_test_pred\n",
    "    )\n",
    "    cm_normalized_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, y_test_pred, normalize='true'\n",
    "    )\n",
    "    roc_curve = metrics.RocCurveDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=pos_label\n",
    "    )\n",
    "    pr_curve = metrics.PrecisionRecallDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=pos_label\n",
    "    )\n",
    "    calibration_curve = CalibrationDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=pos_label\n",
    "    )\n",
    "    show_and_close_plots()\n",
    "\n",
    "    # Plots\n",
    "    plots_dict = {\n",
    "        'Confusion Matrix': cm_curve,\n",
    "        'Normalized Confusion Matrix': cm_normalized_curve,\n",
    "        'ROC Curve': roc_curve,\n",
    "        'Precision-Recall Curve': pr_curve,\n",
    "        'Calibration Curve': calibration_curve,\n",
    "    }\n",
    "\n",
    "    print('=' * 20)\n",
    "    # close_plots()\n",
    "    print('Plotting metrics with y_pred_prob:')\n",
    "    print('='*20)\n",
    "\n",
    "    for plot_name, plot_ in plots_dict.items():\n",
    "        close_plots()\n",
    "        print(f'Plotting {plot_name}:')\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(\n",
    "            f'{str(col)} - {plot_name} - {vectorizer_name} + {classifier_name}'\n",
    "            )\n",
    "        if plot_name == 'ROC Curve':\n",
    "            ax.plot([0, 1], [0, 1], 'r--', lw=1)\n",
    "        plot_.plot(ax=ax)\n",
    "        print('=' * 20)\n",
    "\n",
    "        # Save Plots\n",
    "        print(f'Saving {plot_name}...')\n",
    "        for image_save_format in ['eps', 'png']:\n",
    "            plt.savefig(\n",
    "                f'{plot_save_path}{method} {str(col)} - {plot_name} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "                format=image_save_format, dpi=3000, bbox_inches='tight'\n",
    "            )\n",
    "        show_and_close_plots()\n",
    "        print(f'Saved {plot_name}!')\n",
    "        print('=' * 20)\n",
    "\n",
    "    # Visualisation with plot_metric\n",
    "    bc = plot_metric.functions.BinaryClassification(\n",
    "        y_test, y_test_pred, labels=[0, 1])\n",
    "\n",
    "    # Figures\n",
    "    close_plots()\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    plt.subplot2grid((2, 6), (1, 1), colspan=2)\n",
    "    bc.plot_confusion_matrix(colorbar=True)\n",
    "    plt.subplot2grid((2, 6), (1, 3), colspan=2)\n",
    "    bc.plot_confusion_matrix(normalize=True, colorbar=True)\n",
    "    plt.subplot2grid(shape=(2, 6), loc=(0, 0), colspan=2)\n",
    "    bc.plot_roc_curve()\n",
    "    plt.subplot2grid((2, 6), (0, 2), colspan=2)\n",
    "    bc.plot_precision_recall_curve()\n",
    "    plt.subplot2grid((2, 6), (0, 4), colspan=2)\n",
    "    bc.plot_class_distribution()\n",
    "    bc.print_report()\n",
    "    for image_save_format in ['eps', 'png']:\n",
    "        plt.savefig(\n",
    "            f'{plot_save_path}{method} {str(col)} - plot_metric Curves - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "    show_and_close_plots()\n",
    "\n",
    "    # Heatmap\n",
    "    print('Plotting Heatmap:')\n",
    "    close_plots()\n",
    "    classifications_dict = defaultdict(int)\n",
    "    for _y_test, _y_test_pred in zip(y_test, y_test_pred):\n",
    "        if _y_test != _y_test_pred:\n",
    "            classifications_dict[(_y_test, _y_test_pred)] += 1\n",
    "\n",
    "    dicts_to_plot = [\n",
    "        {\n",
    "            f'True {str(col)} value': _y_test,\n",
    "            f'Predicted {str(col)} value': _y_test_pred,\n",
    "            'Number of Classifications': _count,\n",
    "        }\n",
    "        for (_y_test, _y_test_pred), _count in classifications_dict.items()\n",
    "    ]\n",
    "    df_to_plot = pd.DataFrame(dicts_to_plot)\n",
    "    df_wide = df_to_plot.pivot_table(\n",
    "        index=f'True {str(col)} value', \n",
    "        columns=f'Predicted {str(col)} value', \n",
    "        values='Number of Classifications'\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(9,7))\n",
    "    sns.set(style='ticks', font_scale=1.2)\n",
    "    sns.heatmap(df_wide, linewidths=1, cmap=plt.cm.Blues, annot=True)    \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'{str(col)} Heatmap - {vectorizer_name} + {classifier_name}')\n",
    "    print('Saving Heatmap...')\n",
    "    for image_save_format in ['eps', 'png']:\n",
    "        plt.savefig(\n",
    "            f'{plot_save_path}{method} {str(col)} - Heatmap - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "    print('Saved Heatmap!')\n",
    "    show_and_close_plots()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de7dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_y_pred_prob(\n",
    "    y_test, y_test_pred_prob, col, vectorizer_name, classifier_name,\n",
    "    pos_label=None\n",
    "):\n",
    "\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "\n",
    "    # Using y_pred_prob\n",
    "    average_precision = metrics.average_precision_score(y_test, y_test_pred_prob)\n",
    "    roc_auc = metrics.roc_auc_score(y_test, y_test_pred_prob)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, y_test_pred_prob, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    loss = metrics.log_loss(y_test, y_test_pred_prob)\n",
    "    precision_pr, recall_pr, threshold_pr = metrics.precision_recall_curve(y_test, y_test_pred_prob, pos_label=1)\n",
    "\n",
    "    return (\n",
    "        average_precision, roc_auc, auc,\n",
    "        fpr, tpr, threshold,loss,\n",
    "        precision_pr, recall_pr, threshold_pr\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162b0699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "    col, vectorizer_name, classifier_name,\n",
    "    with_estimator=None, with_y_pred=None, with_y_pred_prob=None,\n",
    "    cv=None, pos_label=None, verbose=None, n_jobs=None,\n",
    "):\n",
    "\n",
    "    if cv is None:\n",
    "        cv = cv\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    if verbose is None:\n",
    "        verbose = 1\n",
    "    if n_jobs is None:\n",
    "        n_jobs = n_jobs\n",
    "\n",
    "    if with_estimator is None:\n",
    "        with_estimator = True\n",
    "    if with_y_pred is None:\n",
    "        with_y_pred = True\n",
    "    if with_y_pred_prob is None:\n",
    "        with_y_pred_prob = True\n",
    "\n",
    "    # Get metrics\n",
    "    print('='*20)\n",
    "    # Using estimator\n",
    "    if with_estimator:\n",
    "        print('Computing metrics using estimator.')\n",
    "        (\n",
    "            df_cv_score_recall,\n",
    "            cv_train_scores, cv_test_scores,\n",
    "            cv_train_recall, cv_test_recall,\n",
    "            cv_train_explained_variance_recall, cv_test_explained_variance_recall\n",
    "        ) = compute_metrics_using_estimator(\n",
    "             estimator, X_test, y_test, col, vectorizer_name, classifier_name,\n",
    "        )\n",
    "    # Using y_test_pred\n",
    "    if with_y_pred:\n",
    "        print('-'*20)\n",
    "        print('Computing metrics using y_test_pred.')\n",
    "        (\n",
    "            explained_variance, accuracy, balanced_accuracy, precision,\n",
    "            recall, f1, mcc, fm, kappa, gmean, report, cm, cm_normalized\n",
    "        ) = compute_metrics_with_y_pred(\n",
    "            y_test, y_test_pred, col, vectorizer_name, classifier_name\n",
    "        )\n",
    "    # Using y_test_pred_prob\n",
    "    if with_y_pred_prob:\n",
    "        print('-'*20)\n",
    "        print('Computing metrics using y_test_pred_prob.')\n",
    "        (\n",
    "            average_precision, roc_auc, auc,\n",
    "            fpr, tpr, threshold,loss,\n",
    "            precision_pr, recall_pr, threshold_pr\n",
    "        ) = compute_metrics_with_y_pred_prob(\n",
    "            y_test, y_test_pred_prob, col, vectorizer_name, classifier_name,\n",
    "        )\n",
    "\n",
    "    #Place metrics into dict\n",
    "    print('-'*20)\n",
    "    print('Appending metrics to dict.')\n",
    "    metrics_dict = {\n",
    "        'Mean Cross Validation Train Score': float(cv_train_scores),\n",
    "        'Mean Cross Validation Test Score': float(cv_test_scores),\n",
    "        f'Mean Cross Validation Train - {scoring.title()}': float(cv_train_recall),\n",
    "        f'Mean Cross Validation Test - {scoring.title()}': float(cv_test_recall),\n",
    "        f'Mean Explained Train Variance - {scoring.title()}': float(cv_train_explained_variance_recall),\n",
    "        f'Mean Explained Test Variance - {scoring.title()}': float(cv_test_explained_variance_recall),\n",
    "        'Explained Variance': float(explained_variance),\n",
    "        'Accuracy': float(accuracy),\n",
    "        'Balanced Accuracy': float(balanced_accuracy),\n",
    "        'Precision': float(precision),\n",
    "        'Average Precision': float(average_precision),\n",
    "        'Recall': float(recall),\n",
    "        'F1-score': float(f1),\n",
    "        'Matthews Correlation Coefficient': float(mcc),\n",
    "        'Fowlkes–Mallows Index': float(fm),\n",
    "        'ROC': float(roc_auc),\n",
    "        'AUC': float(auc),\n",
    "        f'{scoring.title()} Best Threshold': threshold,\n",
    "        f'{scoring.title()} Best Score': float(best_score),\n",
    "        'Log Loss/Cross Entropy': float(loss),\n",
    "        'Cohen’s Kappa': float(kappa),\n",
    "        'Geometric Mean': float(gmean),\n",
    "        'Classification Report': report,\n",
    "        'Confusion Matrix': cm,\n",
    "        'Normalized Confusion Matrix': cm_normalized\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        metrics_dict, df_cv_score_recall,\n",
    "        cv_train_scores, cv_test_scores,\n",
    "        cv_train_recall, cv_test_recall,\n",
    "        cv_train_explained_variance_recall, cv_test_explained_variance_recall,\n",
    "        explained_variance, accuracy, balanced_accuracy, precision, recall,\n",
    "        f1, mcc, fm, kappa, gmean, report, cm, cm_normalized,\n",
    "        average_precision, roc_auc, auc, fpr, tpr, threshold, \n",
    "        loss, precision_pr, recall_pr, threshold_pr,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fdf3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(\n",
    "    estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "    col, vectorizer_name, classifier_name, \n",
    "    with_estimator=None, with_y_pred=None, with_y_pred_prob=None\n",
    "):\n",
    "    if with_estimator is None:\n",
    "        with_estimator = True\n",
    "    if with_y_pred is None:\n",
    "        with_y_pred = True\n",
    "    if with_y_pred_prob is None:\n",
    "        with_y_pred_prob = True\n",
    "\n",
    "    # Plotting\n",
    "    print('~'*20)\n",
    "    print('Plotting metrics.')\n",
    "    print('~'*20)\n",
    "    # Using estimator\n",
    "    if with_estimator:\n",
    "        plot_metrics_with_estimator(\n",
    "             estimator, X_test, y_test, col, vectorizer_name, classifier_name,\n",
    "        )\n",
    "    # Using y_test_pred\n",
    "    if with_y_pred:\n",
    "        plot_metrics_with_y_pred(\n",
    "            y_test, y_test_pred, col, vectorizer_name, classifier_name,\n",
    "        )\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8278ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(\n",
    "    estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "    best_score, df_metrics,\n",
    "    col, vectorizer_name, classifier_name, scoring=None,\n",
    "):\n",
    "    if scoring is None:\n",
    "        scoring = 'recall'\n",
    "\n",
    "    # Get metrics dict\n",
    "    (\n",
    "        metrics_dict, df_cv_score_recall,\n",
    "        cv_train_scores, cv_test_scores,\n",
    "        cv_train_recall, cv_test_recall,\n",
    "        cv_train_explained_variance_recall, cv_test_explained_variance_recall,\n",
    "        explained_variance, accuracy, balanced_accuracy, precision, recall,\n",
    "        f1, mcc, fm, kappa, gmean, report, cm, cm_normalized,\n",
    "        average_precision, roc_auc, auc, fpr, tpr, threshold, \n",
    "        loss, precision_pr, recall_pr, threshold_pr,\n",
    "    ) = compute_metrics(\n",
    "        estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "        col, vectorizer_name, classifier_name\n",
    "    )\n",
    "\n",
    "    # Print metrics\n",
    "    print('=' * 20)\n",
    "    print('~' * 20)\n",
    "    print(f' Testing Metrics for {str(col)} - {vectorizer_name} + {classifier_name}')\n",
    "    print('~' * 20)\n",
    "    print(f'Classification Report:\\n {metrics_dict[\"Classification Report\"]}')\n",
    "    print('-' * 20)\n",
    "    for metric_name, metric_value in metrics_dict.items():\n",
    "        with contextlib.suppress(TypeError, ValueError):\n",
    "            metric_value = float(metric_value)\n",
    "        if isinstance(metric_value, (int, float)):\n",
    "            print(f'{metric_name}: {round(metric_value, 2)}')\n",
    "        else:\n",
    "            print(f'{metric_name}:\\n{metric_value}')\n",
    "        print('-' * 20)\n",
    "\n",
    "        # Fill Table DF\n",
    "        if isinstance(metric_value, float):\n",
    "            df_metrics.loc[\n",
    "                (classifier_name), (col, vectorizer_name, metric_name)\n",
    "            ] = metric_value\n",
    "        else:\n",
    "            df_metrics.loc[\n",
    "                (classifier_name), (col, vectorizer_name, metric_name)\n",
    "            ] = str(metric_value)\n",
    "\n",
    "    print('=' * 20)\n",
    "\n",
    "    # Plot Metrics\n",
    "    plot_metrics(\n",
    "        estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "        col, vectorizer_name, classifier_name,\n",
    "    )\n",
    "\n",
    "    return df_metrics, metrics_dict, df_cv_score_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e46bb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to place Xy and CV data in df and save\n",
    "def save_Xy_search_cv_estimator(\n",
    "    grid_search, searchcv,\n",
    "    X_train, y_train, y_train_pred,\n",
    "    X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "    X_val, y_val,\n",
    "    estimator,\n",
    "    col, vectorizer_name, classifier_name,\n",
    "    compression=None, save_path=None\n",
    "):\n",
    "\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "    if save_path is None:\n",
    "        save_path = f'{final_models_save_path}SearchCV/'\n",
    "\n",
    "    # Save search\n",
    "    ## Save grid_search\n",
    "    with open(\n",
    "        f'{save_path}{method} Grid Search {str(col)} - {vectorizer_name} + {classifier_name}.pkl', 'wb'\n",
    "    ) as f:\n",
    "        joblib.dump(grid_search, f, compress=compression)\n",
    "\n",
    "    ## Save searchcv\n",
    "    with open(\n",
    "        f'{save_path}{method} SearchCV {str(col)} - {vectorizer_name} + {classifier_name}.pkl', 'wb'\n",
    "    ) as f:\n",
    "        joblib.dump(searchcv, f, compress=compression)\n",
    "\n",
    "    ## Save searchcv data\n",
    "    df_cv_results = pd.DataFrame(searchcv.cv_results_)\n",
    "    df_cv_results.to_pickle(\n",
    "        f'{save_path}{method} df_searchcv_results - {col}_{vectorizer_name}_{classifier_name}.pkl'\n",
    "    )\n",
    "\n",
    "    # Save Xy data\n",
    "    ## Save train data\n",
    "    df_train_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'y_train_pred': y_train_pred,\n",
    "        },\n",
    "    )\n",
    "    df_train_data.to_pickle(\n",
    "        f'{save_path}{method} df_train_data - {col}_{vectorizer_name}_{classifier_name}.pkl'\n",
    "    )\n",
    "\n",
    "    ## Save test data\n",
    "    df_test_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'y_test_pred': y_test_pred,\n",
    "            'y_test_pred_prob': y_test_pred_prob,\n",
    "        },\n",
    "    )\n",
    "    df_test_data.to_pickle(\n",
    "        f'{save_path}{method} df_test_data - {col}_{vectorizer_name}_{classifier_name}.pkl'\n",
    "    )\n",
    "\n",
    "    ## Save val data\n",
    "    df_val_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_val': X_val,\n",
    "            'y_val': y_val,\n",
    "        },\n",
    "    )\n",
    "    df_val_data.to_pickle(\n",
    "        f'{save_path}{method} df_val_data - {col}_{vectorizer_name}_{classifier_name}.pkl'\n",
    "    )\n",
    "\n",
    "    # Save estimator\n",
    "    with open(\n",
    "        f'{save_path}{method} Estimator {str(col)} - {vectorizer_name} + {classifier_name}.pkl', 'wb'\n",
    "    ) as f:\n",
    "        joblib.dump(estimator, f, compress=compression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4495381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "def saving_model_and_table(df_metrics, estimator, col, vectorizer_name, classifier_name):\n",
    "\n",
    "    # Save metrics df\n",
    "    print(f'Saving Model and Table for {vectorizer_name} + {classifier_name}.')\n",
    "    df_metrics.to_csv(f'{table_save_path}Classifiers Table.csv')\n",
    "    df_metrics.to_pickle(f'{table_save_path}Classifiers Table.pkl')\n",
    "    df_metrics.to_excel(f'{table_save_path}Classifiers Table.xlsx')\n",
    "    df_metrics.to_latex(f'{table_save_path}Classifiers Table.tex')\n",
    "    df_metrics.to_markdown(f'{table_save_path}Classifiers Table.md')\n",
    "\n",
    "    # Save estimator\n",
    "    with open(f'{final_models_save_path}{method} Estimator {str(col)} - {vectorizer_name} + {classifier_name}.pkl', 'wb') as f:\n",
    "        joblib.dump(estimator, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc524e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "def get_fitted_estimators():\n",
    "    \n",
    "    estimators_list = []\n",
    "\n",
    "    for model_path in glob.glob(f'{final_models_save_path}*.pkl'):\n",
    "        with open(model_path, 'rb') as f:\n",
    "            estimators_list.append(joblib.load(f))\n",
    "\n",
    "    return estimators_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44148bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_plots(estimators_list, X_test, y_test, col, curves_dict=None, cmap=plt.cm.Blues):\n",
    "\n",
    "    curves_dict = {\n",
    "        'ROC Curve': metrics.RocCurveDisplay,\n",
    "        'Precision Recall Curve': metrics.PrecisionRecallDisplay,\n",
    "        'Calibration Curve': metrics.CalibrationDisplay,\n",
    "    }\n",
    "\n",
    "    assert len(estimators_list) != 0\n",
    "\n",
    "    for curve_name, curve_package in curves_dict.items():\n",
    "        print('-' * 20)\n",
    "        print(f'{str(col)} - {str(curve_name)}')\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(f'{str(col)} - {str(curve_name)}')\n",
    "        for estimator in estimators_list:\n",
    "            curve = curve_package.from_estimator(\n",
    "                estimator, X_test, y_test, pos_label=1, ax=ax, cmap=cmap,\n",
    "                name=f'{estimator.steps[0][0]} + {estimator.steps[1][0]} + {estimator.steps[-1][0]}'\n",
    "            )\n",
    "        show_and_close_plots()\n",
    "\n",
    "        # Save Plots\n",
    "        print('Saving plots.')\n",
    "        for image_save_format in ['eps', 'png']:\n",
    "            curve.figure_.savefig(\n",
    "                f'{plot_save_path}{method} {str(col)} - All {str(curve_name)}s.{image_save_format}',\n",
    "                format=image_save_format,\n",
    "                dpi=3000, bbox_inches='tight'\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036fcf10",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe8e7a",
   "metadata": {},
   "source": [
    "### READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fbe29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_trainning.pkl').reset_index(drop=True)\n",
    "# TODO REMOVE THIS!!!!!!\n",
    "df_manual = df_manual.groupby(analysis_columns).sample(n=50).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0212dc9",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print('#'*40)\n",
    "print('Starting!')\n",
    "print('#'*40)\n",
    "\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "with joblib.parallel_backend(backend='multiprocessing', n_jobs=n_jobs):\n",
    "    # Load Table DF\n",
    "    # df_metrics = make_df_metrics(vectorizers_pipe, classifiers_pipe, list(metrics_dict.keys()), analysis_columns)\n",
    "\n",
    "    for col in tqdm.tqdm(analysis_columns):\n",
    "\n",
    "        print('-'*20)\n",
    "        print(f'{\"=\"*30} TRAINING {col.upper()} {\"=\"*30}')\n",
    "        print('-'*20)\n",
    "        print(f'Vectorizers to be used ({len(list(vectorizers_pipe.values()))}):\\n{list(vectorizers_pipe.keys())}')\n",
    "        print(f'Total number of vectorizer parameters = {sum([len(list(vectorizers_pipe.values())[i][1]) for i in range(len(vectorizers_pipe))])}')\n",
    "        print(f'Selectors to be used ({len(list(selectors_pipe.values()))}):\\n{list(selectors_pipe.keys())}')\n",
    "        print(f'Total number of selector parameters = {sum([len(list(selectors_pipe.values())[i][1]) for i in range(len(selectors_pipe))])}')\n",
    "        print(f'Resamplers to be used ({len(list(resamplers_pipe.keys()))}):\\n{list(resamplers_pipe.keys())}')\n",
    "        print(f'Total number of resamplers parameters = {sum([len(list(resamplers_pipe.values())[i][1]) for i in range(len(resamplers_pipe))])}')\n",
    "        print(f'Classifers to be used ({len(list(classifiers_pipe.keys()))}):\\n{list(classifiers_pipe.keys())}')\n",
    "        print(f'Total number of classifers parameters = {sum([len(list(classifiers_pipe.values())[i][1]) for i in range(len(classifiers_pipe))])}')\n",
    "        \n",
    "\n",
    "        assert len(df_manual[df_manual[str(col)].map(df_manual[str(col)].value_counts() > 1)]) != 0\n",
    "\n",
    "        # Split\n",
    "        (\n",
    "            train, X_train, y_train,\n",
    "            test, X_test, y_test,\n",
    "            val, X_val, y_val,\n",
    "            class_weights,\n",
    "            class_weights_ratio,\n",
    "            class_weights_dict\n",
    "        ) = split_data(\n",
    "            df_manual, col, text_col, analysis_columns,\n",
    "        )\n",
    "\n",
    "        for (\n",
    "            vectorizer_name, vectorizer_and_params\n",
    "        ), (\n",
    "            selector_name, selector_and_params\n",
    "        ), (\n",
    "            resampler_name, resampler_and_params\n",
    "        ), (\n",
    "            classifier_name, classifier_and_params\n",
    "        ) in tqdm_product(\n",
    "            vectorizers_pipe.items(), selectors_pipe.items(), resamplers_pipe.items(), classifiers_pipe.items()\n",
    "        ):\n",
    "\n",
    "            vectorizer = vectorizer_and_params[0]\n",
    "            vectorizer_params = vectorizer_and_params[1]\n",
    "\n",
    "            selector = selector_and_params[0]\n",
    "            selector_params = selector_and_params[1]\n",
    "\n",
    "            resampler = resampler_and_params[0]\n",
    "            resampler_params = resampler_and_params[1]\n",
    "\n",
    "            classifier = classifier_and_params[0]\n",
    "            classifier_params = classifier_and_params[1]\n",
    "\n",
    "            # Pipeline\n",
    "            ## Steps\n",
    "            if col == 'Warmth':\n",
    "                steps = [\n",
    "                    (vectorizer_name, vectorizer),\n",
    "                    (selector_name, selector),\n",
    "                    (resampler_name, resampler),\n",
    "                    (classifier_name, classifier)\n",
    "                ]\n",
    "            else:\n",
    "                steps = [\n",
    "                    (vectorizer_name, vectorizer),\n",
    "                    (selector_name, selector),\n",
    "                    (classifier_name, classifier)\n",
    "                ]\n",
    "\n",
    "            ## Params\n",
    "            param_grid = {\n",
    "                **vectorizer_params,\n",
    "                **selector_params,\n",
    "                **classifier_params,\n",
    "            }\n",
    "\n",
    "            ## Pipeline\n",
    "            pipe = imblearn.pipeline.Pipeline(steps=steps)\n",
    "\n",
    "            # Search\n",
    "            print('-'*20)\n",
    "            print(f'{\"=\"*30} Using GridSearchCV {\"=\"*30}')\n",
    "            print('-'*20)\n",
    "            print(f'GridSearchCV with:\\nPipe:\\n{pipe}\\nParams:\\n{param_grid}')\n",
    "            print('+'*30)\n",
    "\n",
    "        # with joblib.parallel_backend(backend='multiprocessing', n_jobs=n_jobs):\n",
    "            grid_search = HalvingGridSearchCV(\n",
    "                estimator=pipe,\n",
    "                param_grid=param_grid,\n",
    "                cv=cv,\n",
    "                n_jobs=n_jobs,\n",
    "                return_train_score=True,\n",
    "                verbose=1,\n",
    "                random_state=random_state,\n",
    "                refit=refit,\n",
    "                scoring=scoring,\n",
    "            )\n",
    "            # grid_search = GridSearchCV(\n",
    "            #     estimator=pipe,\n",
    "            #     param_grid=param_grid,\n",
    "            #     cv=cv,\n",
    "            #     n_jobs=n_jobs,\n",
    "            #     return_train_score=True,\n",
    "            #     verbose=1,\n",
    "            #     # scoring=scores,\n",
    "            #     # refit=scoring,\n",
    "            # )\n",
    "\n",
    "            # Fit SearchCV\n",
    "            searchcv = grid_search.fit(X_train, y_train)\n",
    "\n",
    "            # # Save SearchCV\n",
    "            # with open(f'{final_models_save_path}{method} SearchCV {str(col)} - {vectorizer_name} + {classifier_name}.pkl', 'wb') as f:\n",
    "            #     joblib.dump(searchcv, f)\n",
    "\n",
    "            # HACK\n",
    "            # Identify and name steps in estimator\n",
    "            # estimator = searchcv.best_estimator_\n",
    "            # vectorizer = searchcv.estimator[0]\n",
    "            # vectorizer_name = vectorizer.__class__.__name__\n",
    "            # selector = searchcv.estimator[1]\n",
    "            # selector_name = selector.__class__.__name__\n",
    "            # classifier = searchcv.estimator[-1]\n",
    "            # classifier_name = classifier.__class__.__name__\n",
    "            # if col == 'Warmth':\n",
    "            #     resampler = searchcv.estimator[-2]\n",
    "            #     resampler_name = resampler.__class__.__name__\n",
    "            # Identify and name steps in estimator\n",
    "            # estimator = searchcv.best_estimator_\n",
    "            vectorizer = searchcv.best_estimator_[0]\n",
    "            vectorizer_name = vectorizer.__class__.__name__\n",
    "            selector = searchcv.best_estimator_[1]\n",
    "            selector_name = selector.__class__.__name__\n",
    "            classifier = searchcv.best_estimator_[-1]\n",
    "            classifier_name = classifier.__class__.__name__\n",
    "            if col == 'Warmth':\n",
    "                resampler = searchcv.best_estimator_[-2]\n",
    "                resampler_name = resampler.__class__.__name__\n",
    "\n",
    "            # Best Parameters on CV\n",
    "            # best_index = searchcv.best_index_\n",
    "            # n_splits = searchcv.n_splits_\n",
    "            # estimator = searchcv.best_estimator_\n",
    "            # best_params = searchcv.best_params_\n",
    "            # best_train_score = searchcv.best_score_\n",
    "            # best_test_score = searchcv.score(X_test, y_test)\n",
    "            # train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "            # Make predictions\n",
    "            # estimator = searchcv.best_estimator_\n",
    "            y_train_pred = (estimator:=searchcv.best_estimator_).predict(X_train)\n",
    "            if hasattr(searchcv, 'predict_proba'):\n",
    "                searchcv_predict_attr = searchcv.predict_proba\n",
    "            elif hasattr(searchcv, '_predict_proba_lr'):\n",
    "                searchcv_predict_attr = searchcv._predict_proba_lr\n",
    "            y_test_pred = searchcv.predict(X_test)\n",
    "            y_test_pred_prob = searchcv_predict_attr(X_test)[:, 1]\n",
    "\n",
    "            print('='*20)\n",
    "            print(\n",
    "                f'GridSearch - Best mean train score: M = {float(best_mean_train_score:=searchcv.cv_results_[\"mean_train_score\"][best_index:=searchcv.best_index_]):.2f}, SD = {int(best_std_train_score:=searchcv.cv_results_[\"std_train_score\"][best_index]):.2f}\\n'\n",
    "            )\n",
    "            print(\n",
    "                f'GridSearch - Best mean test score: M = {float(best_mean_test_score:=searchcv.cv_results_[\"mean_test_score\"][best_index]):.2f}, SD = {int(best_std_test_score:=searchcv.cv_results_[\"std_test_score\"][best_index]):.2f}\\n'\n",
    "            )\n",
    "            print(\n",
    "                f'Number of splits: {int(n_splits:=searchcv.n_splits_)}\\n'\n",
    "            )\n",
    "            print(\n",
    "                f'Best estimator and parameters:\\n{estimator}\\n')\n",
    "            print(\n",
    "                f'Best parameters:\\n{(best_params:=searchcv.best_params_)}\\n'\n",
    "            )\n",
    "            print(\n",
    "                f'Training Classification Report:\\n{(train_report:=classification_report(y_train, y_train_pred))}\\n'\n",
    "            )\n",
    "            print('-'*20)\n",
    "            print(\n",
    "                f'Best train score: {float(best_train_score:=searchcv.best_score_):.2f}\\n'\n",
    "            )\n",
    "            print(\n",
    "                f'Best test score: {float(best_test_score:=searchcv.score(X_test, y_test)):.2f}\\n'\n",
    "            )\n",
    "            print('-'*20)\n",
    "            print('Training Confusion Matrix:\\n')\n",
    "            close_plots()\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.set_title(f'{str(col)} - Training Confusion Matrix - {vectorizer_name} + {classifier_name}')\n",
    "            train_cm = metrics.ConfusionMatrixDisplay.from_estimator(\n",
    "                estimator, X_train, y_train, ax=ax, cmap=plt.cm.Blues\n",
    "            )\n",
    "            show_and_close_plots()\n",
    "            print('='*20)\n",
    "\n",
    "            # Place Xy and CV data in df and save\n",
    "            save_Xy_search_cv_estimator(\n",
    "                grid_search, searchcv,\n",
    "                X_train, y_train, y_train_pred,\n",
    "                X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "                X_val, y_val,\n",
    "                estimator,\n",
    "                col, vectorizer_name, classifier_name,\n",
    "            )\n",
    "\n",
    "            # HACK REMOVE THIS!!!!!!\n",
    "            sys.exit(0)\n",
    "            # Evaluate Model\n",
    "            df_metrics, metrics_dict, df_cv_score_recall = evaluation(\n",
    "                estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "                best_score, df_metrics,\n",
    "                col, vectorizer_name, classifier_name, \n",
    "            )\n",
    "\n",
    "            # Fit best model on validation set\n",
    "            print(f'Fitting {estimator}.')\n",
    "            estimator.set_params(**estimator.get_params())\n",
    "            estimator = estimator.fit(X_val, y_val)\n",
    "\n",
    "            # Save Vectorizer, Selector, and Classifier\n",
    "            saving_model_and_table(df_metrics, estimator, col, vectorizer_name, classifier_name)\n",
    "\n",
    "        # Compare Estimators\n",
    "        print('='*20)\n",
    "        print(f'Comparing Estimators for {col}')\n",
    "        comparison_plots(get_fitted_estimators(), X_test, y_test, col)\n",
    "        print('='*20)\n",
    "\n",
    "print('#'*40)\n",
    "print('DONE!')\n",
    "print('#'*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b689f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "study1_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
