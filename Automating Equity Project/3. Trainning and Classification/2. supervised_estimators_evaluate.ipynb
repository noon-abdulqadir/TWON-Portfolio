{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d4c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "if code_dir_name not in str(Path.cwd()).split('/')[-1]:\n",
    "    for _ in range(5):\n",
    "\n",
    "        parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "        if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "            code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "            if code_dir is not None:\n",
    "                break\n",
    "else:\n",
    "    code_dir = str(Path.cwd())\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef3f604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/Automating_Equity1/lib/python3.10/site-packages/outdated/utils.py:14: OutdatedPackageWarning: The package pingouin is out of date. Your version is 0.5.3, the latest is 0.5.4.\n",
      "Set the environment variable OUTDATED_IGNORE=1 to disable these warnings.\n",
      "  return warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ce3a33260442c7a651e0e05a200d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "from setup_module.estimators_get_pipe import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "from setup_module.plot_metric_fork import functions as plot_metric_functions # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55afc383",
   "metadata": {},
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b36fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "method = 'Supervised'\n",
    "with open(f'{data_dir}{method}_results_save_path.txt', 'r') as f:\n",
    "    results_save_path = f.read().strip('\\n')\n",
    "with open(f'{data_dir}{method}_done_xy_save_path.txt', 'r') as f:\n",
    "    done_xy_save_path = f.read().strip('\\n')\n",
    "\n",
    "t = time.time()\n",
    "n_jobs = -1\n",
    "n_splits = 10\n",
    "n_repeats = 3\n",
    "random_state = 42\n",
    "refit = True\n",
    "class_weight = 'balanced'\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=n_splits, n_repeats=n_repeats, random_state=random_state\n",
    ")\n",
    "scoring = 'recall'\n",
    "scores = [\n",
    "    'recall', 'accuracy', 'f1', 'roc_auc',\n",
    "    'explained_variance', 'matthews_corrcoef'\n",
    "]\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score, zero_division=0),\n",
    "    'recall_score': make_scorer(recall_score, zero_division=0),\n",
    "    'accuracy_score': make_scorer(accuracy_score, zero_division=0),\n",
    "}\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "metrics_dict = {\n",
    "    f'{scoring.title()} Best Score': np.nan,\n",
    "    f'{scoring.title()} Best Threshold': np.nan,\n",
    "    'Train - Mean Cross Validation Score': np.nan,\n",
    "    f'Train - Mean Cross Validation - {scoring.title()}': np.nan,\n",
    "    f'Train - Mean Explained Variance - {scoring.title()}': np.nan,\n",
    "    'Test - Mean Cross Validation Score': np.nan,\n",
    "    f'Test - Mean Cross Validation - {scoring.title()}': np.nan,\n",
    "    f'Test - Mean Explained Variance - {scoring.title()}': np.nan,\n",
    "    'Explained Variance': np.nan,\n",
    "    'Accuracy': np.nan,\n",
    "    'Balanced Accuracy': np.nan,\n",
    "    'Precision': np.nan,\n",
    "    'Average Precision': np.nan,\n",
    "    'Recall': np.nan,\n",
    "    'F1-score': np.nan,\n",
    "    'Matthews Correlation Coefficient': np.nan,\n",
    "    'Brier Score': np.nan,\n",
    "    'Fowlkes–Mallows Index': np.nan,\n",
    "    'R2 Score': np.nan,\n",
    "    'ROC': np.nan,\n",
    "    'AUC': np.nan,\n",
    "    'Log Loss/Cross Entropy': np.nan,\n",
    "    'Cohen’s Kappa': np.nan,\n",
    "    'Geometric Mean': np.nan,\n",
    "    'Classification Report': np.nan,\n",
    "    'Imbalanced Classification Report': np.nan,\n",
    "    'Confusion Matrix': np.nan,\n",
    "    'Normalized Confusion Matrix': np.nan,\n",
    "}\n",
    "skip_fitted_estimators = True\n",
    "evaluate_estimator_on_concat = False\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f3c5be4",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f179e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df_metrics(\n",
    "    vectorizers_pipe, classifiers_pipe, transformers_pipe, metrics_list,\n",
    "    col, vectorizer_name, classifier_name, protocol=None,\n",
    "    analysis_columns=analysis_columns,\n",
    "    table_save_path=table_save_path,\n",
    "    method=method, save_name=None,\n",
    "    compression=None, path_suffix=None,\n",
    "):\n",
    "    if save_name is None:\n",
    "        save_name = f'{method} Estimators Table'\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if isinstance(metrics_list, dict):\n",
    "        metrics_list = list(metrics_list.keys())\n",
    "\n",
    "    transformers_tokenizers_list = [\n",
    "        str(tranformer_dict['tokenizer']).split('.')[-1].split(\"'>\")[0]\n",
    "        for tranformer_dict in transformers_pipe.values()\n",
    "    ]\n",
    "    combined_classifiers_list = list(classifiers_pipe.keys()) + list(transformers_pipe.keys())\n",
    "    combined_vectorizers_list = list(vectorizers_pipe.keys()) + transformers_tokenizers_list\n",
    "\n",
    "    print('='*20)\n",
    "    if os.path.exists(f'{table_save_path}{save_name}.pkl') and os.path.getsize(f'{table_save_path}{save_name}.pkl') > 0:\n",
    "        print(f'Loading table from {save_name}.pkl')\n",
    "        df_metrics = pd.read_pickle(f'{table_save_path}{save_name}.pkl')\n",
    "        print('Done loading table!')\n",
    "    else:\n",
    "        print('Table does not exist, creating new table...')\n",
    "        if method == 'Transformers':\n",
    "            index = pd.MultiIndex.from_product(\n",
    "                [list(map(lambda classifier_name: classifier_name, list(transformers_pipe.keys())))],\n",
    "                names=['Classifiers'],\n",
    "            )\n",
    "            columns = pd.MultiIndex.from_product(\n",
    "                [\n",
    "                    analysis_columns,\n",
    "                    metrics_list,\n",
    "                ],\n",
    "                names=['Variable', 'Measures'],\n",
    "            )\n",
    "        elif method == 'Supervised':\n",
    "            index = pd.MultiIndex.from_product(\n",
    "                [list(map(lambda classifier_name: classifier_name, list(classifiers_pipe.keys())))],\n",
    "                names=['Classifiers'],\n",
    "            )\n",
    "            columns = pd.MultiIndex.from_product(\n",
    "                [\n",
    "                    analysis_columns,\n",
    "                    list(map(lambda vectorizer_name: vectorizer_name, list(vectorizers_pipe.keys()))),\n",
    "                    metrics_list,\n",
    "                ],\n",
    "                names=['Variable', 'Vectorizer', 'Measures'],\n",
    "            )\n",
    "        # Make df\n",
    "        df_metrics = pd.DataFrame(index=index, columns=columns)\n",
    "        print('Done creating new table!')\n",
    "    print('='*20)\n",
    "\n",
    "    return df_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccfbba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_files(\n",
    "    results_save_path=results_save_path,\n",
    "    estimator_names_list=None,\n",
    "):\n",
    "    if estimator_names_list is None:\n",
    "        estimator_names_list = []\n",
    "\n",
    "    print(f'Searching for existing estimators in directory.')\n",
    "\n",
    "    for estimators_file in tqdm.tqdm(glob.glob(f'{results_save_path}*.*')):\n",
    "        if f'{method} Estimator - ' in estimators_file:\n",
    "\n",
    "            col=estimators_file.split(f'{method} Estimator - ')[-1].split(' - ')[0]\n",
    "            vectorizer_name=estimators_file.split(f'{col} - ')[-1].split(' + ')[0]\n",
    "            classifier_name=estimators_file.split(f'{vectorizer_name} + ')[-1].split(' (Save_protocol=')[0]\n",
    "\n",
    "            estimator_names_list.append(f'{col} - {vectorizer_name} + {classifier_name}')\n",
    "\n",
    "    return (\n",
    "        list(set(estimator_names_list))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ab2972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Xy_search_cv_estimator(\n",
    "    col, vectorizer_name, classifier_name, protocol,\n",
    "    results_save_path=results_save_path,\n",
    "    done_xy_save_path=done_xy_save_path, method=method,\n",
    "    compression=None, saved_files_list=None,\n",
    "    path_suffix=None, data_dict=None,\n",
    "):\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {col} - {vectorizer_name} + {classifier_name} (Save_protocol={protocol}).pkl'\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "    if saved_files_list is None:\n",
    "        saved_files_list = []\n",
    "\n",
    "    # Load data into dict\n",
    "    for file_path in glob.glob(f'{done_xy_save_path}{method}*{path_suffix}*'):\n",
    "        file_name = file_path.split(f'{done_xy_save_path}{method} ')[-1].split(path_suffix)[0]\n",
    "        print(f'Loading {file_name}')\n",
    "        if 'df_' in file_name:\n",
    "            data_dict[file_name] = pd.read_pickle(file_path)\n",
    "        else:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data_dict[file_name] = joblib.load(f)\n",
    "        saved_files_list.append(file_name)\n",
    "    # Load estimator\n",
    "    print('Loading Estimator.')\n",
    "    with open(\n",
    "        f'{results_save_path}{method} Estimator{path_suffix}', 'rb'\n",
    "    ) as f:\n",
    "        data_dict['Estimator'] = joblib.load(f)\n",
    "    saved_files_list.append('Estimator')\n",
    "\n",
    "    # # Assign data to variables\n",
    "    estimator = data_dict['Estimator']\n",
    "    grid_search = data_dict['Grid Search']\n",
    "    searchcv = data_dict['SearchCV']\n",
    "    df_cv_results = data_dict['df_cv_results']\n",
    "    # Train data\n",
    "    df_train_data = data_dict['df_train_data']\n",
    "    X_train = df_train_data['X_train'].values\n",
    "    y_train = df_train_data['y_train'].values\n",
    "    y_train_pred = df_train_data['y_train_pred'].values\n",
    "    # Test data\n",
    "    df_test_data = data_dict['df_test_data']\n",
    "    X_test = df_test_data['X_test'].values\n",
    "    y_test = df_test_data['y_test'].values\n",
    "    y_test_pred = df_test_data['y_test_pred'].values\n",
    "    y_test_pred_prob = df_test_data['y_test_pred_prob'].values\n",
    "    # Val data\n",
    "    df_val_data = data_dict['df_val_data']\n",
    "    X_val = df_val_data['X_val'].values\n",
    "    y_val = df_val_data['y_val'].values\n",
    "    y_val_pred = df_val_data['y_val_pred'].values\n",
    "    y_val_pred_prob = df_val_data['y_val_pred_prob'].values\n",
    "    # Feature importances\n",
    "    if 'df_feature_importances' in data_dict.keys():\n",
    "        saved_files_list.append('df_feature_importances')\n",
    "        df_feature_importances = data_dict['df_feature_importances']\n",
    "    else:\n",
    "        df_feature_importances = None\n",
    "\n",
    "    # Check data\n",
    "    check_consistent_length(X_train, y_train, y_train_pred)\n",
    "    check_consistent_length(X_test, y_test, y_test_pred, y_test_pred_prob)\n",
    "    check_is_fitted(estimator)\n",
    "\n",
    "    # Get class weights\n",
    "    (\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    ) = get_class_weights(\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "    )\n",
    "    # Print info\n",
    "    print_Xy(\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )\n",
    "\n",
    "    assert set(data_dict.keys()) == set(saved_files_list), f'Not all files were loaded! Missing: {set(data_dict.keys()) ^ set(saved_files_list)}'\n",
    "    print(f'Done loading Xy, CV data, and estimator!\\n{list(data_dict.keys())}')\n",
    "    print('='*20)\n",
    "\n",
    "    return (\n",
    "        grid_search, searchcv,\n",
    "        X_train, y_train, y_train_pred,\n",
    "        X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "        X_val, y_val, y_val_pred, y_val_pred_prob,\n",
    "        df_feature_importances, df_cv_results, estimator,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1f1ef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    X_val, y_val,\n",
    "):\n",
    "    # Get train class weights\n",
    "    train_class_weights = compute_class_weight(class_weight = class_weight, classes = np.unique(y_train), y = y_train)\n",
    "    train_class_weights_ratio = train_class_weights[0]/train_class_weights[1]\n",
    "    train_class_weights_dict = dict(zip(np.unique(y_train), train_class_weights))\n",
    "\n",
    "    # Get train class weights\n",
    "    test_class_weights = compute_class_weight(class_weight = class_weight, classes = np.unique(y_test), y = y_test)\n",
    "    test_class_weights_ratio = test_class_weights[0]/test_class_weights[1]\n",
    "    test_class_weights_dict = dict(zip(np.unique(y_test), test_class_weights))\n",
    "\n",
    "    # Get val class weights\n",
    "    val_class_weights = compute_class_weight(class_weight = class_weight, classes = np.unique(y_val), y = y_val)\n",
    "    val_class_weights_ratio = val_class_weights[0]/val_class_weights[1]\n",
    "    val_class_weights_dict = dict(zip(np.unique(y_val), val_class_weights))\n",
    "\n",
    "    return (\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a58ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_Xy(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    X_val, y_val,\n",
    "    train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "    test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "    val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "):\n",
    "    # Check for consistent length\n",
    "    check_consistent_length(X_train, y_train)\n",
    "    check_consistent_length(X_test, y_test)\n",
    "    check_consistent_length(X_val, y_val)\n",
    "\n",
    "    print('Done splitting data into training and testing sets.')\n",
    "    print('='*20)\n",
    "    print(f'Training set shape: {y_train.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Training set example:\\n{X_train[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Testing set shape: {y_test.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing set example:\\n{X_test[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Validation set shape: {y_val.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation set example:\\n{X_val[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Training data class weights:\\nRatio = {train_class_weights_ratio:.2f} (0 = {train_class_weights[0]:.2f}, 1 = {train_class_weights[1]:.2f})')\n",
    "    print('-'*10)\n",
    "    print(f'Testing data class weights:\\nRatio = {test_class_weights_ratio:.2f} (0 = {test_class_weights[0]:.2f}, 1 = {test_class_weights[1]:.2f})')\n",
    "    print('-'*10)\n",
    "    print(f'Validation data class weights:\\nRatio = {val_class_weights_ratio:.2f} (0 = {val_class_weights[0]:.2f}, 1 = {val_class_weights[1]:.2f})')\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76cc9dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_estimator(\n",
    "    estimator, X, y_labels, col, vectorizer_name, classifier_name,\n",
    "    cv=cv, return_train_score=None, results_save_path=results_save_path, print_enabled=None\n",
    "):\n",
    "    if return_train_score is None:\n",
    "        return_train_score = True\n",
    "    if print_enabled is None:\n",
    "        print_enabled = True\n",
    "\n",
    "    # Using estimator\n",
    "    # Cross Validation\n",
    "    if print_enabled:\n",
    "        print('-'*20)\n",
    "        print('Computing metrics using estimator.')\n",
    "        print('-'*20)\n",
    "        print('Cross Validating without scoring.')\n",
    "    cv_score_noscoring = cross_validate(\n",
    "        estimator,\n",
    "        X,\n",
    "        y_labels,\n",
    "        cv=cv,\n",
    "        scoring=None,\n",
    "        return_train_score=return_train_score,\n",
    "    )\n",
    "\n",
    "    # Cross Validation with scoring\n",
    "    if print_enabled:\n",
    "        print('-'*20)\n",
    "        print(f'Cross Validating with {scores} scoring.')\n",
    "    cv_score_recall = cross_validate(\n",
    "        estimator,\n",
    "        X,\n",
    "        y_labels,\n",
    "        cv=cv,\n",
    "        scoring=scores,\n",
    "        return_train_score=return_train_score,\n",
    "    )\n",
    "\n",
    "    # Get mean and std of cross validation scores\n",
    "    if print_enabled:\n",
    "        print('-'*20)\n",
    "        print('Getting mean and std of cross validation scores.')\n",
    "    cv_train_scores = cv_score_noscoring['train_score'].mean()\n",
    "    cv_test_scores = cv_score_noscoring['test_score'].mean()\n",
    "    cv_train_recall = cv_score_recall['train_recall'].mean()\n",
    "    cv_test_recall = cv_score_recall['test_recall'].mean()\n",
    "    cv_train_explained_variance_recall = cv_score_recall['train_explained_variance'].mean()\n",
    "    cv_test_explained_variance_recall = cv_score_recall['test_explained_variance'].mean()\n",
    "\n",
    "    # Save cross validation scores to dataframe\n",
    "    if print_enabled:\n",
    "        print('-'*20)\n",
    "        print('Saving cross validation scores to dataframe.')\n",
    "    df_cv_score_noscoring = pd.DataFrame(cv_score_noscoring)\n",
    "    df_cv_score_noscoring.to_pickle(f'{results_save_path}df_cv_score_noscoring - {col}_{vectorizer_name}_{classifier_name}.pkl')\n",
    "    df_cv_score_recall = pd.DataFrame(cv_score_recall)\n",
    "    df_cv_score_recall.to_pickle(f'{results_save_path}df_cv_score_recall - {col}_{vectorizer_name}_{classifier_name}.pkl')\n",
    "\n",
    "    return (\n",
    "        df_cv_score_recall,\n",
    "        cv_train_scores, cv_test_scores,\n",
    "        cv_train_recall, cv_test_recall,\n",
    "        cv_train_explained_variance_recall, cv_test_explained_variance_recall\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0e84026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_with_estimator(\n",
    "    estimator, X, y_labels, col, vectorizer_name, classifier_name, random_state=random_state, n_jobs=n_jobs, cv=cv,\n",
    "    params=None, axis=None, alpha=None, verbose=None, print_enabled=None\n",
    "):\n",
    "    if axis is None:\n",
    "        axis = 1\n",
    "    if alpha is None:\n",
    "        alpha = 0.1\n",
    "    if verbose is None:\n",
    "        verbose=1\n",
    "    if print_enabled is None:\n",
    "        print_enabled = True\n",
    "\n",
    "    # Make param names and values\n",
    "    if params is None:\n",
    "        params = {\n",
    "            param_name: classifier_params\n",
    "            for classifier_name, classifier_and_params in classifiers_pipe.items()\n",
    "            if estimator[-1].__class__.__name__ == classifier_name\n",
    "            for param_name_, classifier_params in classifier_and_params[-1].items()\n",
    "            for param_name in [param_name_.split(f'{classifier_name}__')[-1]]\n",
    "            if param_name != 'random_state' and all(isinstance(n, (list, int, float)) for n in classifier_params) and not all(isinstance(n, (bool)) for n in classifier_params)\n",
    "        }\n",
    "\n",
    "\n",
    "    # Learning Curves\n",
    "    print('Plotting Learning Curve.')\n",
    "    print('-'*20)\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator=estimator,\n",
    "        X=X,\n",
    "        y=y_labels,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        random_state=random_state,\n",
    "        shuffle=True,\n",
    "        scoring=scoring,\n",
    "        verbose=verbose,\n",
    "        # train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=axis)\n",
    "    train_scores_std = np.std(train_scores, axis=axis)\n",
    "    test_scores_mean = np.mean(test_scores, axis=axis)\n",
    "    test_scores_std = np.std(test_scores, axis=axis)\n",
    "\n",
    "    close_plots(plt)\n",
    "    plt.figure()\n",
    "    plt.title(\n",
    "        f'{col} - Learning Curves for {scoring.title()} - {vectorizer_name} + {classifier_name}',\n",
    "        )\n",
    "    plt.xlabel('Training examples')\n",
    "    plt.ylabel('Score')\n",
    "    plt.grid()\n",
    "    plt.fill_between(\n",
    "        train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=alpha, color='r'\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=alpha, color='g'\n",
    "    )\n",
    "    plt.plot(\n",
    "        train_sizes, train_scores_mean, 'o-', color='r', label='Training score'\n",
    "    )\n",
    "    plt.plot(\n",
    "        train_sizes, test_scores_mean, 'o-', color='g', label='Cross-validation score'\n",
    "    )\n",
    "    plt.legend(loc='best')\n",
    "    fig = plt.gcf()\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save figure\n",
    "    for image_save_format in tqdm.tqdm(['png', 'svg']):\n",
    "        save_path = f'{plot_save_path}{method} {col} - Learning Curve - {vectorizer_name} + {classifier_name}.{image_save_format}'\n",
    "        print(f'Saving Learning Curve.')\n",
    "        fig.savefig(save_path, format=image_save_format)\n",
    "    if print_enabled: show_and_close_plots(plt)\n",
    "\n",
    "    # Validation Curve\n",
    "    for param_name, param_range in params.items():\n",
    "        param_title = ' '.join(param_name.split('_')).title()\n",
    "        print(f'Plotting Validation Curve for {param_title}.')\n",
    "        print('-'*20)\n",
    "        train_scores, test_scores = validation_curve(\n",
    "            estimator=estimator[-1],\n",
    "            X=X,\n",
    "            y=y_labels,\n",
    "            param_name=param_name,\n",
    "            param_range=param_range,\n",
    "            cv=cv,\n",
    "            n_jobs=n_jobs,\n",
    "            scoring=scorers['recall_score'],\n",
    "            verbose=1,\n",
    "        )\n",
    "        train_scores_mean = np.mean(train_scores, axis=axis)\n",
    "        train_scores_std = np.std(train_scores, axis=axis)\n",
    "        test_scores_mean = np.mean(test_scores, axis=axis)\n",
    "        test_scores_std = np.std(test_scores, axis=axis)\n",
    "\n",
    "        # Ploting\n",
    "        plt.figure()\n",
    "        plt.title(\n",
    "            f'{col} - Validation Curve for {scoring.title()} - {col} - {vectorizer_name} + {classifier_name}',\n",
    "        )\n",
    "        plt.xlabel(param_name)\n",
    "        plt.ylabel('Score')\n",
    "        plt.grid()\n",
    "        plt.fill_between(\n",
    "            param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=alpha, color='r'\n",
    "        )\n",
    "        plt.fill_between(\n",
    "            param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=alpha, color='g'\n",
    "        )\n",
    "        plt.semilogx(\n",
    "            param_range, train_scores_mean, label='Training score', color='r'\n",
    "        )\n",
    "        plt.semilogx(\n",
    "            param_range, test_scores_mean, label='Cross-validation score', color='g'\n",
    "        )\n",
    "        plt.plot(\n",
    "            param_range, train_scores_mean, 'o-', color='r', label='Training score'\n",
    "        )\n",
    "        plt.plot(\n",
    "            param_range, test_scores_mean, 'o-', color='g', label='Cross-validation score'\n",
    "        )\n",
    "        plt.legend(loc='best')\n",
    "        fig = plt.gcf()\n",
    "        fig.tight_layout()\n",
    "\n",
    "        # Save figure\n",
    "        for image_save_format in tqdm.tqdm(['png', 'svg']):\n",
    "            save_path = f'{plot_save_path}{method} {col} - Validation Curve for {scoring.title()} - {vectorizer_name} + {classifier_name}.{image_save_format}'\n",
    "            print(f'Saving Validation Curve')\n",
    "            fig.savefig(\n",
    "                save_path, format=image_save_format\n",
    "            )\n",
    "        if print_enabled: show_and_close_plots(plt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd7a0330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_y_pred(\n",
    "    y_labels, y_pred,\n",
    "    pos_label=None, labels=None, zero_division=None, alpha=None, print_enabled=None\n",
    "):\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    if labels is None:\n",
    "        labels = np.unique(y_pred)\n",
    "    if zero_division is None:\n",
    "        zero_division = 0\n",
    "    if alpha is None:\n",
    "        alpha = 0.1\n",
    "    if print_enabled is None:\n",
    "        print_enabled = True\n",
    "\n",
    "    if print_enabled:\n",
    "        print('-'*20)\n",
    "        print('Computing metrics using y_pred.')\n",
    "    # Using y_pred\n",
    "    explained_variance = metrics.explained_variance_score(y_labels, y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_labels, y_pred)\n",
    "    balanced_accuracy = metrics.balanced_accuracy_score(y_labels, y_pred)\n",
    "    precision = metrics.precision_score(y_labels, y_pred, pos_label=pos_label, labels=labels, zero_division=zero_division)\n",
    "    recall = metrics.recall_score(y_labels, y_pred, pos_label=pos_label, labels=labels, zero_division=zero_division)\n",
    "    f1 = metrics.f1_score(y_labels, y_pred, pos_label=pos_label,labels=labels, zero_division=zero_division)\n",
    "    mcc = metrics.matthews_corrcoef(y_labels, y_pred)\n",
    "    brier = metrics.brier_score_loss(y_labels, y_pred)\n",
    "    fm = metrics.fowlkes_mallows_score(y_labels, y_pred)\n",
    "    r2 = metrics.r2_score(y_labels, y_pred)\n",
    "    kappa = metrics.cohen_kappa_score(y_labels, y_pred, labels=labels)\n",
    "    gmean_iba = imblearn.metrics.make_index_balanced_accuracy(alpha=alpha, squared=True)(geometric_mean_score)\n",
    "    gmean = gmean_iba(y_labels, y_pred)\n",
    "    report = metrics.classification_report(y_labels, y_pred, labels=labels, zero_division=zero_division)\n",
    "    imblearn_report = classification_report_imbalanced(y_labels, y_pred, labels=labels, zero_division=zero_division)\n",
    "    cm = metrics.confusion_matrix(y_labels, y_pred, labels=labels)\n",
    "    cm_normalized = metrics.confusion_matrix(y_labels, y_pred, normalize='true', labels=labels)\n",
    "\n",
    "    return (\n",
    "        explained_variance, accuracy, balanced_accuracy, precision,\n",
    "        recall, f1, mcc, brier, fm, r2, kappa, gmean, report, imblearn_report, cm, cm_normalized\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d89a7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_with_y_pred(\n",
    "    y_labels, y_pred, col, vectorizer_name, classifier_name,\n",
    "    pos_label=None, labels=None, print_enabled=None\n",
    "):\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    if labels is None:\n",
    "        labels = np.unique(y_pred)\n",
    "    if print_enabled is None:\n",
    "        print_enabled = True\n",
    "\n",
    "    # Displays\n",
    "    close_plots(plt)\n",
    "    cm_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "        y_labels, y_pred, display_labels=labels, cmap=plt.cm.Grays, colorbar=True\n",
    "    )\n",
    "    cm_normalized_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "        y_labels, y_pred, normalize='true', display_labels=labels, cmap=plt.cm.Grays, colorbar=True\n",
    "    )\n",
    "    roc_curve = metrics.RocCurveDisplay.from_predictions(\n",
    "        y_labels, y_pred, pos_label=pos_label, color='black'\n",
    "    )\n",
    "    pr_curve = metrics.PrecisionRecallDisplay.from_predictions(\n",
    "        y_labels, y_pred, pos_label=pos_label, color='black'\n",
    "    )\n",
    "    calibration_curve = CalibrationDisplay.from_predictions(\n",
    "        y_labels, y_pred, pos_label=pos_label, color='black'\n",
    "    )\n",
    "    if print_enabled: show_and_close_plots(plt)\n",
    "\n",
    "    # Plots\n",
    "    plots_dict = {\n",
    "        'Confusion Matrix': cm_curve,\n",
    "        'Normalized Confusion Matrix': cm_normalized_curve,\n",
    "        'ROC Curve': roc_curve,\n",
    "        'Precision-Recall Curve': pr_curve,\n",
    "        'Calibration Curve': calibration_curve,\n",
    "    }\n",
    "\n",
    "    print('=' * 20)\n",
    "    close_plots(plt)\n",
    "    print('Plotting metrics with y_pred:')\n",
    "    print('='*20)\n",
    "\n",
    "    for plot_name, plot_ in tqdm.tqdm(plots_dict.items()):\n",
    "        close_plots(plt)\n",
    "        print(f'Plotting {plot_name}:')\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(\n",
    "            f'{col} - {plot_name} - {vectorizer_name} + {classifier_name}'\n",
    "            )\n",
    "        if plot_name == 'ROC Curve':\n",
    "            ax.plot([0, 1], [0, 1], 'r--', lw=1)\n",
    "        try:\n",
    "            plot_.plot(ax=ax, cmap=plt.cm.Grays)\n",
    "        except Exception:\n",
    "            plot_.plot(ax=ax, color='black')\n",
    "        print('=' * 20)\n",
    "        fig = plt.gcf()\n",
    "        fig.tight_layout()\n",
    "\n",
    "        # Save Plots\n",
    "        for image_save_format in tqdm.tqdm(['png', 'svg']):\n",
    "            save_path = f'{plot_save_path}{method} {col} - {plot_name} - {vectorizer_name} + {classifier_name}.{image_save_format}'\n",
    "            print(f'Saving {plot_name}')\n",
    "            try:\n",
    "                fig.savefig(\n",
    "                    save_path, format=image_save_format, dpi=3000, bbox_inches='tight'\n",
    "                )\n",
    "            except Exception:\n",
    "                print(f'Failed to save {plot_name}!')\n",
    "                print('=' * 20)\n",
    "            else:\n",
    "                print(f'Saved {plot_name}!')\n",
    "                print('=' * 20)\n",
    "        if print_enabled: show_and_close_plots(plt)\n",
    "\n",
    "    # with contextlib.suppress(AttributeError):\n",
    "    # Visualisation with plot_metric\n",
    "    bc = plot_metric_functions.BinaryClassification(y_labels, y_pred, labels=[0, 1], matplotlib_style='grayscale', seaborn_style='whitegrid')\n",
    "\n",
    "    # Figures\n",
    "    close_plots(plt)\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    fig.suptitle(f'{col} - {vectorizer_name} + {classifier_name}')\n",
    "    plt.subplot2grid((2, 6), (1, 1), colspan=2)\n",
    "    bc.plot_confusion_matrix(colorbar=True, cmap=plt.cm.Grays)\n",
    "    plt.subplot2grid((2, 6), (1, 3), colspan=2)\n",
    "    bc.plot_confusion_matrix(normalize=True, colorbar=True, cmap=plt.cm.Grays)\n",
    "    plt.subplot2grid(shape=(2, 6), loc=(0, 0), colspan=2)\n",
    "    bc.plot_roc_curve()\n",
    "    plt.subplot2grid((2, 6), (0, 2), colspan=2)\n",
    "    bc.plot_precision_recall_curve()\n",
    "    plt.subplot2grid((2, 6), (0, 4), colspan=2)\n",
    "    bc.plot_class_distribution()\n",
    "    bc.print_report()\n",
    "    fig = plt.gcf()\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save Plots\n",
    "    for image_save_format in tqdm.tqdm(['png', 'svg']):\n",
    "        save_path = f'{plot_save_path}{method} {col} - plot_metric Curves - {vectorizer_name} + {classifier_name}.{image_save_format}'\n",
    "        print(f'Saving plot_metric Curves')\n",
    "        fig.savefig(\n",
    "            save_path, format=image_save_format, dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "    if print_enabled: show_and_close_plots(plt)\n",
    "\n",
    "    # Heatmap\n",
    "    print('Plotting Heatmap:')\n",
    "    close_plots(plt)\n",
    "    classifications_dict = defaultdict(int)\n",
    "    for _y_labels, _y_pred in zip(y_labels, y_pred):\n",
    "        if _y_labels != _y_pred:\n",
    "            classifications_dict[(_y_labels, _y_pred)] += 1\n",
    "\n",
    "    dicts_to_plot = [\n",
    "        {\n",
    "            f'True {col} value': _y_labels,\n",
    "            f'Predicted {col} value': _y_pred,\n",
    "            'Number of Classifications': _count,\n",
    "        }\n",
    "        for (_y_labels, _y_pred), _count in classifications_dict.items()\n",
    "    ]\n",
    "    df_to_plot = pd.DataFrame(dicts_to_plot)\n",
    "    df_wide = df_to_plot.pivot_table(\n",
    "        index=f'True {col} value',\n",
    "        columns=f'Predicted {col} value',\n",
    "        values='Number of Classifications'\n",
    "    )\n",
    "    plt.figure(figsize=(9,7))\n",
    "    sns.set(style='ticks', font_scale=1.2)\n",
    "    sns.heatmap(df_wide, linewidths=1, cmap=plt.cm.Grays, annot=True)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.title(f'{col} Heatmap - {vectorizer_name} + {classifier_name}')\n",
    "    fig = plt.gcf()\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save Heatmap\n",
    "    for image_save_format in tqdm.tqdm(['png', 'svg']):\n",
    "        save_path = f'{plot_save_path}{method} {col} - Heatmap - {vectorizer_name} + {classifier_name}.{image_save_format}'\n",
    "        print(f'Saving Heatmap')\n",
    "        fig.savefig(\n",
    "            save_path, format=image_save_format, dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "    print('Saved Heatmap!')\n",
    "    if print_enabled: show_and_close_plots(plt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_with_y_pred_and_estimator(\n",
    "    estimator, X, y_labels, y_pred, col, vectorizer_name, classifier_name,\n",
    "    pos_label=None, labels=None, print_enabled=None\n",
    "):\n",
    "    # Make pipe\n",
    "    pipe = make_pipeline(estimator.steps[0][1], TruncatedSVD(n_components=2))\n",
    "\n",
    "    # Apply fit_transform on texts\n",
    "    X_viz = pipe.fit_transform(X)\n",
    "\n",
    "    # Plot\n",
    "    close_plots(plt)\n",
    "    altair_plt = plot_text(\n",
    "        X_viz,\n",
    "        X,\n",
    "        color_array=y_pred,\n",
    "        color_words=[\n",
    "            stemmer.stem(word.lower())\n",
    "            for word in warmth_competence_words[col]\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Save Text Clusters\n",
    "    title = f'{col} - Text Clusters - {vectorizer_name}'\n",
    "    altair_plt.title = title\n",
    "    save_path = f'{plot_save_path}{method} {title}'\n",
    "    print(f'Saving Text Clusters')\n",
    "    altair_plt.save(f'{save_path}.html')\n",
    "    Html2Image(output_path=plot_save_path, size=(2644, 1604)).screenshot(\n",
    "        html_file=f'{save_path}.html', save_as=f'{save_path.split(plot_save_path)[-1]}.png'\n",
    "    )\n",
    "    print('Saved Text Clusters!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1de7dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_y_pred_prob(\n",
    "    y_labels, y_pred_prob,\n",
    "    pos_label=None,\n",
    "    print_enabled=None\n",
    "):\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    if print_enabled is None:\n",
    "        print_enabled = True\n",
    "\n",
    "    if print_enabled:\n",
    "        print('-'*20)\n",
    "        print('Computing metrics using y_pred_prob.')\n",
    "    average_precision = metrics.average_precision_score(y_labels, y_pred_prob)\n",
    "    roc_auc = metrics.roc_auc_score(y_labels, y_pred_prob)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_labels, y_pred_prob, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    loss = metrics.log_loss(y_labels, y_pred_prob)\n",
    "    precision_pr, recall_pr, threshold_pr = metrics.precision_recall_curve(y_labels, y_pred_prob, pos_label=1)\n",
    "\n",
    "    return (\n",
    "        average_precision, roc_auc, auc,\n",
    "        fpr, tpr, threshold, loss,\n",
    "        precision_pr, recall_pr, threshold_pr\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "162b0699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_all(\n",
    "    estimator, X, y_labels, y_pred, y_pred_prob,\n",
    "    col, vectorizer_name, classifier_name, cv=cv, n_jobs=n_jobs,\n",
    "    with_estimator=None, with_y_pred=None, with_y_pred_prob=None,\n",
    "    pos_label=None, verbose=None, print_enabled=None\n",
    "):\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    if verbose is None:\n",
    "        verbose = 1\n",
    "    if with_estimator is None:\n",
    "        with_estimator = True\n",
    "    if with_y_pred is None:\n",
    "        with_y_pred = True\n",
    "    if with_y_pred_prob is None:\n",
    "        with_y_pred_prob = True\n",
    "    if print_enabled is None:\n",
    "        print_enabled = True\n",
    "\n",
    "    # Get metrics\n",
    "    print('='*20)\n",
    "    # Using estimator\n",
    "    if with_estimator:\n",
    "        (\n",
    "            df_cv_score_recall,\n",
    "            cv_train_scores, cv_test_scores,\n",
    "            cv_train_recall, cv_test_recall,\n",
    "            cv_train_explained_variance_recall, cv_test_explained_variance_recall\n",
    "        ) = compute_metrics_with_estimator(\n",
    "             estimator, X, y_labels, col, vectorizer_name, classifier_name, print_enabled=print_enabled\n",
    "        )\n",
    "    # Using y_pred\n",
    "    if with_y_pred:\n",
    "        (\n",
    "            explained_variance, accuracy, balanced_accuracy, precision,\n",
    "            recall, f1, mcc, brier, fm, r2, kappa, gmean, report, imblearn_report, cm, cm_normalized\n",
    "        ) = compute_metrics_with_y_pred(\n",
    "            y_labels, y_pred, print_enabled=print_enabled\n",
    "        )\n",
    "    # Using y_pred_prob\n",
    "    if with_y_pred_prob:\n",
    "        (\n",
    "            average_precision, roc_auc, auc,\n",
    "            fpr, tpr, threshold, loss,\n",
    "            precision_pr, recall_pr, threshold_pr\n",
    "        ) = compute_metrics_with_y_pred_prob(\n",
    "            y_labels, y_pred_prob, print_enabled=print_enabled\n",
    "        )\n",
    "\n",
    "    #Place metrics into dict\n",
    "    if print_enabled:\n",
    "        print('-'*20)\n",
    "        print('Appending metrics to dict.')\n",
    "    metrics_dict = {\n",
    "        f'{scoring.title()} Best Score': float(best_train_score),\n",
    "        f'{scoring.title()} Best Threshold': threshold,\n",
    "        'Train - Mean Cross Validation Score': float(cv_train_scores),\n",
    "        f'Train - Mean Cross Validation - {scoring.title()}': float(cv_train_recall),\n",
    "        f'Train - Mean Explained Variance - {scoring.title()}': float(cv_train_explained_variance_recall),\n",
    "        'Test - Mean Cross Validation Score': float(cv_test_scores),\n",
    "        f'Test - Mean Cross Validation - {scoring.title()}': float(cv_test_recall),\n",
    "        f'Test - Mean Explained Variance - {scoring.title()}': float(cv_test_explained_variance_recall),\n",
    "        'Explained Variance': float(explained_variance),\n",
    "        'Accuracy': float(accuracy),\n",
    "        'Balanced Accuracy': float(balanced_accuracy),\n",
    "        'Precision': float(precision),\n",
    "        'Average Precision': float(average_precision),\n",
    "        'Recall': float(recall),\n",
    "        'F1-score': float(f1),\n",
    "        'Matthews Correlation Coefficient': float(mcc),\n",
    "        'Brier Score': float(brier),\n",
    "        'Fowlkes–Mallows Index': float(fm),\n",
    "        'R2 Score': float(r2),\n",
    "        'ROC': float(roc_auc),\n",
    "        'AUC': float(auc),\n",
    "        'Log Loss/Cross Entropy': float(loss),\n",
    "        'Cohen’s Kappa': float(kappa),\n",
    "        'Geometric Mean': float(gmean),\n",
    "        'Classification Report': report,\n",
    "        'Imbalanced Classification Report': str(imblearn_report),\n",
    "        'Confusion Matrix': str(cm),\n",
    "        'Normalized Confusion Matrix': str(cm_normalized),\n",
    "    }\n",
    "    if print_enabled: print('Done appending metrics to dict.')\n",
    "\n",
    "    return (\n",
    "        metrics_dict, df_cv_score_recall,\n",
    "        cv_train_scores, cv_test_scores,\n",
    "        cv_train_recall, cv_test_recall,\n",
    "        cv_train_explained_variance_recall, cv_test_explained_variance_recall,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42fdf3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(\n",
    "    estimator, X, y_labels, y_pred, y_pred_prob,\n",
    "    col, vectorizer_name, classifier_name,\n",
    "    with_estimator=None, with_y_pred=None, with_y_pred_prob=None, print_enabled=None\n",
    "):\n",
    "    if with_estimator is None:\n",
    "        with_estimator = True\n",
    "    if with_y_pred is None:\n",
    "        with_y_pred = True\n",
    "    if with_y_pred_prob is None:\n",
    "        with_y_pred_prob = True\n",
    "    if print_enabled is None:\n",
    "        print_enabled = True\n",
    "\n",
    "    # Plotting\n",
    "    print('~'*20)\n",
    "    print('Plotting metrics.')\n",
    "    print('~'*20)\n",
    "    # Using estimator\n",
    "    if with_estimator:\n",
    "        plot_metrics_with_estimator(\n",
    "             estimator, X, y_labels, col, vectorizer_name, classifier_name, print_enabled=print_enabled\n",
    "        )\n",
    "    # Using y_pred\n",
    "    if with_y_pred:\n",
    "        plot_metrics_with_y_pred(\n",
    "            y_labels, y_pred, col, vectorizer_name, classifier_name, print_enabled=print_enabled\n",
    "        )\n",
    "    # Using y_pred and estimator\n",
    "    if with_y_pred and with_estimator:\n",
    "        plot_metrics_with_y_pred_and_estimator(\n",
    "             estimator, X, y_labels, y_pred, col, vectorizer_name, classifier_name, print_enabled=print_enabled\n",
    "        )\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "585fa8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_predictions(\n",
    "    X_test, y_test, y_test_pred, col\n",
    "):\n",
    "    # Examine predictions\n",
    "    print('~'*20)\n",
    "    print(f'Examining predictions for {col}')\n",
    "    print('Incorrectly Classified Reviews:')\n",
    "    for _y_test, _y_test_pred, _X_test in random.sample(list(zip(y_test, y_test_pred, X_test)), 5):\n",
    "        if _y_test != _y_test_pred:\n",
    "            print('-'*20)\n",
    "            print(f'TRUE LABEL: {_y_test}')\n",
    "            print(f'PREDICTED LABEL: {_y_test_pred}')\n",
    "            print(f'REVIEW TEXT: {_X_test[:100]}')\n",
    "            print('-'*20)\n",
    "    print('~'*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8278ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(\n",
    "    estimator, X, y_labels, y_pred, y_pred_prob,\n",
    "    best_train_score, df_metrics,\n",
    "    col, vectorizer_name, classifier_name, scorig=scoring, plot_enabled=None, print_enabled=None\n",
    "):\n",
    "    if plot_enabled is None:\n",
    "        plot_enabled = True\n",
    "    if print_enabled is None:\n",
    "        print_enabled = True\n",
    "\n",
    "    # Get metrics dict\n",
    "    (\n",
    "        metrics_dict, df_cv_score_recall,\n",
    "        cv_train_scores, cv_test_scores,\n",
    "        cv_train_recall, cv_test_recall,\n",
    "        cv_train_explained_variance_recall, cv_test_explained_variance_recall,\n",
    "    ) = compute_metrics_all(\n",
    "        estimator, X, y_labels, y_pred, y_pred_prob,\n",
    "        col, vectorizer_name, classifier_name, print_enabled=print_enabled\n",
    "    )\n",
    "\n",
    "    # Print metrics\n",
    "    print('=' * 20)\n",
    "    print('~' * 20)\n",
    "    print(f' Testing Metrics for {col} - {vectorizer_name} + {classifier_name}')\n",
    "    print('~' * 20)\n",
    "    print(f'Classification Report:\\n {metrics_dict[\"Classification Report\"]}')\n",
    "    print('-' * 20)\n",
    "    for metric_name, metric_value in metrics_dict.items():\n",
    "        if 'Threshold' not in metric_name:\n",
    "            with contextlib.suppress(TypeError, ValueError):\n",
    "                metric_value = float(metric_value)\n",
    "            if isinstance(metric_value, (int, float)):\n",
    "                df_metrics.loc[\n",
    "                    (classifier_name), (col, vectorizer_name, metric_name)\n",
    "                ] = metric_value\n",
    "                print(f'{metric_name}: {round(metric_value, 2)}')\n",
    "            else:\n",
    "                print(f'{metric_name}:\\n{metric_value}')\n",
    "                df_metrics.loc[\n",
    "                    (classifier_name), (col, vectorizer_name, metric_name)\n",
    "                ] = str(metric_value)\n",
    "            print('-' * 20)\n",
    "\n",
    "    print('=' * 20)\n",
    "\n",
    "    if plot_enabled:\n",
    "        # Plot Metrics\n",
    "        plot_metrics(\n",
    "            estimator, X, y_labels, y_pred, y_pred_prob,\n",
    "            col, vectorizer_name, classifier_name,\n",
    "        )\n",
    "\n",
    "    return df_metrics, metrics_dict, df_cv_score_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7ccd653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_confirmatory_tests(y_pred, y_pred_prob):\n",
    "\n",
    "    # Confirmatory Regression\n",
    "    print('+'*20)\n",
    "    print('Confirmatory Tests validating the linear relationship between y_pred and y_pred_prob')\n",
    "    print('-'*20)\n",
    "    print('T-Test y_pred_prob ~ y_pred:')\n",
    "    levene = scipy.stats.levene(y_pred_prob, y_pred)\n",
    "    equal_var_levene = levene.pvalue < 0.05\n",
    "    print(scipy.stats.ttest_ind(y_pred_prob, y_pred, equal_var=equal_var_levene))\n",
    "\n",
    "\n",
    "    print('\\n')\n",
    "    print('-'*20)\n",
    "    print('Logit y_pred ~ y_pred_prob:')\n",
    "    try:\n",
    "        logit_model = sm.Logit(endog=y_pred, exog=y_pred_prob)\n",
    "        logit_results = logit_model.fit()\n",
    "        std_coef = logit_results.params[0] / np.std(y_pred_prob)\n",
    "        std_err = logit_results.bse[0]\n",
    "        log_likelihood = logit_results.llf\n",
    "        print(logit_results.summary())\n",
    "        print('-'*20)\n",
    "        print(f'Std Coef: {std_coef}')\n",
    "        print(f'Std Err: {std_err}')\n",
    "        print(f'Log Likelihood: {log_likelihood}')\n",
    "    except Exception as e:\n",
    "        print(type(e).__name__)\n",
    "\n",
    "    print('-'*20)\n",
    "    print('\\n')\n",
    "    print('-'*20)\n",
    "    print('OLS y_pred_prob ~ y_pred:')\n",
    "    try:\n",
    "        ols_model = sm.OLS(endog=y_pred_prob, exog=y_pred)\n",
    "        ols_results = ols_model.fit()\n",
    "        std_coef = ols_results.params[0] / np.std(y_pred)\n",
    "        std_err = ols_results.bse[0]\n",
    "        print(ols_results.summary())\n",
    "        print('-'*20)\n",
    "        print(f'Std Coef: {std_coef}')\n",
    "        print(f'Std Err: {std_err}')\n",
    "    except Exception as e:\n",
    "        print(type(e).__name__)\n",
    "\n",
    "    print('-'*20)\n",
    "    print('+'*20)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9d5b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fitted_estimator(\n",
    "    estimator,\n",
    "    col, vectorizer_name, classifier_name,\n",
    "    protocol=None,\n",
    "    results_save_path=results_save_path,\n",
    "    method=method, done_xy_save_path=done_xy_save_path,\n",
    "    path_suffix=None, data_dict=None,\n",
    "    compression=None,\n",
    "):\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {col} - {vectorizer_name} + {classifier_name} (Save_protocol={protocol}).pkl'\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "\n",
    "    # Save fitted estimator\n",
    "    print('~'*20)\n",
    "    print(f'Saving fitted estimator {classifier_name}')\n",
    "    with open(\n",
    "        f'{results_save_path}{method} Fitted Estimator{path_suffix}', 'wb'\n",
    "    ) as f:\n",
    "        joblib.dump(estimator, f, compress=compression, protocol=protocol)\n",
    "    print('~'*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a295f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize unusual classifiers after fitting\n",
    "def normalize_after_fitting(estimator, X_train, y_train, X_test, y_test, grid_search, searchcv, vectorizer_name, classifier_name):\n",
    "    # Classifiers to normalize = ['GaussianNB', 'DecisionTreeClassifier', 'RandomForestClassifier', 'ExtraTreesClassifier', 'AdaBoostClassifier', 'GradientBoostingClassifier', 'XGBClassifier', 'Perceptron', 'Sequential']\n",
    "\n",
    "    # Get feature importance if classifier provides them and use as X\n",
    "    if any(hasattr(estimator, feature_attr) for feature_attr in ['feature_importances_', 'coef_']):\n",
    "        feature_selector = SelectFromModel(estimator, prefit=True)\n",
    "        X_train = feature_selector.transform(X_train)\n",
    "        X_test = X_test[:, feature_selector.get_support()]\n",
    "        df_feature_importances = pd.DataFrame(\n",
    "            {\n",
    "                'features': X_test.values,\n",
    "                'feature_importances': estimator.feature_importances_\n",
    "            }\n",
    "        )\n",
    "        df_feature_importances = df_feature_importances.sort_values('feature_importances', ascending=False)\n",
    "        print(df_feature_importances.head(20))\n",
    "        print(f'Best estimator has feature_importances of shape:\\n{estimator}')\n",
    "    else:\n",
    "        df_feature_importances = None\n",
    "\n",
    "    # For perceptron: calibrate classifier to get prediction probabilities\n",
    "    if (not hasattr(searchcv, 'predict_proba') and not hasattr(searchcv, '_predict_proba_lr') and hasattr(searchcv, 'decision_function')) or classifier_name == 'Perceptron' or estimator.__class__.__name__ == 'Perceptron':\n",
    "        path_suffix = f' - {col} - {vectorizer_name} + {classifier_name} (Save_protocol={protocol}).pkl'\n",
    "        data_dict = {\n",
    "            'Estimator': estimator,\n",
    "            'Grid Search': grid_search,\n",
    "            'SearchCV': searchcv,\n",
    "        }\n",
    "        for file_name, file_ in data_dict.items():\n",
    "            with open(\n",
    "                f'{results_save_path}{method} {file_name}{path_suffix}', 'wb'\n",
    "            ) as f:\n",
    "                joblib.dump(file_, f, compress=False, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(f'Saved {method} {file_name}{path_suffix} to {results_save_path}')\n",
    "\n",
    "        searchcv = CalibratedClassifierCV(\n",
    "            searchcv, cv=cv, method='sigmoid'\n",
    "        ).fit(X_train, y_train)\n",
    "\n",
    "    # For Sequential classifier: compile for binary classification, optimize with adam and score on recall\n",
    "    if classifier_name == 'Sequential':\n",
    "        searchcv.compile(\n",
    "            loss='binary_crossentropy', optimizer='adamw', metrics=list(scoring)\n",
    "        ).fit(X_train, y_train)\n",
    "\n",
    "    return (\n",
    "        estimator, X_train, y_train, X_test, y_test, searchcv, df_feature_importances\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f33cf875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_table(\n",
    "    df_metrics,\n",
    "    col, vectorizer_name, classifier_name, protocol,\n",
    "    table_save_path=table_save_path,\n",
    "    method=method, save_name=None,\n",
    "    compression=None,\n",
    "    path_suffix=None,\n",
    "):\n",
    "    if save_name is None:\n",
    "        save_name = f'{method} Estimators Table'\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {col} - {vectorizer_name} + {classifier_name} (Save_protocol={protocol}).pkl'\n",
    "\n",
    "    # Save metrics df\n",
    "    save_path = f'{table_save_path}{save_name}'\n",
    "    print(f'Saving fitted estimator and table')\n",
    "    df_metrics.to_csv(f'{save_path}.csv')\n",
    "    df_metrics.to_pickle(f'{save_path}.pkl')\n",
    "    df_metrics.to_excel(f'{save_path}.xlsx')\n",
    "    df_metrics.style.to_latex(f'{save_path}.tex', hrules=True)\n",
    "    df_metrics.to_markdown(f'{save_path}.md')\n",
    "    df_metrics.to_html(f'{save_path}.html')\n",
    "\n",
    "    print('Done saving fitted estimator and table!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fc524e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completed_estimators(\n",
    "    results_save_path=results_save_path, method=method, classifiers_pipe=classifiers_pipe,\n",
    "    estimators_list=None, used_classifiers=None,\n",
    "):\n",
    "    if estimators_list is None:\n",
    "        estimators_list = []\n",
    "    if used_classifiers is None:\n",
    "        used_classifiers = []\n",
    "\n",
    "    for estimator_path in glob.glob(f'{results_save_path}{method} Estimator - *.pkl'):\n",
    "        classifier_name = estimator_path.split(f'{results_save_path}{method} ')[1].split(' + ')[1].split(' (Save_protocol=')[0]\n",
    "        used_classifiers.append(classifier_name)\n",
    "        with open(estimator_path, 'rb') as f:\n",
    "            estimators_list.append(joblib.load(f))\n",
    "\n",
    "    return estimators_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a44148bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_plots(\n",
    "    estimators_list, X_test, y_test, col,\n",
    "    curves_dict=None, cmap=plt.cm.Grays\n",
    "):\n",
    "\n",
    "    curves_dict = {\n",
    "        'ROC Curve': metrics.RocCurveDisplay,\n",
    "        'Precision Recall Curve': metrics.PrecisionRecallDisplay,\n",
    "        # 'Calibration Curve': CalibrationDisplay,\n",
    "        # 'Validation Curve': ValidationCurveDisplay,\n",
    "        # 'Learning Curve': LearningCurveDisplay,\n",
    "    }\n",
    "\n",
    "    assert len(estimators_list) != 0\n",
    "\n",
    "    for curve_name, curve_package in curves_dict.items():\n",
    "        print('-' * 20)\n",
    "        print(f'{col} - {str(curve_name)}')\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(f'{col} - {str(curve_name)}')\n",
    "        for estimator in estimators_list:\n",
    "            try:\n",
    "                curve = curve_package.from_estimator(\n",
    "                    estimator, X_test, y_test, pos_label=1, ax=ax, cmap=cmap,\n",
    "                    name=f'{estimator.steps[0][0]} + {estimator.steps[1][0]} + {estimator.steps[-1][0]}'\n",
    "                )\n",
    "            except AttributeError:\n",
    "                curve = curve_package.from_estimator(\n",
    "                    estimator, X_test, y_test, pos_label=1, ax=ax, color='black',\n",
    "                    name=f'{estimator.steps[0][0]} + {estimator.steps[1][0]} + {estimator.steps[-1][0]}'\n",
    "                )\n",
    "        show_and_close_plots(plt)\n",
    "\n",
    "        # Save Plots\n",
    "        for image_save_format in tqdm.tqdm(['png', 'svg']):\n",
    "            save_path = f'{plot_save_path}{method} {col} - All {str(curve_name)}s.{image_save_format}'\n",
    "            print(f'Saving {curve_name}')\n",
    "            curve.figure_.savefig(\n",
    "                save_path, format=image_save_format, dpi=3000, bbox_inches='tight'\n",
    "            )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "036fcf10",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0212dc9",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Starting!\n",
      "########################################\n",
      "Searching for existing estimators in directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 304/304 [00:00<00:00, 1029942.18it/s]\n",
      "100%|██████████| 70/70 [00:00<00:00, 1188669.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Loading table from Supervised Estimators Table.pkl\n",
      "Done loading table!\n",
      "====================\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading data for Warmth - TfidfVectorizer + PassiveAggressiveClassifier\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading df_val_data\n",
      "Loading Grid Search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SearchCV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/88 [00:44<1:04:32, 44.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading df_train_data\n",
      "Loading df_cv_results\n",
      "Loading df_test_data\n",
      "Loading Estimator.\n",
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (4023,)\n",
      "----------\n",
      "Training set example:\n",
      "Analysis (technical, quantitative and qualitative) of multiple sources of information (commercial Intelligence, OSINT, community, **ISACs sharing) to provide timely, actionable intelligence and reporting.\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (537,)\n",
      "----------\n",
      "Testing set example:\n",
      "Experience with agile development practices, particularly owning and running specific agile events such as backlog refinement and sprint reviews Knowledge of multi channel supply chain processes, preferable in a retail context.\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (805,)\n",
      "----------\n",
      "Validation set example:\n",
      "Duties and responsibilities: Handling incoming phone calls and emails from the website users;Acting as an intermediary between the customers and accommodations;Managing reservations, special requests, and complaints and finding solutions to website users inquiries.\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.33 (0 = 0.67, 1 = 2.00)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.40 (0 = 0.70, 1 = 1.74)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.30 (0 = 0.65, 1 = 2.14)\n",
      "====================\n",
      "Done loading Xy, CV data, and estimator!\n",
      "['df_val_data', 'Grid Search', 'SearchCV', 'df_train_data', 'df_cv_results', 'df_test_data', 'Estimator']\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 584863.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - TfidfVectorizer + VotingClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 70/70 [00:00<00:00, 947100.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - FeatureUnion + LogisticRegression (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1249367.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - CountVectorizer + DummyClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 988556.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - TfidfVectorizer + StackingClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1002052.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - TfidfVectorizer + MultinomialNB (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 384294.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - CountVectorizer + KNeighborsClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1044844.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - CountVectorizer + MultinomialNB (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 873813.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Loading table from Supervised Estimators Table.pkl\n",
      "Done loading table!\n",
      "====================\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading data for Warmth - CountVectorizer + PassiveAggressiveClassifier\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading df_val_data\n",
      "Loading Grid Search\n",
      "Loading df_train_data\n",
      "Loading SearchCV\n",
      "Loading df_cv_results\n",
      "Loading df_test_data\n",
      "Loading Estimator.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 9/88 [01:13<09:02,  6.87s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (4023,)\n",
      "----------\n",
      "Training set example:\n",
      "Analysis (technical, quantitative and qualitative) of multiple sources of information (commercial Intelligence, OSINT, community, **ISACs sharing) to provide timely, actionable intelligence and reporting.\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (537,)\n",
      "----------\n",
      "Testing set example:\n",
      "Experience with agile development practices, particularly owning and running specific agile events such as backlog refinement and sprint reviews Knowledge of multi channel supply chain processes, preferable in a retail context.\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (805,)\n",
      "----------\n",
      "Validation set example:\n",
      "Duties and responsibilities: Handling incoming phone calls and emails from the website users;Acting as an intermediary between the customers and accommodations;Managing reservations, special requests, and complaints and finding solutions to website users inquiries.\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.33 (0 = 0.67, 1 = 2.00)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.40 (0 = 0.70, 1 = 1.74)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.30 (0 = 0.65, 1 = 2.14)\n",
      "====================\n",
      "Done loading Xy, CV data, and estimator!\n",
      "['df_val_data', 'Grid Search', 'df_train_data', 'SearchCV', 'df_cv_results', 'df_test_data', 'Estimator']\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 959481.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - CountVectorizer + BaggingClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1165084.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - TfidfVectorizer + XGBClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 998643.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - CountVectorizer + MLPClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1346794.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - FeatureUnion + DecisionTreeClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1151377.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Loading table from Supervised Estimators Table.pkl\n",
      "Done loading table!\n",
      "====================\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading data for Competence - FeatureUnion + SGDClassifier\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading df_val_data\n",
      "Loading df_train_data\n",
      "Loading df_test_data\n",
      "Loading df_cv_results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Grid Search\n",
      "Loading SearchCV\n",
      "Loading Estimator.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 14/88 [02:10<11:01,  8.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (3997,)\n",
      "----------\n",
      "Training set example:\n",
      "The job includes statistical programming and handling/processing of big data sets, for which you will need to have extensive experience in coding.\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (534,)\n",
      "----------\n",
      "Testing set example:\n",
      "1-3 or more years advisory/consulting/industry project experience in a high calibre and international environment, ideally working with industry, energy, manufacturing, or agricultural clients\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (799,)\n",
      "----------\n",
      "Validation set example:\n",
      "Developing your team and motivating them to achieve their goals.\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.83 (0 = 0.92, 1 = 1.10)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.83 (0 = 0.91, 1 = 1.10)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.91 (0 = 0.95, 1 = 1.05)\n",
      "====================\n",
      "Done loading Xy, CV data, and estimator!\n",
      "['df_val_data', 'df_train_data', 'df_test_data', 'df_cv_results', 'Grid Search', 'SearchCV', 'Estimator']\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 171295.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - FeatureUnion + KNeighborsClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1165084.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - TfidfVectorizer + VotingClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1453471.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - TfidfVectorizer + MLPClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1468006.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - CountVectorizer + XGBClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1521250.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - CountVectorizer + StackingClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1249367.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Loading table from Supervised Estimators Table.pkl\n",
      "Done loading table!\n",
      "====================\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading data for Competence - TfidfVectorizer + Perceptron\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading df_val_data\n",
      "Loading df_train_data\n",
      "Loading df_test_data\n",
      "Loading df_cv_results\n",
      "Loading Grid Search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SearchCV\n",
      "Loading Estimator.\n",
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (3997,)\n",
      "----------\n",
      "Training set example:\n",
      "The job includes statistical programming and handling/processing of big data sets, for which you will need to have extensive experience in coding.\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (534,)\n",
      "----------\n",
      "Testing set example:\n",
      "1-3 or more years advisory/consulting/industry project experience in a high calibre and international environment, ideally working with industry, energy, manufacturing, or agricultural clients\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (799,)\n",
      "----------\n",
      "Validation set example:\n",
      "Developing your team and motivating them to achieve their goals.\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.83 (0 = 0.92, 1 = 1.10)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.83 (0 = 0.91, 1 = 1.10)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.91 (0 = 0.95, 1 = 1.05)\n",
      "====================\n",
      "Done loading Xy, CV data, and estimator!\n",
      "['df_val_data', 'df_train_data', 'df_test_data', 'df_cv_results', 'Grid Search', 'SearchCV', 'Estimator']\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 853492.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - FeatureUnion + DecisionTreeClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 897863.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Loading table from Supervised Estimators Table.pkl\n",
      "Done loading table!\n",
      "====================\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading data for Warmth - CountVectorizer + LinearSVC\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading df_val_data\n",
      "Loading Grid Search\n",
      "Loading SearchCV\n",
      "Loading df_train_data\n",
      "Loading df_cv_results\n",
      "Loading df_test_data\n",
      "Loading Estimator.\n",
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (4023,)\n",
      "----------\n",
      "Training set example:\n",
      "Analysis (technical, quantitative and qualitative) of multiple sources of information (commercial Intelligence, OSINT, community, **ISACs sharing) to provide timely, actionable intelligence and reporting.\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (537,)\n",
      "----------\n",
      "Testing set example:\n",
      "Experience with agile development practices, particularly owning and running specific agile events such as backlog refinement and sprint reviews Knowledge of multi channel supply chain processes, preferable in a retail context.\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (805,)\n",
      "----------\n",
      "Validation set example:\n",
      "Duties and responsibilities: Handling incoming phone calls and emails from the website users;Acting as an intermediary between the customers and accommodations;Managing reservations, special requests, and complaints and finding solutions to website users inquiries.\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.33 (0 = 0.67, 1 = 2.00)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.40 (0 = 0.70, 1 = 1.74)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.30 (0 = 0.65, 1 = 2.14)\n",
      "====================\n",
      "Done loading Xy, CV data, and estimator!\n",
      "['df_val_data', 'Grid Search', 'SearchCV', 'df_train_data', 'df_cv_results', 'df_test_data', 'Estimator']\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 667275.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Loading table from Supervised Estimators Table.pkl\n",
      "Done loading table!\n",
      "====================\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading data for Warmth - TfidfVectorizer + SGDClassifier\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading SearchCV\n",
      "Loading df_val_data\n",
      "Loading Grid Search\n",
      "Loading df_train_data\n",
      "Loading df_cv_results\n",
      "Loading df_test_data\n",
      "Loading Estimator.\n",
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (4023,)\n",
      "----------\n",
      "Training set example:\n",
      "Analysis (technical, quantitative and qualitative) of multiple sources of information (commercial Intelligence, OSINT, community, **ISACs sharing) to provide timely, actionable intelligence and reporting.\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (537,)\n",
      "----------\n",
      "Testing set example:\n",
      "Experience with agile development practices, particularly owning and running specific agile events such as backlog refinement and sprint reviews Knowledge of multi channel supply chain processes, preferable in a retail context.\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (805,)\n",
      "----------\n",
      "Validation set example:\n",
      "Duties and responsibilities: Handling incoming phone calls and emails from the website users;Acting as an intermediary between the customers and accommodations;Managing reservations, special requests, and complaints and finding solutions to website users inquiries.\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.33 (0 = 0.67, 1 = 2.00)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.40 (0 = 0.70, 1 = 1.74)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.30 (0 = 0.65, 1 = 2.14)\n",
      "====================\n",
      "Done loading Xy, CV data, and estimator!\n",
      "['SearchCV', 'df_val_data', 'Grid Search', 'df_train_data', 'df_cv_results', 'df_test_data', 'Estimator']\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1033807.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - TfidfVectorizer + StackingClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1165084.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - FeatureUnion + VotingClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 262144.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Loading table from Supervised Estimators Table.pkl\n",
      "Done loading table!\n",
      "====================\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading data for Competence - CountVectorizer + SGDClassifier\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading df_val_data\n",
      "Loading SearchCV\n",
      "Loading df_train_data\n",
      "Loading Grid Search\n",
      "Loading df_cv_results\n",
      "Loading df_test_data\n",
      "Loading Estimator.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 26/88 [02:42<04:25,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (3997,)\n",
      "----------\n",
      "Training set example:\n",
      "The job includes statistical programming and handling/processing of big data sets, for which you will need to have extensive experience in coding.\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (534,)\n",
      "----------\n",
      "Testing set example:\n",
      "1-3 or more years advisory/consulting/industry project experience in a high calibre and international environment, ideally working with industry, energy, manufacturing, or agricultural clients\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (799,)\n",
      "----------\n",
      "Validation set example:\n",
      "Developing your team and motivating them to achieve their goals.\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.83 (0 = 0.92, 1 = 1.10)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.83 (0 = 0.91, 1 = 1.10)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.91 (0 = 0.95, 1 = 1.05)\n",
      "====================\n",
      "Done loading Xy, CV data, and estimator!\n",
      "['df_val_data', 'SearchCV', 'df_train_data', 'Grid Search', 'df_cv_results', 'df_test_data', 'Estimator']\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 492619.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - TfidfVectorizer + KNeighborsClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1228457.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Loading table from Supervised Estimators Table.pkl\n",
      "Done loading table!\n",
      "====================\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading data for Warmth - FeatureUnion + SGDClassifier\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading df_test_data\n",
      "Loading Grid Search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading df_val_data\n",
      "Loading df_train_data\n",
      "Loading SearchCV\n",
      "Loading df_cv_results\n",
      "Loading Estimator.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 28/88 [03:24<08:09,  8.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (4446,)\n",
      "----------\n",
      "Training set example:\n",
      "This internship is for you if:\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (593,)\n",
      "----------\n",
      "Testing set example:\n",
      "General switchboard number +44 (0)207 801 3380.\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (889,)\n",
      "----------\n",
      "Validation set example:\n",
      "Effective ability to prioritize tasks and deliver on deadlines, with high performance standards and a commitment to excellence.\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.37 (0 = 0.68, 1 = 1.86)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.39 (0 = 0.69, 1 = 1.79)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.40 (0 = 0.70, 1 = 1.76)\n",
      "====================\n",
      "Done loading Xy, CV data, and estimator!\n",
      "['df_test_data', 'Grid Search', 'df_val_data', 'df_train_data', 'SearchCV', 'df_cv_results', 'Estimator']\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 998643.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Loading table from Supervised Estimators Table.pkl\n",
      "Done loading table!\n",
      "====================\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading data for Competence - CountVectorizer + PassiveAggressiveClassifier\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading df_val_data\n",
      "Loading df_train_data\n",
      "Loading SearchCV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Grid Search\n",
      "Loading df_cv_results\n",
      "Loading df_test_data\n",
      "Loading Estimator.\n",
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (3997,)\n",
      "----------\n",
      "Training set example:\n",
      "The job includes statistical programming and handling/processing of big data sets, for which you will need to have extensive experience in coding.\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (534,)\n",
      "----------\n",
      "Testing set example:\n",
      "1-3 or more years advisory/consulting/industry project experience in a high calibre and international environment, ideally working with industry, energy, manufacturing, or agricultural clients\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (799,)\n",
      "----------\n",
      "Validation set example:\n",
      "Developing your team and motivating them to achieve their goals.\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.83 (0 = 0.92, 1 = 1.10)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.83 (0 = 0.91, 1 = 1.10)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.91 (0 = 0.95, 1 = 1.05)\n",
      "====================\n",
      "Done loading Xy, CV data, and estimator!\n",
      "['df_val_data', 'df_train_data', 'SearchCV', 'Grid Search', 'df_cv_results', 'df_test_data', 'Estimator']\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1112126.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - CountVectorizer + DummyClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 897863.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - FeatureUnion + LogisticRegression (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 518730.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Loading table from Supervised Estimators Table.pkl\n",
      "Done loading table!\n",
      "====================\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading data for Warmth - TfidfVectorizer + Perceptron\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading df_test_data\n",
      "Loading Grid Search\n",
      "Loading df_val_data\n",
      "Loading df_train_data\n",
      "Loading SearchCV\n",
      "Loading df_cv_results\n",
      "Loading Estimator.\n",
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (4023,)\n",
      "----------\n",
      "Training set example:\n",
      "Analysis (technical, quantitative and qualitative) of multiple sources of information (commercial Intelligence, OSINT, community, **ISACs sharing) to provide timely, actionable intelligence and reporting.\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (537,)\n",
      "----------\n",
      "Testing set example:\n",
      "Experience with agile development practices, particularly owning and running specific agile events such as backlog refinement and sprint reviews Knowledge of multi channel supply chain processes, preferable in a retail context.\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (805,)\n",
      "----------\n",
      "Validation set example:\n",
      "Duties and responsibilities: Handling incoming phone calls and emails from the website users;Acting as an intermediary between the customers and accommodations;Managing reservations, special requests, and complaints and finding solutions to website users inquiries.\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.33 (0 = 0.67, 1 = 2.00)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.40 (0 = 0.70, 1 = 1.74)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.30 (0 = 0.65, 1 = 2.14)\n",
      "====================\n",
      "Done loading Xy, CV data, and estimator!\n",
      "['df_test_data', 'Grid Search', 'df_val_data', 'df_train_data', 'SearchCV', 'df_cv_results', 'Estimator']\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1002052.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - CountVectorizer + GradientBoostingClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 985239.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - CountVectorizer + StackingClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1030179.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - TfidfVectorizer + BaggingClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 935035.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - CountVectorizer + GradientBoostingClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 737691.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - FeatureUnion + RandomForestClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 346227.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Loading table from Supervised Estimators Table.pkl\n",
      "Done loading table!\n",
      "====================\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading data for Competence - CountVectorizer + LinearSVC\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading SearchCV\n",
      "Loading df_val_data\n",
      "Loading Grid Search\n",
      "Loading df_cv_results\n",
      "Loading df_train_data\n",
      "Loading df_test_data\n",
      "Loading Estimator.\n",
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (3997,)\n",
      "----------\n",
      "Training set example:\n",
      "The job includes statistical programming and handling/processing of big data sets, for which you will need to have extensive experience in coding.\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (534,)\n",
      "----------\n",
      "Testing set example:\n",
      "1-3 or more years advisory/consulting/industry project experience in a high calibre and international environment, ideally working with industry, energy, manufacturing, or agricultural clients\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (799,)\n",
      "----------\n",
      "Validation set example:\n",
      "Developing your team and motivating them to achieve their goals.\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.83 (0 = 0.92, 1 = 1.10)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.83 (0 = 0.91, 1 = 1.10)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.91 (0 = 0.95, 1 = 1.05)\n",
      "====================\n",
      "Done loading Xy, CV data, and estimator!\n",
      "['SearchCV', 'df_val_data', 'Grid Search', 'df_cv_results', 'df_train_data', 'df_test_data', 'Estimator']\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 932067.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - FeatureUnion + RandomForestClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1346794.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Loading table from Supervised Estimators Table.pkl\n",
      "Done loading table!\n",
      "====================\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading data for Competence - TfidfVectorizer + PassiveAggressiveClassifier\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading df_val_data\n",
      "Loading df_train_data\n",
      "Loading df_cv_results\n",
      "Loading SearchCV\n",
      "Loading Grid Search\n",
      "Loading df_test_data\n",
      "Loading Estimator.\n",
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (3997,)\n",
      "----------\n",
      "Training set example:\n",
      "The job includes statistical programming and handling/processing of big data sets, for which you will need to have extensive experience in coding.\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (534,)\n",
      "----------\n",
      "Testing set example:\n",
      "1-3 or more years advisory/consulting/industry project experience in a high calibre and international environment, ideally working with industry, energy, manufacturing, or agricultural clients\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (799,)\n",
      "----------\n",
      "Validation set example:\n",
      "Developing your team and motivating them to achieve their goals.\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.83 (0 = 0.92, 1 = 1.10)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.83 (0 = 0.91, 1 = 1.10)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.91 (0 = 0.95, 1 = 1.05)\n",
      "====================\n",
      "Done loading Xy, CV data, and estimator!\n",
      "['df_val_data', 'df_train_data', 'df_cv_results', 'SearchCV', 'Grid Search', 'df_test_data', 'Estimator']\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 843681.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - CountVectorizer + VotingClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1091454.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - FeatureUnion + MultinomialNB (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 975419.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - TfidfVectorizer + KNeighborsClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 579095.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Loading table from Supervised Estimators Table.pkl\n",
      "Done loading table!\n",
      "====================\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading data for Competence - CountVectorizer + Perceptron\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading SearchCV\n",
      "Loading df_cv_results\n",
      "Loading Grid Search\n",
      "Loading df_val_data\n",
      "Loading df_train_data\n",
      "Loading df_test_data\n",
      "Loading Estimator.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 44/88 [05:33<05:26,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (3997,)\n",
      "----------\n",
      "Training set example:\n",
      "The job includes statistical programming and handling/processing of big data sets, for which you will need to have extensive experience in coding.\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (534,)\n",
      "----------\n",
      "Testing set example:\n",
      "1-3 or more years advisory/consulting/industry project experience in a high calibre and international environment, ideally working with industry, energy, manufacturing, or agricultural clients\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (799,)\n",
      "----------\n",
      "Validation set example:\n",
      "Developing your team and motivating them to achieve their goals.\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.83 (0 = 0.92, 1 = 1.10)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.83 (0 = 0.91, 1 = 1.10)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.91 (0 = 0.95, 1 = 1.05)\n",
      "====================\n",
      "Done loading Xy, CV data, and estimator!\n",
      "['SearchCV', 'df_cv_results', 'Grid Search', 'df_val_data', 'df_train_data', 'df_test_data', 'Estimator']\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 887012.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - TfidfVectorizer + DummyClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 555011.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - TfidfVectorizer + BaggingClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1213228.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - CountVectorizer + AdaBoostClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1188669.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - TfidfVectorizer + LogisticRegression (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 351618.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - TfidfVectorizer + RandomForestClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1026577.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - TfidfVectorizer + RandomForestClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1169726.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - FeatureUnion + StackingClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 866080.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - FeatureUnion + DummyClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1129235.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - TfidfVectorizer + GradientBoostingClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 972189.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - TfidfVectorizer + GradientBoostingClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1254706.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - CountVectorizer + VotingClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1188669.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Loading table from Supervised Estimators Table.pkl\n",
      "Done loading table!\n",
      "====================\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading data for Warmth - CountVectorizer + SGDClassifier\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading df_test_data\n",
      "Loading df_cv_results\n",
      "Loading df_train_data\n",
      "Loading Grid Search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading df_val_data\n",
      "Loading SearchCV\n",
      "Loading Estimator.\n",
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (4023,)\n",
      "----------\n",
      "Training set example:\n",
      "Analysis (technical, quantitative and qualitative) of multiple sources of information (commercial Intelligence, OSINT, community, **ISACs sharing) to provide timely, actionable intelligence and reporting.\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (537,)\n",
      "----------\n",
      "Testing set example:\n",
      "Experience with agile development practices, particularly owning and running specific agile events such as backlog refinement and sprint reviews Knowledge of multi channel supply chain processes, preferable in a retail context.\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (805,)\n",
      "----------\n",
      "Validation set example:\n",
      "Duties and responsibilities: Handling incoming phone calls and emails from the website users;Acting as an intermediary between the customers and accommodations;Managing reservations, special requests, and complaints and finding solutions to website users inquiries.\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.33 (0 = 0.67, 1 = 2.00)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.40 (0 = 0.70, 1 = 1.74)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.30 (0 = 0.65, 1 = 2.14)\n",
      "====================\n",
      "Done loading Xy, CV data, and estimator!\n",
      "['df_test_data', 'df_cv_results', 'df_train_data', 'Grid Search', 'df_val_data', 'SearchCV', 'Estimator']\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 437557.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - FeatureUnion + VotingClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 533820.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - CountVectorizer + LogisticRegression (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 281227.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - TfidfVectorizer + AdaBoostClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 404409.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - CountVectorizer + RandomForestClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1015921.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Loading table from Supervised Estimators Table.pkl\n",
      "Done loading table!\n",
      "====================\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading data for Warmth - TfidfVectorizer + LinearSVC\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading df_cv_results\n",
      "Loading df_test_data\n",
      "Loading df_val_data\n",
      "Loading Grid Search\n",
      "Loading SearchCV\n",
      "Loading df_train_data\n",
      "Loading Estimator.\n",
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (4023,)\n",
      "----------\n",
      "Training set example:\n",
      "Analysis (technical, quantitative and qualitative) of multiple sources of information (commercial Intelligence, OSINT, community, **ISACs sharing) to provide timely, actionable intelligence and reporting.\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (537,)\n",
      "----------\n",
      "Testing set example:\n",
      "Experience with agile development practices, particularly owning and running specific agile events such as backlog refinement and sprint reviews Knowledge of multi channel supply chain processes, preferable in a retail context.\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (805,)\n",
      "----------\n",
      "Validation set example:\n",
      "Duties and responsibilities: Handling incoming phone calls and emails from the website users;Acting as an intermediary between the customers and accommodations;Managing reservations, special requests, and complaints and finding solutions to website users inquiries.\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.33 (0 = 0.67, 1 = 2.00)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.40 (0 = 0.70, 1 = 1.74)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.30 (0 = 0.65, 1 = 2.14)\n",
      "====================\n",
      "Done loading Xy, CV data, and estimator!\n",
      "['df_cv_results', 'df_test_data', 'df_val_data', 'Grid Search', 'SearchCV', 'df_train_data', 'Estimator']\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 280153.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - FeatureUnion + DummyClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 469762.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - CountVectorizer + RandomForestClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 138360.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Loading table from Supervised Estimators Table.pkl\n",
      "Done loading table!\n",
      "====================\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading data for Competence - TfidfVectorizer + SGDClassifier\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading df_test_data\n",
      "Loading df_cv_results\n",
      "Loading df_train_data\n",
      "Loading Grid Search\n",
      "Loading df_val_data\n",
      "Loading SearchCV\n",
      "Loading Estimator.\n",
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (3997,)\n",
      "----------\n",
      "Training set example:\n",
      "The job includes statistical programming and handling/processing of big data sets, for which you will need to have extensive experience in coding.\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (534,)\n",
      "----------\n",
      "Testing set example:\n",
      "1-3 or more years advisory/consulting/industry project experience in a high calibre and international environment, ideally working with industry, energy, manufacturing, or agricultural clients\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (799,)\n",
      "----------\n",
      "Validation set example:\n",
      "Developing your team and motivating them to achieve their goals.\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.83 (0 = 0.92, 1 = 1.10)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.83 (0 = 0.91, 1 = 1.10)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.91 (0 = 0.95, 1 = 1.05)\n",
      "====================\n",
      "Done loading Xy, CV data, and estimator!\n",
      "['df_test_data', 'df_cv_results', 'df_train_data', 'Grid Search', 'df_val_data', 'SearchCV', 'Estimator']\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 160175.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - FeatureUnion + XGBClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 863533.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - FeatureUnion + StackingClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 785030.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - TfidfVectorizer + DecisionTreeClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1056119.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - TfidfVectorizer + DecisionTreeClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 324063.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - CountVectorizer + KNeighborsClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 225847.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - FeatureUnion + XGBClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 959481.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - CountVectorizer + XGBClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 932067.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - TfidfVectorizer + MLPClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 770607.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - FeatureUnion + KNeighborsClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 129396.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - CountVectorizer + BaggingClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 959481.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - TfidfVectorizer + DummyClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 609131.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - CountVectorizer + MLPClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1316597.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - TfidfVectorizer + XGBClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1044844.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - CountVectorizer + AdaBoostClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 533820.51it/s]\n",
      " 90%|████████▉ | 79/88 [05:58<00:11,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - TfidfVectorizer + LogisticRegression (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 336313.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Loading table from Supervised Estimators Table.pkl\n",
      "Done loading table!\n",
      "====================\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading data for Competence - TfidfVectorizer + LinearSVC\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading df_cv_results\n",
      "Loading df_train_data\n",
      "Loading df_test_data\n",
      "Loading SearchCV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 79/88 [06:10<00:11,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading df_val_data\n",
      "Loading Grid Search\n",
      "Loading Estimator.\n",
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (3997,)\n",
      "----------\n",
      "Training set example:\n",
      "The job includes statistical programming and handling/processing of big data sets, for which you will need to have extensive experience in coding.\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (534,)\n",
      "----------\n",
      "Testing set example:\n",
      "1-3 or more years advisory/consulting/industry project experience in a high calibre and international environment, ideally working with industry, energy, manufacturing, or agricultural clients\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (799,)\n",
      "----------\n",
      "Validation set example:\n",
      "Developing your team and motivating them to achieve their goals.\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.83 (0 = 0.92, 1 = 1.10)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.83 (0 = 0.91, 1 = 1.10)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.91 (0 = 0.95, 1 = 1.05)\n",
      "====================\n",
      "Done loading Xy, CV data, and estimator!\n",
      "['df_cv_results', 'df_train_data', 'df_test_data', 'SearchCV', 'df_val_data', 'Grid Search', 'Estimator']\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 770607.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - CountVectorizer + MultinomialNB (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 635500.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - CountVectorizer + DecisionTreeClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 1188669.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Competence - CountVectorizer + DecisionTreeClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 831731.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - TfidfVectorizer + MultinomialNB (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 486900.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - FeatureUnion + MultinomialNB (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 653900.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - CountVectorizer + LogisticRegression (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 760625.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Warmth - TfidfVectorizer + AdaBoostClassifier (Save_protocol=5).pkl already fitted! Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 920380.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Loading table from Supervised Estimators Table.pkl\n",
      "Done loading table!\n",
      "====================\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading data for Warmth - CountVectorizer + Perceptron\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading df_train_data\n",
      "Loading SearchCV\n",
      "Loading df_cv_results\n",
      "Loading df_test_data\n",
      "Loading Grid Search\n",
      "Loading df_val_data\n",
      "Loading Estimator.\n",
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (4023,)\n",
      "----------\n",
      "Training set example:\n",
      "Analysis (technical, quantitative and qualitative) of multiple sources of information (commercial Intelligence, OSINT, community, **ISACs sharing) to provide timely, actionable intelligence and reporting.\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (537,)\n",
      "----------\n",
      "Testing set example:\n",
      "Experience with agile development practices, particularly owning and running specific agile events such as backlog refinement and sprint reviews Knowledge of multi channel supply chain processes, preferable in a retail context.\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (805,)\n",
      "----------\n",
      "Validation set example:\n",
      "Duties and responsibilities: Handling incoming phone calls and emails from the website users;Acting as an intermediary between the customers and accommodations;Managing reservations, special requests, and complaints and finding solutions to website users inquiries.\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.33 (0 = 0.67, 1 = 2.00)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.40 (0 = 0.70, 1 = 1.74)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.30 (0 = 0.65, 1 = 2.14)\n",
      "====================\n",
      "Done loading Xy, CV data, and estimator!\n",
      "['df_train_data', 'SearchCV', 'df_cv_results', 'df_test_data', 'Grid Search', 'df_val_data', 'Estimator']\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [06:19<00:00,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "DONE!\n",
      "########################################\n",
      "CPU times: user 4min 2s, sys: 29.6 s, total: 4min 32s\n",
      "Wall time: 6min 19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('#'*40)\n",
    "print('Starting!')\n",
    "print('#'*40)\n",
    "\n",
    "method = 'Supervised'\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "# Get existing estimators\n",
    "estimator_names_list = get_existing_files()\n",
    "done_estimators = glob.glob(f'{done_xy_save_path}*')\n",
    "done_files = [\n",
    "    'df_train_data', 'df_test_data', 'df_val_data', 'df_cv_results', 'Grid Search', 'SearchCV'\n",
    "]\n",
    "\n",
    "# Identify cols, vectorizers and classifiers\n",
    "for estimators_file in tqdm.tqdm(glob.glob(f'{results_save_path}{method} Estimator - *.pkl')):\n",
    "    assert f'{method} Estimator - ' in estimators_file, f'Estimators file name {estimators_file} does not contain {method} Estimator - '\n",
    "    estimate_file_name = estimators_file.split(f'{method} Estimator - ')[-1]\n",
    "\n",
    "    # Skip fitted estimators\n",
    "    fitted_estimators = [fitted_estimators_file.split(f'{method} Fitted Estimator - ')[-1] for fitted_estimators_file in tqdm.tqdm(glob.glob(f'{results_save_path}{method} Fitted Estimator - *.pkl'))]\n",
    "    if estimate_file_name in fitted_estimators and skip_fitted_estimators is True:\n",
    "        print(f'Estimator {estimate_file_name} already fitted! Skipping...')\n",
    "        continue\n",
    "\n",
    "    # Specify col, vectorizer and classifier\n",
    "    col = estimate_file_name.split(' - ')[0]\n",
    "    vectorizer_name = estimators_file.split(f'{col} - ')[-1].split(' + ')[0]\n",
    "    classifier_name = estimators_file.split(f'{vectorizer_name} + ')[-1].split(' (Save_protocol=')[0]\n",
    "    protocol = int(estimators_file.split(f'{vectorizer_name} + ')[-1].split(' (Save_protocol=')[-1].split(').pkl')[0])\n",
    "\n",
    "    # Load Table DF\n",
    "    df_metrics = make_df_metrics(\n",
    "        vectorizers_pipe=vectorizers_pipe, classifiers_pipe=classifiers_pipe, transformers_pipe=transformers_pipe,\n",
    "        metrics_list=metrics_dict,\n",
    "        col=col, vectorizer_name=vectorizer_name, classifier_name=classifier_name, protocol=protocol\n",
    "    )\n",
    "\n",
    "    print('~'*20)\n",
    "    print(f'Loading data for {col} - {vectorizer_name} + {classifier_name}')\n",
    "    print('~'*20)\n",
    "    # Load X, y, search_cv, estimator\n",
    "    try:\n",
    "        (\n",
    "            grid_search, searchcv,\n",
    "            X_train, y_train, y_train_pred,\n",
    "            X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "            X_val, y_val, y_val_pred, y_val_pred_prob,\n",
    "            df_feature_importances, df_cv_results, estimator,\n",
    "        ) = load_Xy_search_cv_estimator(\n",
    "            col, vectorizer_name, classifier_name, protocol\n",
    "        )\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    if searchcv.__class__.__name__ != 'CalibratedClassifierCV':\n",
    "        print('-'*20)\n",
    "        print(f'{\"=\"*30} EVALUATING DATASET OF LENGTH {len(X_train)+len(X_test)+len(X_val)} ON {col.upper()} {\"=\"*30}')\n",
    "        print('='*20)\n",
    "        print(\n",
    "            f'GridSearch - Best mean train score: M = {float(best_mean_train_score:=searchcv.cv_results_[\"mean_train_score\"][best_index:=searchcv.best_index_]):.2f}, SD = {int(best_std_train_score:=searchcv.cv_results_[\"std_train_score\"][best_index]):.2f}\\n'\n",
    "        )\n",
    "        print(\n",
    "            f'GridSearch - Best mean test score: M = {float(best_mean_test_score:=searchcv.cv_results_[\"mean_test_score\"][best_index]):.2f}, SD = {int(best_std_test_score:=searchcv.cv_results_[\"std_test_score\"][best_index]):.2f}\\n'\n",
    "        )\n",
    "        print(\n",
    "            f'Best train score for {scoring}: {float(best_train_score:=searchcv.best_score_):.2f}\\n'\n",
    "        )\n",
    "        print(\n",
    "            f'Best test score for {scoring}: {float(best_test_score:=searchcv.score(X_test, y_test)):.2f}\\n'\n",
    "        )\n",
    "        print(\n",
    "            f'Number of splits: {int(n_splits:=searchcv.n_splits_)}\\n'\n",
    "        )\n",
    "        print(\n",
    "            f'Best estimator:\\n{searchcv.best_estimator_}\\n'\n",
    "        )\n",
    "        print(\n",
    "            f'Best estimator and parameters:\\n{searchcv.best_params_}\\n'\n",
    "        )\n",
    "        print(\n",
    "            f'Testing Classification Report:\\n{(train_report:=metrics.classification_report(y_test, y_test_pred, labels=np.unique(y_test_pred), zero_division=0))}\\n'\n",
    "        )\n",
    "        # Examine predictions\n",
    "        examine_predictions(\n",
    "            X_test, y_test, y_test_pred, col\n",
    "        )\n",
    "        print('='*20)\n",
    "        # Train and Test Confusion Matrix\n",
    "        print('='*20)\n",
    "        print('Train and Test Confusion Matrix:\\n')\n",
    "        close_plots(plt)\n",
    "        fig, axs = plt.subplots(1, 2)\n",
    "        fig.suptitle(f'{col} - Train and Test Confusion Matrix - {vectorizer_name} + {classifier_name}')\n",
    "        for ax in axs:\n",
    "            ax.set_aspect('equal')\n",
    "        train_cm = metrics.ConfusionMatrixDisplay.from_estimator(\n",
    "            estimator, X_train, y_train, normalize='true', ax=axs[0], cmap=plt.cm.Grays, colorbar=False\n",
    "        )\n",
    "        train_cm.ax_.set_title('training Data')\n",
    "        test_cm = metrics.ConfusionMatrixDisplay.from_estimator(\n",
    "            estimator, X_test, y_test, normalize='true', ax=axs[1], cmap=plt.cm.Grays, colorbar=False\n",
    "        )\n",
    "        test_cm.ax_.set_title('Testing Data')\n",
    "        plt.tight_layout()\n",
    "        for image_save_format in tqdm.tqdm(['png', 'svg']):\n",
    "            save_path = f'{plot_save_path}{method} {col} - Train and Test Confusion Matrix - {vectorizer_name} + {classifier_name}.{image_save_format}'\n",
    "            print(f'Train and Test Confusion Matrix plot')\n",
    "            fig.savefig(\n",
    "                save_path, format=image_save_format, dpi=3000, bbox_inches='tight'\n",
    "            )\n",
    "        show_and_close_plots(plt)\n",
    "        print('='*20)\n",
    "        # Train and Test ROC Curve\n",
    "        print('='*20)\n",
    "        print('Train and Test Scores in K Folds Cross Validation:')\n",
    "        close_plots(plt)\n",
    "        fig = plt.figure(figsize=(10, 5))\n",
    "        plt.title(f'K-folds Cross Validation Train vs. Test Scores for {col} - {vectorizer_name} + {classifier_name}')\n",
    "        plt.plot(searchcv.cv_results_[\"mean_train_score\"], label='Train Scores')\n",
    "        plt.plot(searchcv.cv_results_[\"mean_test_score\"], label='Test Scores')\n",
    "        plt.legend(loc='best')\n",
    "        plt.xlabel('Cross Validation Steps Over K Number of Folds')\n",
    "        plt.ylabel('Recall Score')\n",
    "        fig.text(0.1, 0.01, '*Number of folds used (K) = 10', ha='center', va='center', fontsize=10)\n",
    "        for image_save_format in tqdm.tqdm(['png', 'svg']):\n",
    "            save_path = f'{plot_save_path}{method} {col} - Train and Test Scores in K Folds Cross Validation - {vectorizer_name} + {classifier_name}.{image_save_format}'\n",
    "            print(f'K Folds plot')\n",
    "            fig.savefig(\n",
    "                save_path, format=image_save_format, dpi=3000, bbox_inches='tight'\n",
    "            )\n",
    "        show_and_close_plots(plt)\n",
    "        print('='*20)\n",
    "\n",
    "        if evaluate_estimator_on_concat:\n",
    "            # Fit estimator\n",
    "            print('~'*20)\n",
    "            print('Fitting best params to estimator')\n",
    "            X = np.concatenate((X_train, X_val), axis=0)\n",
    "            y_labels = np.concatenate((y_train, y_val), axis=0)\n",
    "            estimator = estimator.set_params(**searchcv.best_params_)\n",
    "            estimator.fit(X, y)\n",
    "\n",
    "            # Normalize Xy for unusual classifiers after fitting\n",
    "            (\n",
    "                estimator, X, y_labels, X_test, y_test, searchcv, df_feature_importances\n",
    "            ) = normalize_after_fitting(\n",
    "                estimator, X, y_labels, X_test, y_test, grid_search, searchcv, vectorizer_name, classifier_name\n",
    "            )\n",
    "\n",
    "            # Set prediction probability attribute\n",
    "            if hasattr(estimator, 'predict_proba'):\n",
    "                searchcv_predict_attr = estimator.predict_proba\n",
    "            elif hasattr(estimator, '_predict_proba_lr'):\n",
    "                searchcv_predict_attr = estimator._predict_proba_lr\n",
    "\n",
    "            # Get predictions and probabilities\n",
    "            y_pred = estimator.predict(X_test)\n",
    "            y_pred_prob = searchcv_predict_attr(X_test)[:, 1]\n",
    "        else:\n",
    "            X = X_test\n",
    "            y_labels = y_test\n",
    "            y_pred = y_test_pred\n",
    "            y_pred_prob = y_test_pred_prob\n",
    "\n",
    "        print('Saving fitted estimator')\n",
    "        save_fitted_estimator(\n",
    "            estimator, col, vectorizer_name, classifier_name, protocol,\n",
    "        )\n",
    "        print('Fitted estimator saved')\n",
    "        print('~'*20)\n",
    "\n",
    "        # Evaluate Model\n",
    "        df_metrics, metrics_dict, df_cv_score_recall = evaluation(\n",
    "            estimator, X, y_labels, y_pred, y_pred_prob,\n",
    "            best_train_score, df_metrics,\n",
    "            col, vectorizer_name, classifier_name, plot_enabled=True, print_enabled=True\n",
    "        )\n",
    "\n",
    "        # Confirmatory Regression\n",
    "        prob_confirmatory_tests(y_pred, y_pred_prob)\n",
    "\n",
    "        # Save Vectorizer, Selector, and Classifier\n",
    "        save_table(df_metrics, col, vectorizer_name, classifier_name, protocol)\n",
    "        print(df_metrics)\n",
    "\n",
    "    # # Compare Estimators\n",
    "    # print('='*20)\n",
    "    # print(f'Comparing Estimators for {col}')\n",
    "    # comparison_plots(get_completed_estimators(), X_test, y_test, col)\n",
    "    # print('='*20)\n",
    "\n",
    "print('#'*40)\n",
    "print('DONE!')\n",
    "print('#'*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d4a826",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
